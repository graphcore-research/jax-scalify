{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aAoSgjO1FhVr"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from typing import *\n",
        "_torch_tensor_to = torch.Tensor.to\n",
        "_torch_tensor = torch.tensor\n",
        "\n",
        "def numeric_info(dtype):\n",
        "  return torch.finfo(dtype) if dtype.is_floating_point else torch.iinfo(dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM8RBHPwIkV2",
        "outputId": "353862f6-c2e3-4346-b891-25de11eb3550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stensor([1.0002443 2.0004885 3.       ], dtype=sfloat16)\n",
            "Wasted 4.0 bits\n",
            "stensor([1.5262516e-05 3.0525032e-05 4.5776367e-05], dtype=sfloat16)\n"
          ]
        }
      ],
      "source": [
        "# Core\n",
        "\n",
        "@dataclass\n",
        "class ScaledDType:\n",
        "    name: str\n",
        "    dtype: torch.dtype\n",
        "    base_scale: float\n",
        "\n",
        "    def __init__(self, name: str, dtype: torch.dtype):\n",
        "        self.name = name\n",
        "        self.dtype = dtype\n",
        "        dmax = numeric_info(dtype).max\n",
        "        self.base_scale = 1 / dmax\n",
        "\n",
        "    def __hash__(self) -> int:\n",
        "        return hash((self.name, self.dtype))\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return self.name\n",
        "\n",
        "    @property\n",
        "    def bits(self) -> float:\n",
        "        return numeric_info(self.dtype).bits\n",
        "\n",
        "\n",
        "sfloat16 = ScaledDType(\"sfloat16\", torch.float16)\n",
        "sfloat8_e4m3fn = ScaledDType(\"sfloat8_e4m3fn\", torch.float8_e4m3fn)\n",
        "sfloat8_e5m2 = ScaledDType(\"sfloat8_e5m2\", torch.float8_e5m2)\n",
        "sint32 = ScaledDType(\"sint32\", torch.int32)\n",
        "sint16 = ScaledDType(\"sint16\", torch.int16)\n",
        "sint8 = ScaledDType(\"sint8\", torch.int8)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ScaledTensor:\n",
        "    data: Tensor\n",
        "    scale: Tensor\n",
        "    dtype: ScaledDType\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        assert self.data.dtype == self.dtype.dtype\n",
        "        assert self.scale.dtype == torch.float32\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"stensor({self.to(torch.float32).numpy()}, dtype={self.dtype})\"\n",
        "\n",
        "    def to(self, dtype: torch.dtype) -> Tensor:\n",
        "        return self.data.to(dtype) * self.scale.to(dtype) * self.dtype.base_scale\n",
        "\n",
        "    @property\n",
        "    def shape(self) -> torch.Size:\n",
        "        return self.data.shape\n",
        "\n",
        "def quantise(x: Tensor, dtype: ScaledDType) -> ScaledTensor:\n",
        "    scale = x.abs().max().to(torch.float32)\n",
        "    if scale == 0:\n",
        "        scale = torch.tensor(1, dtype=torch.float32)\n",
        "    return ScaledTensor(data=(x / scale / dtype.base_scale).to(dtype.dtype), scale=scale, dtype=dtype)\n",
        "\n",
        "def wasted_bits(st: ScaledTensor) -> float:\n",
        "    range = (st.data.abs().max() * st.dtype.base_scale).to(torch.float32)\n",
        "    if range == 0:\n",
        "        return st.dtype.bits\n",
        "    logrange = torch.log2(1/range)\n",
        "    if st.dtype.dtype.is_floating_point:\n",
        "        return float(torch.log2(logrange))\n",
        "    return float(logrange)\n",
        "\n",
        "def requantise(st: ScaledTensor) -> ScaledTensor:\n",
        "    rescale = st.data.abs().max().to(torch.float32) * st.dtype.base_scale\n",
        "    return ScaledTensor((st.data / rescale).to(st.dtype.dtype), scale=st.scale * rescale, dtype=st.dtype)\n",
        "\n",
        "\n",
        "# Monkeypatch for convenience\n",
        "@partial(setattr, torch.Tensor, 'to')\n",
        "def _(self: Tensor, *args: Any, **kwargs: Any) -> Tensor:\n",
        "    for arg in args + (kwargs.get(\"dtype\"),):\n",
        "        if isinstance(arg, ScaledDType):\n",
        "            assert len(args) + len(kwargs) == 1\n",
        "            return quantise(self, arg)\n",
        "    return _torch_tensor_to(self, *args, **kwargs)\n",
        "\n",
        "@partial(setattr, torch, 'tensor')\n",
        "def _(data: Any, *, dtype: Union[None, torch.dtype, ScaledDType] = None, **kwargs: Any) -> Union[Tensor, ScaledTensor]:\n",
        "    if isinstance(dtype, ScaledDType):\n",
        "        return _torch_tensor(data, **kwargs).to(dtype)\n",
        "    return _torch_tensor(data, dtype=dtype, **kwargs)\n",
        "\n",
        "d = torch.tensor([1, 2, 3], dtype=sfloat16)\n",
        "print(d)\n",
        "d.data[...] /= 2.0**16\n",
        "print(f\"Wasted {wasted_bits(d)} bits\")\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylM9CdOGKpn2",
        "outputId": "9d4fa786-a6b2-4d04-aa23-3200a789588e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stensor(300.0, dtype=sint8)\n",
            "stensor([[390636.9 390636.9 390636.9]\n",
            " [390636.9 390636.9 390636.9]], dtype=sint16)\n"
          ]
        }
      ],
      "source": [
        "# Ops\n",
        "# - Use worst-case scaling rules (no overflow!)\n",
        "# - Placeholder impl (fast impl requires custom kernls)\n",
        "# - No autograd support\n",
        "\n",
        "\n",
        "def add(a: ScaledTensor, b: ScaledTensor) -> ScaledTensor:\n",
        "    assert a.dtype == b.dtype\n",
        "    scale = a.scale + b.scale\n",
        "    data = (a.data * (a.scale / scale) + b.data * (b.scale / scale)).to(a.data.dtype)\n",
        "    return ScaledTensor(data, scale, a.dtype)\n",
        "\n",
        "\n",
        "def matmul(a: ScaledTensor, b: ScaledTensor) -> ScaledTensor:\n",
        "    assert a.dtype == b.dtype\n",
        "    scale = a.scale * b.scale * a.shape[-1]\n",
        "    downscale = (a.shape[-1] / a.dtype.base_scale) ** -.5\n",
        "    return ScaledTensor(\n",
        "        (a.data * downscale).to(a.dtype.dtype) @ (b.data * downscale).to(b.dtype.dtype),\n",
        "        scale, a.dtype)\n",
        "\n",
        "\n",
        "def relu(a: ScaledTensor) -> ScaledTensor:\n",
        "    return ScaledTensor(nn.functional.relu(a.data), a.scale, a.dtype)\n",
        "\n",
        "\n",
        "ScaledTensor.__add__ = add\n",
        "ScaledTensor.__matmul__ = matmul\n",
        "\n",
        "\n",
        "print(torch.tensor(100, dtype=sint8) + torch.tensor(200, dtype=sint8))\n",
        "print(torch.full((2, 20), 100).to(sint16) @ torch.full((20, 3), 200).to(sint16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtrOg8FMpDb3",
        "outputId": "cd7bc783-0567-4c37-908d-56ba2ec2e1a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wasted bits #1: 8.258488655090332\n",
            "wasted bits #2: 10.414993286132812\n",
            "\n",
            "stensor([[   0.        -41.124104  164.49641  ...  287.8687     41.124104\n",
            "   205.62051 ]\n",
            " [ -82.24821  -123.37231   -82.24821  ... -164.49641  -164.49641\n",
            "  -164.49641 ]\n",
            " [ 123.37231  -205.62051   411.24103  ...  -82.24821     0.\n",
            "     0.      ]\n",
            " ...\n",
            " [ -41.124104  123.37231   205.62051  ... -287.8687   -287.8687\n",
            "  -411.24103 ]\n",
            " [ -82.24821  -164.49641    82.24821  ...   82.24821    41.124104\n",
            "   -41.124104]\n",
            " [  41.124104  205.62051    82.24821  ...   82.24821  -164.49641\n",
            "  -123.37231 ]], dtype=sint16)\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, hidden_size: int, dtype: Union[torch.dtype, ScaledDType]):\n",
        "        super().__init__()\n",
        "        self.W0 = torch.randn(hidden_size, 4*hidden_size).to(dtype)\n",
        "        self.W1 = torch.randn(4*hidden_size, hidden_size).to(dtype)\n",
        "        self.relu = relu if isinstance(dtype, ScaledDType) else nn.functional.relu\n",
        "\n",
        "    def forward(self, x: Union[Tensor, ScaledTensor]) -> Union[Tensor, ScaledTensor]:\n",
        "        y = self.relu(x @ self.W0)\n",
        "\n",
        "        if isinstance(y, ScaledTensor):\n",
        "            print(f\"wasted bits #1: {wasted_bits(y)}\")\n",
        "            y = requantise(y)\n",
        "\n",
        "        y = y @ self.W1\n",
        "\n",
        "        if isinstance(y, ScaledTensor):\n",
        "            print(f\"wasted bits #2: {wasted_bits(y)}\")\n",
        "\n",
        "        return y\n",
        "\n",
        "hidden_size = 1024\n",
        "dtype = sint16\n",
        "module = FFN(hidden_size, dtype)\n",
        "result = module(torch.randn(10, hidden_size).to(dtype))\n",
        "print()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
