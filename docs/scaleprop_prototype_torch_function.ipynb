{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aAoSgjO1FhVr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING\u001b[0m : Nothing to worry about\n"
     ]
    }
   ],
   "source": [
    "from types import MethodWrapperType, GetSetDescriptorType\n",
    "from typing import *\n",
    "from warnings import warn\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from icecream import ic\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, tensor, nn\n",
    "from torch.utils._pytree import tree_map\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Make warn=print for notebooks, as otherwise outputs are not interleaved correctly\n",
    "from termcolor import colored\n",
    "\n",
    "aten = torch.ops.aten\n",
    "\n",
    "\n",
    "def warn(msg):\n",
    "    print(colored(\"WARNING\", \"yellow\"), f\": {msg}\")\n",
    "\n",
    "\n",
    "warn(\"Nothing to worry about\")\n",
    "\n",
    "\n",
    "def numeric_info(dtype):\n",
    "    return torch.finfo(dtype) if dtype.is_floating_point else torch.iinfo(dtype)\n",
    "\n",
    "\n",
    "def dtype_max(dtype):\n",
    "    return numeric_info(dtype).max\n",
    "\n",
    "\n",
    "def _round(x: Tensor, dtype: torch.dtype) -> Tensor:\n",
    "    \"\"\"\n",
    "    Convert to dtype, rounding if the destination is integer\n",
    "    \"\"\"\n",
    "    if dtype.is_floating_point:\n",
    "        return x.to(dtype)\n",
    "    else:\n",
    "        return torch.round(x).to(dtype)\n",
    "\n",
    "\n",
    "def function_str(func: Callable) -> str:\n",
    "    return torch.overrides.resolve_name(func)\n",
    "\n",
    "    # https://stackoverflow.com/questions/251464/how-to-get-a-function-name-as-a-string\n",
    "    # for future expansion e.g. properties\n",
    "    if hasattr(func, \"__module__\"):\n",
    "        return func.__module__ + \".\" + func.__qualname__\n",
    "    else:\n",
    "        return func.__qualname__\n",
    "\n",
    "\n",
    "def type_str(x: type) -> str:\n",
    "    return x.__name__\n",
    "\n",
    "\n",
    "def _uptype(dtype) -> torch.dtype:\n",
    "    \"\"\"\n",
    "    For DTYPE, what is the type in which most arithmetic (e.g. max, abs) is defined?\n",
    "    \"\"\"\n",
    "    GPU = False  # Check more accurately, and choose bf16 as appropriate\n",
    "    f16_t = torch.float16 if GPU else torch.float32\n",
    "    map = {\n",
    "        torch.int8: torch.int16,\n",
    "        torch.int16: torch.int16,\n",
    "        torch.float8_e4m3fn: f16_t,\n",
    "        torch.float8_e5m2: f16_t,\n",
    "        torch.float16: f16_t,\n",
    "        torch.float32: torch.float32,\n",
    "    }\n",
    "    return map[dtype]\n",
    "\n",
    "\n",
    "def _to_uptype(t: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Convert t to its _uptype\n",
    "    \"\"\"\n",
    "    return torch.as_tensor(t, dtype=_uptype(t.dtype))\n",
    "\n",
    "\n",
    "def _maxval(t: Tensor):\n",
    "    \"\"\"\n",
    "    Max absolute value of tensor, returned in its `_uptype`\n",
    "    \"\"\"\n",
    "    return _to_uptype(t).abs().max()\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision=3, threshold=32)\n",
    "\n",
    "# From https://github.com/albanD/subclass_zoo/blob/main/utils.py\n",
    "import contextlib\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _no_dispatch():\n",
    "    guard = torch._C._DisableTorchDispatch()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        del guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(100x300,f32) 10^-9 x Quants{-5.080|-0.442|-0.031|-0.000|0.031|7.563}\n",
      "Tensor(100x300,f32) Quants{-0.093|-0.004|-0.000|-0.000|0.000|0.058}\n",
      "Tensor(100x300,f32) Quants{-0.477|-0.043|-0.003|-0.000|0.003|0.537}\n",
      "Tensor(100x300,f32) Quants{-7.255|-0.447|-0.030|-0.000|0.031|7.686}\n",
      "Tensor(100x300,f32) Quants{-54.827|-4.489|-0.285|0.000|0.308|62.219}\n",
      "Tensor(100x300,f32) Quants{-1006.063|-44.348|-2.883|0.000|3.168|564.190}\n",
      "Tensor(100x300,f32) Quants{-8416.035|-432.668|-27.389|0.000|31.403|7809.401}\n",
      "Tensor(100x300,f32) 10^4 x Quants{-5.812|-0.428|-0.030|-0.000|0.031|9.379}\n",
      "Tensor(100x300,f32) 10^11 x Quants{-7.704|-0.444|-0.029|0.000|0.031|6.558}\n",
      "Tensor(100x300,u8) Quants{0|12|64|128|192|255}\n"
     ]
    }
   ],
   "source": [
    "def tensor_oneline_str(t):\n",
    "    shape_str = \"x\".join(map(str, t.shape))\n",
    "    quantiles = torch.tensor([0, 0.05, 0.25, 0.5, 0.75, 1.0])\n",
    "    vals = torch.quantile(torch.flatten(t).to(torch.float32), quantiles, interpolation=\"nearest\")\n",
    "    if t.dtype.is_floating_point:\n",
    "        # scale down vals\n",
    "        finite_vals = vals[torch.isfinite(vals)]\n",
    "        max = finite_vals.abs().max()\n",
    "        if max > 0:\n",
    "            logmax = torch.floor(torch.log10(max))\n",
    "            if -2 <= logmax <= 3:\n",
    "                logmax = 0\n",
    "            max_scale = 10**-logmax\n",
    "            max_scale_str = f\"10^{int(logmax)} x \" if logmax != 0 else \"\"\n",
    "        else:\n",
    "            max_scale = 1\n",
    "            max_scale_str = \"\"\n",
    "        vals_str = max_scale_str + \"Quants{\" + \"|\".join(f\"{v.item():.3f}\" for v in vals * max_scale) + \"}\"\n",
    "    else:\n",
    "        # Assume integer, print as integers\n",
    "        vals_str = \"Quants{\" + \"|\".join(f\"{int(v)}\" for v in vals) + \"}\"\n",
    "\n",
    "    classname = type(t).__name__\n",
    "\n",
    "    dtype_str = f\"{t.dtype}\".replace(\"torch.float\", \"f\").replace(\"torch.int\", \"i\").replace(\"torch.uint\", \"u\")\n",
    "\n",
    "    return f\"{classname}({shape_str},{dtype_str}) {vals_str}\"\n",
    "\n",
    "\n",
    "for scale in [-10, -3, -2, -1, 0, 1, 2, 3, 10]:\n",
    "    kurt = 3\n",
    "    print(tensor_oneline_str(torch.randn(100, 300) ** kurt * (10**scale)))\n",
    "\n",
    "print(tensor_oneline_str((torch.randn(100, 300) * 10000).to(torch.uint8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core functionality, no syntactic sugar\n",
    "\n",
    "Define a `ScaleTensorData` object, with the minimal information, and with operations\n",
    "defined cleanly, but without PyTorch Tensor integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sM8RBHPwIkV2",
    "outputId": "353862f6-c2e3-4346-b891-25de11eb3550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float16)\n",
      "tensor([1., 2., 3.], dtype=torch.float8_e4m3fn)\n",
      "ScaledTensorData(0.0066964286379516125 * Tensor(3,f8_e4m3fn) Quants{144.000|144.000|144.000|288.000|448.000|448.000})\n",
      "wasted_bits(st_data.data)=tensor(0.023) <-- of a max of 8 bits, should be wasting nearly zero\n",
      "ScaledTensorData(0.0066964286379516125 * Tensor(3,f8_e4m3fn) Quants{144.000|144.000|144.000|288.000|448.000|448.000})\n",
      "tensor([0.964, 1.928, 3.000], dtype=torch.float16) # <- rounding errors at the high end of the f8 range\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ScaledTensorData:\n",
    "    data: Tensor\n",
    "    scale: Tensor\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not isinstance(self.scale, Tensor):\n",
    "            self.scale = tensor(self.scale)\n",
    "        assert self.scale.dtype == torch.float32\n",
    "        assert self.scale.shape == ()\n",
    "        # Possible future expansion to e.g. row-scaled, column-scaled, etc, but\n",
    "        # for now, insist st_scale is a single-element tensor\n",
    "\n",
    "    def _contents_str(self) -> str:\n",
    "        return f\"{self.scale} * {tensor_oneline_str(self.data)}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"ScaledTensorData({self._contents_str()})\"\n",
    "\n",
    "    def to_tensor(self, dtype: torch.dtype = None) -> Tensor:\n",
    "        dtype = dtype or self.scale.dtype\n",
    "        return self.data.to(dtype) * self.scale.to(dtype)\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> torch.Size:\n",
    "        return self.data.shape\n",
    "\n",
    "    @property\n",
    "    def dtype_max(self):\n",
    "        return dtype_max(self.data.dtype)\n",
    "\n",
    "\n",
    "def st_quantise(x: Tensor, dtype: torch.dtype) -> ScaledTensorData:\n",
    "    \"\"\"\n",
    "    Rescale so other max(|data|) == maxFinite(data.dtype)\n",
    "    \"\"\"\n",
    "    maxval = _maxval(x)\n",
    "    if maxval == 0:\n",
    "        # Tensor is all zeros - set scale to 1\n",
    "        scale = torch.tensor(1.0, dtype=torch.float32)\n",
    "    else:\n",
    "        # Scale so other largest element is the largest finite value of dtype\n",
    "        scale = maxval / dtype_max(dtype)\n",
    "\n",
    "    return ScaledTensorData(_round(x / scale, dtype), scale)\n",
    "\n",
    "\n",
    "def st_requantise(st: ScaledTensorData) -> ScaledTensorData:\n",
    "    \"\"\"\n",
    "    Rescale so other max(|data|) == maxFinite(data.dtype)\n",
    "\n",
    "    Equivalent to quantise(st.to_tensor(torch.float32)) but avoids the conversion\n",
    "\n",
    "    Returned tensor may share its data with input tensor\n",
    "    \"\"\"\n",
    "    maxdataval = _maxval(st.data)\n",
    "    if maxdataval == 0:\n",
    "        # All zero, reset scale to 1\n",
    "        return ScaledTensorData(st.data, st.scale)\n",
    "    else:\n",
    "        rescale = maxdataval / st.dtype_max\n",
    "    return ScaledTensorData((_to_uptype(st.data) * (1 / rescale)).to(st.data.dtype), st.scale * rescale)\n",
    "\n",
    "\n",
    "def wasted_bits(st_data, maxval=None) -> float:\n",
    "    \"\"\"\n",
    "    By how much is tensor `st_data` not using the full dynamic range of its dtype?\n",
    "\n",
    "    E.g.\n",
    "       t = torch.tensor([1,2,-16], dtype=torch.int8)\n",
    "\n",
    "    Is using only 5 (4 + sign) of the available 8 bits.\n",
    "    Therefore\n",
    "       wasted_bits(t) == 3 == 8-3\n",
    "\n",
    "    Optional argument maxval, if the maximum value in the tensor has already\n",
    "    been computed, perhaps with a higher-accuracy method (e.g. pre-rounding)\n",
    "    \"\"\"\n",
    "    if maxval is None:\n",
    "        maxval = _maxval(st_data)\n",
    "\n",
    "    maxval = maxval.to(st_data.dtype)\n",
    "    dtype_bits = numeric_info(st_data.dtype).bits\n",
    "    if maxval == 0:\n",
    "        # All values zero -> all bits are wasted\n",
    "        return dtype_bits\n",
    "\n",
    "    # Otherwise, how many bits is maxval using.\n",
    "    if st_data.dtype.is_floating_point:\n",
    "        # Convert maxval to integer of the same bitwidth\n",
    "        ints = {8: torch.int8, 16: torch.int16, 32: torch.int32}\n",
    "        maxval = maxval.view(ints[dtype_bits])\n",
    "\n",
    "    # Assuming a signed type, max usable bits are dtype_bits-1\n",
    "    return dtype_bits - 1 - torch.log2(maxval)\n",
    "\n",
    "\n",
    "def test_wasted_bits():\n",
    "    t = torch.tensor([1, 2, -16], dtype=torch.int8)\n",
    "    assert wasted_bits(t) == 3\n",
    "\n",
    "\n",
    "test_wasted_bits()\n",
    "\n",
    "### Quick testing\n",
    "f16 = tensor([1, 2, 3], dtype=torch.float16)\n",
    "f8_t = torch.float8_e4m3fn\n",
    "print(f16)\n",
    "print(f16.to(f8_t))\n",
    "\n",
    "st_data = st_quantise(f16, f8_t)\n",
    "\n",
    "# TODO: wasting a lot more bits at the low end here -- range of 144->448.\n",
    "print(st_data)\n",
    "print(f\"{wasted_bits(st_data.data)=} <-- of a max of {numeric_info(f8_t).bits} bits, should be wasting nearly zero\")\n",
    "print(st_requantise(st_data))\n",
    "print(st_data.to_tensor(f16.dtype), \"# <- rounding errors at the high end of the f8 range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Operations, without syntactic sugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| st1: ScaledTensorData(0.5 * Tensor(,i8) Quants{32|32|32|32|32|32})\n",
      "ic| st2: ScaledTensorData(0.25 * Tensor(,i8) Quants{64|64|64|64|64|64})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| st_add(st1, st2): ScaledTensorData(0.75 * Tensor(,i8) Quants{42|42|42|42|42|42})\n",
      "ic| f32(st_add(st1, st2)): tensor(31.500)\n",
      "ic| f32(st1) + f32(st2): tensor(32.)\n",
      "ic| st3: ScaledTensorData(0.01775376871228218 * Tensor(2x32,i8) Quants{-127|-86|-52|-9|25|108})\n",
      "ic| st4: ScaledTensorData(0.021980037912726402 * Tensor(32x3,i8) Quants{-127|-80|-26|-1|23|96})\n",
      "ic| st_matmul(st3, st4): ScaledTensorData(0.00042443175334483385 * Tensor(2x3,i16) Quants{-9714|-9714|-7678|-5303|26302|32767})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f32(st_matmul(st3, st4)) = tensor([[11.163, -4.123, -3.259],\n",
      "        [-2.251,  4.451, 13.907]]) <-- quantized\n",
      "f32(st3) @ f32(st4) = tensor([[11.163, -4.123, -3.259],\n",
      "        [-2.251,  4.451, 13.907]]) <-- intermediate\n",
      "t3 @ t4 = tensor([[11.197, -4.210, -3.202],\n",
      "        [-2.218,  4.478, 13.879]]) <-- exact\n"
     ]
    }
   ],
   "source": [
    "# Ops\n",
    "# - Use worst-case scaling rules (no overflow!)\n",
    "# - Placeholder impl (fast impl requires custom kernels)\n",
    "# - No autograd support\n",
    "\n",
    "\n",
    "def st_add(a: ScaledTensorData, b: ScaledTensorData, alpha=1.0) -> ScaledTensorData:\n",
    "    b_scale = alpha * b.scale\n",
    "    out_dtype = a.data.dtype\n",
    "    scale = a.scale + b_scale\n",
    "    data = (_to_uptype(a.data) * (a.scale / scale) + _to_uptype(b.data) * (b_scale / scale)).to(out_dtype)\n",
    "    return ScaledTensorData(data, scale)\n",
    "\n",
    "\n",
    "def st_matmul(a: ScaledTensorData, b: ScaledTensorData, debug=True) -> ScaledTensorData:\n",
    "    a_uptype = _uptype(a.data.dtype)\n",
    "    b_uptype = _uptype(b.data.dtype)\n",
    "    assert a_uptype == b_uptype\n",
    "    out_dtype = a_uptype\n",
    "\n",
    "    # Do this in f32 until we are debugged\n",
    "    return st_quantise(a.to_tensor() @ b.to_tensor(), out_dtype)\n",
    "\n",
    "    # a_maxval = a.scale * a.dtype_max\n",
    "    # b_maxval = b.scale * b.dtype_max\n",
    "\n",
    "    # # Predicted maxval for NxK @ KxM\n",
    "    # K = a.shape[-1]\n",
    "    # out_maxval_estimate = a_maxval * b_maxval * np.sqrt(K)*2\n",
    "\n",
    "    # out_scale = out_maxval_estimate / dtype_max(out_dtype)\n",
    "\n",
    "    # # Derivation of matmul scale factors:\n",
    "    # # (ad * as) @ (bd * bs) = (ad @ bd) * (as * bs)\n",
    "    # #                       = (ad @ bd) * (as * bs / os * os)\n",
    "    # #                       = (ad @ bd * as * bs / os) * os\n",
    "    # #                       = (ad @ bd * rat) * os\n",
    "    # #                         where rat = as * bs / os\n",
    "    # #                       = (ad * sqrt(rat)) @ (bd * sqrt(rat)) * os\n",
    "\n",
    "    # rat = a.scale * b.scale / out_scale\n",
    "\n",
    "    # a_bits = numeric_info(a.data.dtype).bits\n",
    "    # b_bits = numeric_info(b.data.dtype).bits\n",
    "\n",
    "    # if max(a_bits, b_bits) < numeric_info(out_dtype).bits:\n",
    "    #     # Assume low-precision muls will accumulate to uptype, so won't overflow\n",
    "    #     # to simulate this on cpu, uptype before the matmul;\n",
    "    #     # on appropriate hardware (e.g. graphcore, h100), call the special matmul\n",
    "    #     adbd = _to_uptype(a.data) @ _to_uptype(b.data)\n",
    "    #     if debug:\n",
    "    #         out_maxval = _maxval(adbd) * (a.scale * b.scale)\n",
    "    #     out_data = adbd * rat\n",
    "    #     out_data = out_data.to(out_dtype)\n",
    "    # else:\n",
    "    #     # Inputs are in 16+ bits, and we know the products will certainly\n",
    "    #     # overflow, as they are scaled to dtype_max, so downscale before multiplying\n",
    "    #     sqrt_rat = torch.sqrt(rat)\n",
    "    #     a_down = _to_uptype(a.data) * sqrt_rat\n",
    "    #     b_down = _to_uptype(b.data) * sqrt_rat\n",
    "    #     out_data = a_down @ b_down\n",
    "    #     if debug:\n",
    "    #         out_maxval = _maxval(out_data) * out_scale\n",
    "    #     out_data = out_data.to(out_dtype)\n",
    "\n",
    "    # # debug check how bad out_maxval_estimate was\n",
    "    # if debug:\n",
    "    #     assert out_maxval_estimate > out_maxval  # Should always be an upper bound\n",
    "    #     wasted = wasted_bits(out_data)\n",
    "    #     if wasted > numeric_info(out_dtype).bits / 2:\n",
    "    #         warn(\n",
    "    #             f\"st_matmul: Very bad maxval estimate {out_maxval_estimate} vs {out_maxval}, {out_maxval_estimate/out_maxval:.1f}x too large - will lose at least {wasted} bits of precision\"\n",
    "    #         )\n",
    "\n",
    "    #     if _maxval(out_data) == 0:\n",
    "    #         raise ValueError(\"All-data zero - rerun with debug and view st_matmul: WARNING above\")\n",
    "\n",
    "    return ScaledTensorData(out_data, out_scale)\n",
    "\n",
    "\n",
    "def st_relu(a: ScaledTensorData) -> ScaledTensorData:\n",
    "    data = nn.functional.relu(a.data).to(a.data.dtype)\n",
    "    return ScaledTensorData(data, a.scale)\n",
    "\n",
    "\n",
    "# Check operators behave sensibly\n",
    "st1 = ScaledTensorData(tensor(32, dtype=torch.int8), 0.5)\n",
    "st2 = ScaledTensorData(tensor(64, dtype=torch.int8), 0.25)\n",
    "\n",
    "f32 = lambda x: x.to_tensor(torch.float32) if isinstance(x, ScaledTensorData) else x.to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "ic(st1)\n",
    "ic(st2)\n",
    "ic(st_add(st1, st2))\n",
    "ic(f32(st_add(st1, st2)))\n",
    "ic(f32(st1) + f32(st2))\n",
    "\n",
    "hidden = 32\n",
    "t3 = torch.randn(2, hidden)\n",
    "t4 = torch.randn(hidden, 3)\n",
    "st3 = st_quantise(t3, torch.int8)\n",
    "st4 = st_quantise(t4, torch.int8)\n",
    "ic(st3)\n",
    "ic(st4)\n",
    "ic(st_matmul(st3, st4))\n",
    "print(f\"{f32(st_matmul(st3, st4)) = } <-- quantized\")\n",
    "print(f\"{f32(st3) @ f32(st4) = } <-- intermediate\")\n",
    "print(f\"{t3 @ t4 = } <-- exact\")\n",
    "\n",
    "# st5 = st_quantise(tensor([-2, 3, -0.06, 4]), torch.int8)\n",
    "# ic(st5, f32(st5))\n",
    "# rt5 = st_relu(st5)\n",
    "# ic(rt5, f32(rt5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### [Aside: Possibly surprising rounding]\n",
    "\n",
    "# print('Starting point: ', tensor([-2, 0.09, 4]))\n",
    "\n",
    "# st5 = st_quantise(tensor([-2, 0.09, 4]), torch.int8)\n",
    "# print(f'{st5=} {f32(st5)=} <-- 0.0900 input rounds to 0.0630')\n",
    "\n",
    "# print('So, put in 0.0630 to begin with')\n",
    "# st5 = st_quantise(tensor([-2, 0.0630, 4]), torch.int8)\n",
    "# print(f'{st5=} {f32(st5)=} <-- great, 0.0630 rounds to 0.0630')\n",
    "\n",
    "# print('But now, put in 0.06')\n",
    "# st5 = st_quantise(tensor([-2, 0.06, 4]), torch.int8)\n",
    "# print(f'{st5=} {f32(st5)=} <-- Eh, 0.06 rounds down to 0.0315?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sugar hit: override Tensor operations in subclass\n",
    "\n",
    "We define a `ScaledTensor` object other behaves like a torch Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.]], dtype=torch.float16)\n",
      "st.shape=torch.Size([1, 3])\n",
      "st=ScaledTensor(0.023622047156095505 * Tensor(1x3,i8) Quants{42|42|42|85|127|127})\n",
      "requantise(st)=ScaledTensor(0.023622047156095505 * Tensor(1x3,i8) Quants{42|42|42|85|127|127})\n",
      "Rounding errors at the high end of the f8 range: tensor([[0.992, 2.008, 3.000]], dtype=torch.float16)\n",
      "Reshaped: ScaledTensor(0.023622047156095505 * Tensor(3x1,i8) Quants{42|42|42|85|127|127})\n",
      "\u001b[33mExpect a warning...\u001b[0m\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.add.Tensor@(ScaledTensor,int)\n",
      "Addition, but note warning above about \"Upcasting to float32\", so prints as a normal tensor: tensor([[2.992, 4.008, 5.000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=ScaledTensor(0.023622047156095505 * Tensor(1x3,i8) Quants{42|42|42|85|127|127},\n",
       "             grad_fn=<SortBackward0>),\n",
       "indices=tensor([[0, 1, 2]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See https://pytorch.org/docs/stable/notes/extending.html#extending-torch-with-a-tensor-like-type\n",
    "# TODO: check\n",
    "#   - _make_wrapper_subclass THPVariable_make_wrapper_subclass https://github.com/pytorch/pytorch/pull/65340\n",
    "#   - _make_subclass\n",
    "#   - See [Note] at https://github.com/albanD/subclass_zoo/blob/276d2f005484d80ebbcd9e274d79685adb6a1da2/negative_tensor.py#L24\n",
    "#     - Doesn't apply in this case as we are composing, not deriving?\n",
    "# Looking at https://github.com/albanD/subclass_zoo\n",
    "#   - trivial_tensor doesn't do autograd\n",
    "#   - inner_autograd_tensor explicitly defers to its `elem`, which is incorrect\n",
    "# In PyTorch core\n",
    "#   - MaskedTensor\n",
    "# See FP8Tensor in subclass_zoo: https://github.com/albanD/subclass_zoo/pull/44/files\n",
    "\n",
    "\n",
    "class ScaledTensor(Tensor):\n",
    "    @staticmethod\n",
    "    def __new__(cls, st: ScaledTensorData, *, requires_grad=False):\n",
    "        assert not st.data.requires_grad\n",
    "        ret = torch.Tensor._make_wrapper_subclass(\n",
    "            cls,\n",
    "            size=st.data.size(),\n",
    "            strides=st.data.stride(),\n",
    "            storage_offset=st.data.storage_offset(),\n",
    "            dtype=st.scale.dtype,\n",
    "            layout=st.data.layout,\n",
    "            requires_grad=requires_grad,\n",
    "            device=st.data.device,\n",
    "        )\n",
    "        ret.st = st\n",
    "        return ret\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # See https://github.com/pytorch/pytorch/issues/73665\n",
    "        with _no_dispatch():\n",
    "            return super().__repr__(tensor_contents=str(self.st._contents_str()))\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n",
    "        kwargs = kwargs or {}\n",
    "\n",
    "        func_str = f'{function_str(func)}@({\",\".join(type_str(type(x)) for x in args)})'\n",
    "        # print(':', func_str)\n",
    "\n",
    "        # 1. Is this one we handle?\n",
    "        extra_msg = \"\"\n",
    "        if func in cls.HANDLED_FUNCTIONS:\n",
    "            # Call the handler\n",
    "            handler = cls.HANDLED_FUNCTIONS[func]\n",
    "            try:  # TODO: just for debug\n",
    "                ret = handler(func, *args, **kwargs)\n",
    "            except Exception as e:\n",
    "                warn(f\"dispatch handler exception, e={repr(e)}\")\n",
    "                raise\n",
    "\n",
    "            if ret != NotImplemented:\n",
    "                return ret\n",
    "\n",
    "            # handler may have returned \"NotImplemented\" to tell us to run the fallback\n",
    "            extra_msg = f\" -- [Handler {function_str(handler)} returned NotImplemented]\"\n",
    "\n",
    "        # Not handled, convert all ScaledTensors to Tensors, and run\n",
    "        func_str = f'{function_str(func)}@({\",\".join(type_str(type(x)) for x in args)})'\n",
    "        warn(f\"ScaledTensor.__torch_dispatch__: Upcasting to float32 for {func_str}\" + extra_msg)\n",
    "\n",
    "        return upcast_args_and_redispatch(func, *args, **kwargs)\n",
    "\n",
    "    __torch_function__ = torch._C._disabled_torch_function_impl\n",
    "\n",
    "\n",
    "ScaledTensor.HANDLED_FUNCTIONS = {}\n",
    "\n",
    "\n",
    "def tensor_subclass_override(cls, funcs):\n",
    "    \"\"\"\n",
    "    Decorator to add an implementation of an operation to a Tensor subclass\n",
    "\n",
    "    @tensor_subclass_override(MySubclass, aten.view.default)\n",
    "    def _(func, *args, *kwargs):\n",
    "      print(f'Calling wrapped {func} with {len(args)} args)\n",
    "      with _no_dispatch():\n",
    "        return func(*args, *kwargs)\n",
    "\n",
    "    Calling the implementation \"_\" allows this decorator to overwrite the name with\n",
    "    a more sensible one \"@torch_function_override(MySubclass, aten.view.default)\"\n",
    "    but of course you can just call it \"MySubclass_impl_view\" or \"foo42\" if you prefer.\n",
    "    This is mainly useful in backtraces to see the decorated function easily\n",
    "    \"\"\"\n",
    "    funcs = funcs if isinstance(funcs, tuple) else (funcs,)\n",
    "    funcs_str = \",\".join(map(function_str, funcs))\n",
    "\n",
    "    def doit(impl):\n",
    "        # Override impl name if it was just \"_\"\n",
    "        if hasattr(impl, \"__name__\") and impl.__name__ == \"_\":\n",
    "            impl.__name__ = f\"@torch_function_override({cls.__name__}, {funcs_str})\"\n",
    "            if impl.__qualname__ != \"_\":\n",
    "                print(f\"torch_function_override: NOTE: {impl.__qualname__} not overridden\")\n",
    "\n",
    "            if impl.__qualname__ == \"_\":\n",
    "                impl.__qualname__ = f\"torch_function_override({cls.__name__}, {funcs_str})\"\n",
    "\n",
    "        # Record handler in the dictionary for each func\n",
    "        for func in funcs:\n",
    "            cls.HANDLED_FUNCTIONS[func] = impl\n",
    "\n",
    "        return impl\n",
    "\n",
    "    return doit\n",
    "\n",
    "\n",
    "def to_f32_if_scaled(t: Any) -> Tensor:\n",
    "    if isinstance(t, ScaledTensor):\n",
    "        return t.st.to_tensor(torch.float32)\n",
    "    else:\n",
    "        return t\n",
    "\n",
    "\n",
    "def to_scaled_if_f32(x, q_dtype):\n",
    "    if isinstance(x, Tensor) and x.dtype == torch.float32:\n",
    "        return ScaledTensor(st_quantise(x, q_dtype))\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "@tensor_subclass_override(\n",
    "    ScaledTensor,\n",
    "    (\n",
    "        aten.is_same_size.default,\n",
    "        aten.gt.Scalar,\n",
    "        aten.eq.Tensor,\n",
    "        aten.isnan.default,\n",
    "        aten.ne.Scalar,\n",
    "        aten.ge.Scalar,\n",
    "        aten.le.Scalar,\n",
    "    ),\n",
    ")\n",
    "def upcast_args_and_redispatch(func, *args, **kwargs):\n",
    "    new_args = tree_map(to_f32_if_scaled, args)\n",
    "    new_kwargs = tree_map(to_f32_if_scaled, kwargs)\n",
    "\n",
    "    with _no_dispatch():  # TODO: not needed?\n",
    "        return func(*new_args, **new_kwargs)\n",
    "\n",
    "\n",
    "# These ops punt silently to float32, but cast down to ScaledTensor - TODO: implement more efficiently\n",
    "@tensor_subclass_override(\n",
    "    ScaledTensor,\n",
    "    (\n",
    "        aten.convolution.default,\n",
    "        aten.convolution_backward.default,\n",
    "        aten.max_pool2d_with_indices.default,\n",
    "        aten.addmm.default,\n",
    "        aten.threshold_backward.default,\n",
    "        aten._log_softmax.default,\n",
    "        aten.nll_loss_forward.default,\n",
    "        aten.nll_loss_backward.default,\n",
    "        aten.sum.dim_IntList,\n",
    "        aten._log_softmax_backward_data.default,\n",
    "        aten.max_pool2d_with_indices_backward.default,\n",
    "        aten._local_scalar_dense.default,\n",
    "        aten.max.default,\n",
    "        aten.log10.default,\n",
    "        aten.floor.default\n",
    "    ),\n",
    ")\n",
    "def to_float32_and_wrap(func, *args, **kwargs):\n",
    "    ret = upcast_args_and_redispatch(func, *args, **kwargs)\n",
    "\n",
    "    q_dtypes = {a.st.data.dtype for a in args if isinstance(a, ScaledTensor)}\n",
    "    if len(q_dtypes) != 1:\n",
    "        q_dtype = {_uptype(ty) for ty in q_dtypes}\n",
    "        warn(f\"{func}: Widening to {q_dtype}... from {q_dtypes}\")\n",
    "    else:\n",
    "        q_dtype = q_dtypes\n",
    "    assert len(q_dtype) == 1\n",
    "    q_dtype = q_dtype.pop()\n",
    "\n",
    "    return tree_map(lambda x: to_scaled_if_f32(x, q_dtype), ret)\n",
    "\n",
    "\n",
    "@tensor_subclass_override(\n",
    "    ScaledTensor,\n",
    "    (\n",
    "        aten.view.default,\n",
    "        aten.permute.default,\n",
    "        aten.t.default,\n",
    "        aten.gather.default,\n",
    "        aten.index.Tensor,\n",
    "    ),\n",
    ")\n",
    "def redispatch_via_data(func, t_self, *args, **kwargs):\n",
    "    new_data = func(t_self.st.data, *args, **kwargs)\n",
    "    return ScaledTensor(ScaledTensorData(new_data, t_self.st.scale))\n",
    "\n",
    "\n",
    "@tensor_subclass_override(ScaledTensor, (aten.unbind.int))\n",
    "def _(func, t_self, *args, **kwargs):\n",
    "    ret = func(t_self.st.data, *args, **kwargs)\n",
    "    return tuple(ScaledTensor(ScaledTensorData(new_data, t_self.st.scale)) for new_data in ret)\n",
    "\n",
    "\n",
    "@tensor_subclass_override(\n",
    "    ScaledTensor,\n",
    "    (\n",
    "        aten.detach.default,\n",
    "        aten.ones_like.default,\n",
    "        aten.clone.default,\n",
    "        aten.abs.default,\n",
    "    ),\n",
    ")\n",
    "def redispatch_via_data_and_scale(func, t_self, *args, **kwargs) -> Tensor:\n",
    "    new_data = func(t_self.st.data, *args, **kwargs)\n",
    "    new_scale = func(t_self.st.scale, *args, **kwargs)\n",
    "    return ScaledTensor(ScaledTensorData(new_data, new_scale))\n",
    "\n",
    "\n",
    "@tensor_subclass_override(ScaledTensor, aten.sort.default)\n",
    "def _(func, t_self, *args, **kwargs):\n",
    "    new_data, indices = func(t_self.st.data, *args, **kwargs)\n",
    "    return ScaledTensor(ScaledTensorData(new_data, t_self.st.scale)), indices\n",
    "\n",
    "\n",
    "# Forward the ScaledTensorData ops above on the ScaledTensor type\n",
    "def quantise(x: Tensor, dtype: torch.dtype) -> ScaledTensor:\n",
    "    return ScaledTensor(st_quantise(x, dtype), requires_grad=x.requires_grad)\n",
    "\n",
    "\n",
    "def requantise(st) -> ScaledTensor:\n",
    "    return ScaledTensor(st_requantise(st.st), requires_grad=st.requires_grad)\n",
    "\n",
    "\n",
    "# Check basic to/from Subclass\n",
    "f16 = tensor([[1, 2, 3]], dtype=torch.float16)\n",
    "f8_t = torch.int8\n",
    "print(f16)\n",
    "\n",
    "st = quantise(f16, f8_t)\n",
    "\n",
    "print(f\"{st.shape=}\")\n",
    "assert st.shape == f16.shape\n",
    "s = str(st)\n",
    "print(f\"{st=}\")\n",
    "\n",
    "print(f\"{requantise(st)=}\")\n",
    "print(\"Rounding errors at the high end of the f8 range:\", st.st.to_tensor(f16.dtype))\n",
    "\n",
    "print(\"Reshaped:\", st.T)\n",
    "\n",
    "print(colored(\"Expect a warning...\", \"yellow\"))\n",
    "print('Addition, but note warning above about \"Upcasting to float32\", so prints as a normal tensor:', st + 2)\n",
    "\n",
    "st.requires_grad_(True)\n",
    "torch.sort(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now overrides work, but just punt up to f32 for all ops\n",
    "\n",
    "We will do some adds/multiplies etc, and note other the torch function\n",
    "implementation issues a sequence of \"WARNING: Upcasting to float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mExpect four warnings...\u001b[0m\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.add.Tensor@(ScaledTensor,int)\n",
      "tensor([[2.992, 4.008, 5.000]], grad_fn=<AddBackward0>)\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.mul.Tensor@(ScaledTensor,int)\n",
      "tensor([[1.984, 4.016, 6.000]], grad_fn=<MulBackward0>)\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.add.Tensor@(ScaledTensor,ScaledTensor)\n",
      "tensor([[1.984, 4.016, 6.000]], grad_fn=<AddBackward0>)\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.mm.default@(ScaledTensor,ScaledTensor)\n",
      "f32(st3 @ st4)       = tensor([[60000., 60000., 60000., 60000.],\n",
      "        [60000., 60000., 60000., 60000.]]) <-- should be ~21000 quantized, but was done in f32 so exact\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.mm.default@(ScaledTensor,ScaledTensor)\n",
      "f32(st3) @ f32(st4)  = tensor([[60000., 60000., 60000., 60000.],\n",
      "        [60000., 60000., 60000., 60000.]]) <-- 60000 exact\n"
     ]
    }
   ],
   "source": [
    "print(colored(\"Expect four warnings...\", \"yellow\"))\n",
    "\n",
    "print(st + 2)\n",
    "print(2 * st)\n",
    "print(st + st)\n",
    "\n",
    "st3 = quantise(torch.full((2, 3), 100.0), torch.int8)\n",
    "st4 = quantise(torch.full((3, 4), 200.0), torch.int8)\n",
    "\n",
    "print(f\"{f32(st3 @ st4)       = } <-- should be ~21000 quantized, but was done in f32 so exact\")\n",
    "print(f\"{f32(st3) @ f32(st4)  = } <-- 60000 exact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qx=ScaledTensor(0.025984251871705055 * Tensor(3,i8) Quants{42|42|42|85|127|127},\n",
      "             requires_grad=True)\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.add.Tensor@(ScaledTensor,ScaledTensor)\n",
      "qy=tensor([2.183, 4.417, 6.600], grad_fn=<AddBackward0>)\n",
      "calling backward\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.add.Tensor@(ScaledTensor,ScaledTensor)\n",
      "qx.grad=tensor([0.002, 0.004, 0.007])\n"
     ]
    }
   ],
   "source": [
    "# class ScaledTensor_add(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, a, b):\n",
    "#         print(\"st add fwd\")\n",
    "#         assert isinstance(a, ScaledTensor) and isinstance(b, ScaledTensor)\n",
    "#         return ScaledTensor(st_add(a.st, b.st))\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, dout):\n",
    "#         print(\"st add bwd\")\n",
    "#         return dout, dout\n",
    "\n",
    "# # Can't use\n",
    "\n",
    "# ret = ScaledTensor_add.apply(a, b)\n",
    "# print(\"add ret\", ret)\n",
    "# return ret\n",
    "\n",
    "\n",
    "x = tensor([1.1, 2.2, 3.3])\n",
    "\n",
    "qx = quantise(x, torch.int8)\n",
    "qx.requires_grad_(True)\n",
    "\n",
    "print(f\"{qx=}\")\n",
    "qy = qx + qx\n",
    "# qy = ScaledTensor_add.apply(qx, qx)\n",
    "print(f\"{qy=}\")\n",
    "\n",
    "\n",
    "dx = x * 0.001  # no actual need to do 0.001\n",
    "qdx = quantise(dx, torch.int8)\n",
    "print(\"calling backward\")\n",
    "qy.backward(qdx)\n",
    "print(f\"{qx.grad=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| st3: ScaledTensor"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st3=ScaledTensor(0.787401556968689 * Tensor(2x3,i8) Quants{127|127|127|127|127|127})\n",
      "ScaledTensor(1.8311105966567993 * Tensor(2x4,i16) Quants{32767|32767|32767|32767|32767|32767})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(0.787401556968689 * Tensor(2x3,i8) Quants{127|127|127|127|127|127})\n",
      "ic| st3.T: ScaledTensor(0.787401556968689 * Tensor(3x2,i8) Quants{127|127|127|127|127|127})\n",
      "ic| st3.flatten(): ScaledTensor(0.787401556968689 * Tensor(6,i8) Quants{127|127|127|127|127|127})\n",
      "ic| torch.flatten(st3): ScaledTensor(0.787401556968689 * Tensor(6,i8) Quants{127|127|127|127|127|127})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f32(st3 @ st4)       = ScaledTensor(1.8311105966567993 * Tensor(2x4,i16) Quants{32767|32767|32767|32767|32767|32767}) <-- ~21000 quantized\n",
      "f32(st3) @ f32(st4)  = ScaledTensor(1.8311105966567993 * Tensor(2x4,i16) Quants{32767|32767|32767|32767|32767|32767}) <-- 60000 exact\n",
      "ScaledTensor(0.023622047156095505 * Tensor(1x3,i8) Quants{42|42|42|85|127|127},\n",
      "             grad_fn=<ReluBackward0>)\n",
      "\u001b[33mWARNING\u001b[0m : aten.addmm.default: Widening to {torch.int16}... from {torch.int8, torch.int16}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tensor_subclass_override(ScaledTensor, aten.add.Tensor)\n",
    "def _(func_, a: Tensor, b: Tensor, alpha=1) -> Tensor:\n",
    "    if not (isinstance(a, ScaledTensor) and isinstance(b, ScaledTensor)):\n",
    "        return NotImplemented\n",
    "\n",
    "    return ScaledTensor(st_add(a.st, b.st, alpha))\n",
    "\n",
    "\n",
    "@tensor_subclass_override(ScaledTensor, aten.mm.default)\n",
    "def _(func_, a: Tensor, b: Tensor, *args, **kwargs) -> Tensor:\n",
    "    if not (isinstance(a, ScaledTensor) and isinstance(b, ScaledTensor)):\n",
    "        # Just upcast both.\n",
    "        print(end=\"u\")\n",
    "        return upcast_args_and_redispatch(func_, a, b, *args, **kwargs)\n",
    "\n",
    "    assert not args and not kwargs\n",
    "    return ScaledTensor(st_matmul(a.st, b.st, debug=True))\n",
    "\n",
    "\n",
    "@tensor_subclass_override(ScaledTensor, aten.relu.default)\n",
    "def _(func_, a: Tensor, inplace=False) -> Tensor:\n",
    "    if not isinstance(a, ScaledTensor):\n",
    "        return NotImplemented\n",
    "\n",
    "    assert not inplace\n",
    "    return ScaledTensor(st_relu(a.st))\n",
    "\n",
    "\n",
    "@tensor_subclass_override(\n",
    "    ScaledTensor,\n",
    "    (\n",
    "        aten.unsqueeze_.default,\n",
    "        aten.transpose_.default,\n",
    "        aten.squeeze_.dim,\n",
    "    ),\n",
    ")\n",
    "def inplace_delegate_to_data(func, t_self, *args, **kwargs):\n",
    "    ## inplace operation on first operand\n",
    "    if not isinstance(t_self, ScaledTensor):\n",
    "        # t_self is a normal tensor, just upcast\n",
    "        return upcast_args_and_redispatch(func, t_self, *args, **kwargs)\n",
    "\n",
    "    t_self.st.data = func(t_self.st.data, *args, **kwargs)\n",
    "\n",
    "    return t_self\n",
    "\n",
    "\n",
    "# - func: add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)\n",
    "@tensor_subclass_override(ScaledTensor, aten.add_.Tensor)\n",
    "def _(func, t_self, other, *, alpha=1):\n",
    "    ## inplace\n",
    "    if not isinstance(t_self, ScaledTensor):\n",
    "        # t_self is a normal tensor, just upcast and add in place\n",
    "        return t_self.add_(other.st.to_tensor(), alpha)\n",
    "\n",
    "    if isinstance(other, ScaledTensor):\n",
    "        t_self.st = st_add(t_self.st, other.st)\n",
    "    else:\n",
    "        # Do the add in high precision and quantize\n",
    "        ret = quantise(t_self.st.to_tensor() + other, dtype=t_self.st.data.dtype)\n",
    "        t_self.st = ret.st\n",
    "\n",
    "    return t_self\n",
    "\n",
    "\n",
    "# - func: mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)\n",
    "@tensor_subclass_override(ScaledTensor, aten.mul_.Tensor)\n",
    "def _(func, t_self, other, *args, **kwargs):\n",
    "    ## inplace\n",
    "    if not isinstance(t_self, ScaledTensor):\n",
    "        # t_self is a normal tensor, just upcast and add in place\n",
    "        return t_self.mul_(other.st.to_tensor())\n",
    "\n",
    "    if np.isscalar(other) or torch.numel(other) == 1:\n",
    "        t_self.st.scale *= other\n",
    "        return t_self\n",
    "\n",
    "    assert False\n",
    "\n",
    "\n",
    "@tensor_subclass_override(ScaledTensor, aten.mul.Tensor)\n",
    "def _(func, t_self, other, *args, **kwargs):\n",
    "    if np.isscalar(other) or torch.numel(other) == 1:\n",
    "        return ScaledTensor(ScaledTensorData(t_self.st.data, t_self.st.scale * other))\n",
    "\n",
    "    return upcast_args_and_redispatch(t_self, other, *args, **kwargs)\n",
    "\n",
    "\n",
    "st3 = quantise(torch.full((2, 3), 100.0), torch.int8)\n",
    "st4 = quantise(torch.full((3, 4), 200.0), torch.int8)\n",
    "\n",
    "\n",
    "def fred():\n",
    "    print(f\"{st3=}\")\n",
    "    print(st3 @ st4)\n",
    "\n",
    "\n",
    "fred()\n",
    "\n",
    "ic(st3)\n",
    "ic(st3.T)\n",
    "ic(st3.flatten())\n",
    "ic(torch.flatten(st3))\n",
    "\n",
    "# ic(st4)\n",
    "print(f\"{f32(st3 @ st4)       = } <-- ~21000 quantized\")\n",
    "print(f\"{f32(st3) @ f32(st4)  = } <-- 60000 exact\")\n",
    "\n",
    "print(nn.functional.relu(st))\n",
    "\n",
    "torch.addmm(st3 @ st3.T, st3, st3.T)\n",
    "torch.isnan(st3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And in a network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtrOg8FMpDb3",
    "outputId": "cd7bc783-0567-4c37-908d-56ba2ec2e1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledTensor(256x64,f32) Quants{-3.894|-1.594|-0.675|0.000|0.675|3.833}\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.eq.Scalar@(ScaledTensor,int)\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.view.dtype@(ScaledTensor,dtype)\n",
      "wasted bits #1: 0.9575748443603516\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.eq.Scalar@(ScaledTensor,int)\n",
      "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for aten.view.dtype@(ScaledTensor,dtype)\n",
      "wasted bits #2: 0.9221382141113281\n",
      "\n",
      "ScaledTensor(0.008577282540500164 * Tensor(10x64,i16) Quants{-27665|-14893|-4625|2450|8658|32767})\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_size: int, dtype: torch.dtype):\n",
    "        super().__init__()\n",
    "        q = lambda x: quantise(x, dtype)\n",
    "        self.W0 = q(torch.randn(hidden_size, 4 * hidden_size))\n",
    "        self.W1 = q(torch.randn(4 * hidden_size, hidden_size))\n",
    "        print(f\"{tensor_oneline_str(self.W1)}\")\n",
    "\n",
    "    def forward(self, x: Union[Tensor, ScaledTensor]) -> Union[Tensor, ScaledTensor]:\n",
    "        y = nn.functional.relu(x @ self.W0)\n",
    "\n",
    "        if isinstance(y, ScaledTensor):\n",
    "            print(f\"wasted bits #1: {wasted_bits(y)}\")\n",
    "            y = requantise(y)\n",
    "\n",
    "        y = y @ self.W1\n",
    "\n",
    "        if isinstance(y, ScaledTensor):\n",
    "            print(f\"wasted bits #2: {wasted_bits(y)}\")\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "hidden_size = 64\n",
    "storage_type = torch.int8\n",
    "module = FFN(hidden_size, storage_type)\n",
    "x = quantise(torch.randn(10, hidden_size), storage_type)\n",
    "result = module(x)\n",
    "print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "frog  truck bird  dog   bird  horse dog   ship  truck ship  horse dog   car   horse cat   cat   horse ship  truck ship  cat   deer  plane ship  ship  deer  ship  horse horse horse plane plane frog  cat   frog  horse cat   horse deer  deer  plane bird  deer  deer  deer  plane plane plane bird  frog  truck frog  horse bird  ship  ship  horse car   truck frog \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(\" \".join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight Parameter(6x3x5x5,f32) Quants{-0.114|-0.105|-0.066|-0.006|0.062|0.115}\n",
      "conv1.bias Parameter(6,f32) Quants{-0.112|-0.112|-0.110|-0.081|0.066|0.081}\n",
      "conv2.weight Parameter(16x6x5x5,f32) Quants{-0.082|-0.074|-0.040|-0.000|0.041|0.082}\n",
      "conv2.bias Parameter(16,f32) Quants{-0.067|-0.050|-0.006|0.015|0.027|0.079}\n",
      "fc1.weight Parameter(120x400,f32) Quants{-0.050|-0.045|-0.025|-0.000|0.025|0.050}\n",
      "fc1.bias Parameter(120,f32) Quants{-0.050|-0.045|-0.033|-0.005|0.021|0.046}\n",
      "fc2.weight Parameter(84x120,f32) Quants{-0.091|-0.082|-0.046|-0.000|0.046|0.091}\n",
      "fc2.bias Parameter(84,f32) Quants{-0.089|-0.081|-0.043|0.005|0.043|0.089}\n",
      "fc3.weight Parameter(10x84,f32) Quants{-0.108|-0.095|-0.051|0.000|0.055|0.109}\n",
      "fc3.bias Parameter(10,f32) Quants{-0.098|-0.098|-0.037|0.006|0.024|0.051}\n",
      "\n",
      "Post quantisation\n",
      "=================\n",
      "conv1.weight Parameter(ScaledTensor(3.516209289955441e-06 * Tensor(6x3x5x5,i16) Quants{-32335|-29836|-18705|-1838|17765|32767},\n",
      "             requires_grad=True))\n",
      "conv1.bias Parameter(ScaledTensor(3.4105596569133922e-06 * Tensor(6,i16) Quants{-32767|-32767|-32396|-23671|19426|23606},\n",
      "             requires_grad=True))\n",
      "conv2.weight Parameter(ScaledTensor(2.4914179448387586e-06 * Tensor(16x6x5x5,i16) Quants{-32725|-29627|-16010|-46|16440|32767},\n",
      "             requires_grad=True))\n",
      "conv2.bias Parameter(ScaledTensor(2.4198495793825714e-06 * Tensor(16,i16) Quants{-27673|-20633|-2434|6366|11059|32767},\n",
      "             requires_grad=True))\n",
      "fc1.weight Parameter(ScaledTensor(1.525913035038684e-06 * Tensor(120x400,i16) Quants{-32767|-29522|-16516|-48|16122|32766},\n",
      "             requires_grad=True))\n",
      "fc1.bias Parameter(ScaledTensor(1.5131246300370549e-06 * Tensor(120,i16) Quants{-32767|-29588|-21542|-3128|13853|30420},\n",
      "             requires_grad=True))\n",
      "fc2.weight Parameter(ScaledTensor(2.7858686735271476e-06 * Tensor(84x120,i16) Quants{-32751|-29266|-16611|-160|16652|32767},\n",
      "             requires_grad=True))\n",
      "fc2.bias Parameter(ScaledTensor(2.718601990636671e-06 * Tensor(84,i16) Quants{-32693|-29893|-15974|1714|15748|32767},\n",
      "             requires_grad=True))\n",
      "fc3.weight Parameter(ScaledTensor(3.3175285807374166e-06 * Tensor(10x84,i16) Quants{-32651|-28741|-15383|8|16615|32767},\n",
      "             requires_grad=True))\n",
      "fc3.bias Parameter(ScaledTensor(2.9902007554483134e-06 * Tensor(10,i16) Quants{-32767|-32767|-12403|2015|7867|16940},\n",
      "             requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "for n, p in net.named_parameters():\n",
    "    print(n, tensor_oneline_str(p))\n",
    "\n",
    "d = net.state_dict()\n",
    "for k in d:\n",
    "    d[k] = quantise(d[k], torch.int16)\n",
    "\n",
    "net.load_state_dict(d, assign=True)\n",
    "\n",
    "print()\n",
    "print(\"Post quantisation\")\n",
    "print(\"=================\")\n",
    "for n, p in net.named_parameters():\n",
    "    print(n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8616/3528326397.py:12: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_8616/3528326397.py\", line 14, in <module>\n",
      "    loss = criterion(outputs, labels)\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1179, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/home/awf/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/nn/functional.py\", line 3053, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'LogSoftmaxBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[1;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'LogSoftmaxBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 0:  # print every 2000 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tensor(4x3x32x32,f32) Quants{-1.000|-0.804|-0.404|-0.137|0.122|1.000}',\n",
       " 'Tensor(4,i64) Quants{1|1|2|8|8|9}')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(tensor_oneline_str(x) for x in data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
