{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aAoSgjO1FhVr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING\u001b[0m : Nothing to worry about\n"
          ]
        }
      ],
      "source": [
        "from types import MethodWrapperType, GetSetDescriptorType\n",
        "from typing import *\n",
        "from warnings import warn\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "from icecream import ic\n",
        "\n",
        "import torch\n",
        "from torch import Tensor, tensor, nn\n",
        "from torch.utils._pytree import tree_map\n",
        "\n",
        "# Make warn=print for notebooks, as otherwise outputs are not interleaved correctly\n",
        "from termcolor import colored\n",
        "def warn(msg): print(colored('WARNING', 'yellow'), f': {msg}')\n",
        "warn('Nothing to worry about')\n",
        "\n",
        "def numeric_info(dtype):\n",
        "  return torch.finfo(dtype) if dtype.is_floating_point else torch.iinfo(dtype)\n",
        "\n",
        "def dtype_max(dtype):\n",
        "  return numeric_info(dtype).max\n",
        "\n",
        "def _round(x : Tensor, dtype : torch.dtype) -> Tensor:\n",
        "  \"\"\"\n",
        "  Convert to dtype, rounding if the destination is integer\n",
        "  \"\"\"\n",
        "  if dtype.is_floating_point:\n",
        "    return x.to(dtype)\n",
        "  else:\n",
        "    return torch.round(x).to(dtype)\n",
        "\n",
        "def function_str(func : Callable) -> str:\n",
        "  # https://stackoverflow.com/questions/251464/how-to-get-a-function-name-as-a-string\n",
        "  # for future expansion e.g. properties\n",
        "  if hasattr(func, '__module__'):\n",
        "    return func.__module__ + '.' + func.__qualname__\n",
        "  else:\n",
        "    return func.__qualname__\n",
        "  \n",
        "def type_str(x : type) -> str:\n",
        "   return x.__name__\n",
        "\n",
        "def _uptype(dtype) -> torch.dtype:\n",
        "    \"\"\"\n",
        "    For DTYPE, what is the type in which most arithmetic (e.g. max, abs) is defined?\n",
        "    \"\"\"\n",
        "    GPU = False # Check more accurately, and choose bf16 as appropriate\n",
        "    f16_t = torch.float16 if GPU else torch.float32\n",
        "    map = {\n",
        "        torch.int8: torch.int16,\n",
        "        torch.int16: torch.int16,\n",
        "        torch.float8_e4m3fn: f16_t,\n",
        "        torch.float8_e5m2: f16_t,\n",
        "        torch.float16: f16_t,\n",
        "        torch.float32: torch.float32,\n",
        "    }\n",
        "    return map[dtype]\n",
        "\n",
        "def _to_uptype(t : Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Convert t to its _uptype\n",
        "    \"\"\"\n",
        "    return torch.as_tensor(t, dtype=_uptype(t.dtype))\n",
        "\n",
        "\n",
        "def _maxval(t : Tensor):\n",
        "    \"\"\"\n",
        "    Max absolute value of tensor, returned in its `_uptype`\n",
        "    \"\"\"\n",
        "    return _to_uptype(t).abs().max()\n",
        "\n",
        "torch.set_printoptions(precision=3, threshold=32)\n",
        "\n",
        "# From https://github.com/albanD/subclass_zoo/blob/main/utils.py\n",
        "import contextlib\n",
        "@contextlib.contextmanager\n",
        "def _no_dispatch():\n",
        "    guard = torch._C._DisableTorchDispatch()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        del guard\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(100x300) 10^-9 x Quants{-8.775|-0.429|-0.030|0.000|0.027|5.335}\n",
            "Tensor(100x300) Quants{-0.087|-0.005|-0.000|-0.000|0.000|0.081}\n",
            "Tensor(100x300) Quants{-0.587|-0.044|-0.003|-0.000|0.003|0.665}\n",
            "Tensor(100x300) Quants{-8.363|-0.440|-0.034|-0.000|0.026|7.029}\n",
            "Tensor(100x300) Quants{-82.555|-4.271|-0.308|0.000|0.260|70.899}\n",
            "Tensor(100x300) Quants{-790.282|-45.315|-3.046|-0.000|2.505|697.158}\n",
            "Tensor(100x300) 10^4 x Quants{-1.239|-0.044|-0.003|0.000|0.003|0.609}\n",
            "Tensor(100x300) 10^5 x Quants{-0.937|-0.044|-0.003|0.000|0.003|1.011}\n",
            "Tensor(100x300) 10^11 x Quants{-5.628|-0.450|-0.032|-0.000|0.027|6.434}\n",
            "Tensor(100x300) Quants{-32756|-16489|-6689|23|6519|32707}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def tensor_oneline_str(t):\n",
        "  shape_str = \"x\".join(map(str, t.shape))\n",
        "  quantiles = torch.tensor([0, 0.05,.25, .5, .74, 1.0])\n",
        "  vals = torch.quantile(torch.flatten(t).to(torch.float32), quantiles, interpolation = 'nearest')\n",
        "  if t.dtype.is_floating_point:\n",
        "    # scale down vals\n",
        "    max = torch.floor(torch.log10(vals.abs().max()))\n",
        "    if -2 <= max <= 3:\n",
        "      max = 0\n",
        "    max_scale = 10 ** -max\n",
        "    max_scale_str = f\"10^{int(max)} x \" if max != 0 else \"\"\n",
        "    vals_str = max_scale_str + \"Quants{\" + \"|\".join(f'{v:.3f}' for v in vals*max_scale) + \"}\"\n",
        "  else:\n",
        "    # Assume integer, print as integers\n",
        "    vals_str = \"Quants{\" + \"|\".join(f'{int(v)}' for v in vals) + \"}\"\n",
        "\n",
        "  classname = type(t).__name__\n",
        "\n",
        "  return f'{classname}({shape_str}) {vals_str}'\n",
        "\n",
        "for scale in [-10, -3, -2, -1, 0, 1, 2, 3, 10]:\n",
        "  kurt = 3\n",
        "  print(tensor_oneline_str(torch.randn(100,300)**kurt*(10**scale)))\n",
        "\n",
        "print(tensor_oneline_str((torch.randn(100,300) * 10000).to(torch.int16)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core functionality, no syntactic sugar\n",
        "\n",
        "Define a `ScaleTensorData` object, with the minimal information, and with operations\n",
        "defined cleanly, but without PyTorch Tensor integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM8RBHPwIkV2",
        "outputId": "353862f6-c2e3-4346-b891-25de11eb3550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.], dtype=torch.float16)\n",
            "tensor([1., 2., 3.], dtype=torch.float8_e4m3fn)\n",
            "ScaledTensorData(0.0066964286379516125 * Tensor(3) Quants{144.000|144.000|144.000|288.000|288.000|448.000})\n",
            "wasted_bits(st_data.data)=tensor(0.023) <-- of a max of 8 bits, should be wasting nearly zero\n",
            "ScaledTensorData(0.0066964286379516125 * Tensor(3) Quants{144.000|144.000|144.000|288.000|288.000|448.000})\n",
            "tensor([0.964, 1.928, 3.000], dtype=torch.float16) # <- rounding errors at the high end of the f8 range\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class ScaledTensorData:\n",
        "    data: Tensor\n",
        "    scale: Tensor\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        if not isinstance(self.scale, Tensor):\n",
        "            self.scale = tensor(self.scale)\n",
        "        assert self.scale.dtype == torch.float32\n",
        "        assert self.scale.shape == () \n",
        "        # Possible future expansion to e.g. row-scaled, column-scaled, etc, but\n",
        "        # for now, insist st_scale is a single-element tensor\n",
        "\n",
        "    def _contents_str(self) -> str:\n",
        "        return f\"{self.scale} * {tensor_oneline_str(self.data)}\"\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"ScaledTensorData({self._contents_str()})\"\n",
        "\n",
        "    def to_tensor(self, dtype: torch.dtype = None) -> Tensor:\n",
        "        dtype = dtype or self.scale.dtype\n",
        "        return self.data.to(dtype) * self.scale.to(dtype)\n",
        "\n",
        "    @property\n",
        "    def shape(self) -> torch.Size:\n",
        "        return self.data.shape\n",
        "    \n",
        "    @property\n",
        "    def dtype_max(self):\n",
        "        return dtype_max(self.data.dtype)\n",
        "\n",
        "def st_quantise(x: Tensor, dtype: torch.dtype) -> ScaledTensorData:\n",
        "    \"\"\"\n",
        "    Rescale so that max(|data|) == maxFinite(data.dtype)\n",
        "    \"\"\"\n",
        "    maxval = _maxval(x)\n",
        "    if maxval == 0:\n",
        "        # Tensor is all zeros - set scale to 1\n",
        "        scale = Tensor(1.0, dtype=torch.float32)\n",
        "    else:\n",
        "        # Scale so that largest element is the largest finite value of dtype\n",
        "        scale = maxval / dtype_max(dtype)\n",
        "\n",
        "    return ScaledTensorData(_round(x / scale, dtype), scale)\n",
        "\n",
        "\n",
        "def st_requantise(st: ScaledTensorData) -> ScaledTensorData:\n",
        "    \"\"\"\n",
        "    Rescale so that max(|data|) == maxFinite(data.dtype)\n",
        "\n",
        "    Equivalent to quantise(st.to_tensor(torch.float32)) but avoids the conversion\n",
        "\n",
        "    Returned tensor may share its data with input tensor\n",
        "    \"\"\"\n",
        "    maxdataval = _maxval(st.data)\n",
        "    if maxdataval == 0:\n",
        "        # All zero, reset scale to 1\n",
        "        return ScaledTensorData(st.data, st.scale)\n",
        "    else:\n",
        "         rescale = maxdataval / st.dtype_max\n",
        "    return ScaledTensorData((_to_uptype(st.data) * (1 / rescale)).to(st.data.dtype), st.scale * rescale)\n",
        "\n",
        "\n",
        "def wasted_bits(st_data, maxval = None) -> float:\n",
        "    \"\"\"\n",
        "    By how much is tensor `st_data` not using the full dynamic range of its dtype?\n",
        "\n",
        "    E.g.\n",
        "       t = torch.tensor([1,2,-16], dtype=torch.int8)\n",
        "\n",
        "    Is using only 5 (4 + sign) of the available 8 bits.\n",
        "    Therefore \n",
        "       wasted_bits(t) == 3 == 8-3\n",
        "\n",
        "    Optional argument maxval, if the maximum value in the tensor has already \n",
        "    been computed, perhaps with a higher-accuracy method (e.g. pre-rounding)\n",
        "    \"\"\"\n",
        "    if maxval is None:\n",
        "        maxval = _maxval(st_data)\n",
        "\n",
        "    maxval = maxval.to(st_data.dtype)\n",
        "    dtype_bits = numeric_info(st_data.dtype).bits\n",
        "    if maxval == 0:\n",
        "        # All values zero -> all bits are wasted\n",
        "        return dtype_bits\n",
        "    \n",
        "    # Otherwise, how many bits is maxval using.\n",
        "    if st_data.dtype.is_floating_point:\n",
        "      # Convert maxval to integer of the same bitwidth\n",
        "      ints = {\n",
        "          8: torch.int8,\n",
        "          16: torch.int16,\n",
        "          32: torch.int32\n",
        "      }\n",
        "      maxval = maxval.view(ints[dtype_bits])\n",
        "\n",
        "    # Assuming a signed type, max usable bits are dtype_bits-1\n",
        "    return dtype_bits-1 - torch.log2(maxval)\n",
        "\n",
        "def test_wasted_bits():\n",
        "    t = torch.tensor([1,2,-16], dtype=torch.int8)\n",
        "    assert wasted_bits(t) == 3\n",
        "test_wasted_bits()\n",
        "\n",
        "### Quick testing\n",
        "f16 = tensor([1, 2, 3], dtype=torch.float16)\n",
        "f8_t = torch.float8_e4m3fn\n",
        "print(f16)\n",
        "print(f16.to(f8_t))\n",
        "\n",
        "st_data = st_quantise(f16, f8_t)\n",
        "\n",
        "# TODO: wasting a lot more bits at the low end here -- range of 144->448.\n",
        "print(st_data)\n",
        "print(f'{wasted_bits(st_data.data)=} <-- of a max of {numeric_info(f8_t).bits} bits, should be wasting nearly zero')\n",
        "print(st_requantise(st_data))\n",
        "print(st_data.to_tensor(f16.dtype), \"# <- rounding errors at the high end of the f8 range\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Operations, without syntactic sugar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| st1: ScaledTensorData(0.5 * Tensor() Quants{32|32|32|32|32|32})\n",
            "ic| st2: ScaledTensorData(0.25 * Tensor() Quants{64|64|64|64|64|"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "64})\n",
            "ic| st_add(st1, st2): ScaledTensorData(0.75 * Tensor() Quants{42|42|42|42|42|42})\n",
            "ic| f32(st_add(st1, st2)): tensor(31.500)\n",
            "ic| f32(st1) + f32(st2): tensor(32.)\n",
            "ic| st3: ScaledTensorData(0.021498018875718117 * Tensor(2x32) Quants{-103|-79|-19|16|33|127})\n",
            "ic| st4: ScaledTensorData(0.016995994374155998 * Tensor(32x3) Quants{-119|-90|-38|13|48|127})\n",
            "ic| st_matmul(st3, st4): ScaledTensorData(1.4849052429199219 * Tensor(2x3) Quants{-4|-4|-3|0|2|3})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING\u001b[0m : st_matmul: Very bad maxval estimate 188.5829620361328 vs 6.242886066436768, 30.2x too large - will lose at least 5.0 bits of precision\n",
            "\u001b[33mWARNING\u001b[0m : st_matmul: Very bad maxval estimate 188.5829620361328 vs 6.242886066436768, 30.2x too large - will lose at least 5.0 bits of precision\n",
            "f32(st_matmul(st3, st4)) = tensor([[ 0.000,  1.485,  4.455],\n",
            "        [ 2.970, -4.455, -5.940]]) <-- quantized\n",
            "f32(st3) @ f32(st4) = tensor([[-0.688,  2.254,  5.069],\n",
            "        [ 4.033, -5.583, -6.243]]) <-- intermediate\n",
            "t3 @ t4 = tensor([[-0.611,  2.142,  5.089],\n",
            "        [ 4.000, -5.604, -6.256]]) <-- exact\n"
          ]
        }
      ],
      "source": [
        "# Ops\n",
        "# - Use worst-case scaling rules (no overflow!)\n",
        "# - Placeholder impl (fast impl requires custom kernels)\n",
        "# - No autograd support\n",
        "\n",
        "def st_add(a: ScaledTensorData, b: ScaledTensorData) -> ScaledTensorData:\n",
        "    out_dtype = a.data.dtype\n",
        "    scale = a.scale + b.scale\n",
        "    data = (_to_uptype(a.data) * (a.scale / scale) + _to_uptype(b.data) * (b.scale / scale)).to(out_dtype)\n",
        "    return ScaledTensorData(data, scale)\n",
        "\n",
        "\n",
        "def st_matmul(a: ScaledTensorData, b: ScaledTensorData, debug = True) -> ScaledTensorData:\n",
        "    assert a.data.dtype == b.data.dtype\n",
        "    in_dtype = a.data.dtype\n",
        "    out_dtype = a.data.dtype\n",
        "\n",
        "    a_maxval = a.scale * a.dtype_max\n",
        "    b_maxval = b.scale * b.dtype_max\n",
        "\n",
        "    # Predicted maxval for NxK @ KxM\n",
        "    K = a.shape[-1]\n",
        "    out_maxval_estimate = a_maxval * b_maxval * K\n",
        "\n",
        "    out_scale = out_maxval_estimate / dtype_max(out_dtype) \n",
        "\n",
        "    # Derivation of matmul scale factors:\n",
        "    # (ad * as) @ (bd * bs) = (ad @ bd) * (as * bs)\n",
        "    #                       = (ad @ bd) * (as * bs / os * os)\n",
        "    #                       = (ad @ bd * as * bs / os) * os\n",
        "    #                       = (ad @ bd * rat) * os\n",
        "    #                         where rat = as * bs / os\n",
        "    #                       = (ad * sqrt(rat)) @ (bd * sqrt(rat)) * os\n",
        "\n",
        "    rat = a.scale * b.scale / out_scale\n",
        "\n",
        "    if numeric_info(in_dtype).bits < numeric_info(_uptype(in_dtype)).bits:\n",
        "        # Assume low-precision muls will accumulate to uptype, so won't overflow\n",
        "        # to simulate this on cpu, uptype before the matmul;\n",
        "        # on appropriate hardware (e.g. graphcore, h100), call the special matmul\n",
        "        adbd = _to_uptype(a.data) @ _to_uptype(b.data)\n",
        "        if debug:\n",
        "            out_maxval = _maxval(adbd) * (a.scale * b.scale)\n",
        "        out_data = adbd * rat\n",
        "        out_data = out_data.to(out_dtype)\n",
        "    else:\n",
        "        # Inputs are in 16+ bits, and we know the products will certainly \n",
        "        # overflow, as they are scaled to dtype_max, so downscale before multiplying \n",
        "        sqrt_rat = torch.sqrt(rat)\n",
        "        a_down = _to_uptype(a.data) * sqrt_rat\n",
        "        b_down = _to_uptype(b.data) * sqrt_rat\n",
        "        out_data = a_down @ b_down\n",
        "        if debug:\n",
        "            out_maxval = _maxval(out_data) * out_scale\n",
        "        out_data = out_data.to(out_dtype)\n",
        "\n",
        "    # debug check how bad out_maxval_estimate was\n",
        "    if debug:\n",
        "        assert out_maxval_estimate > out_maxval # Should always be an upper bound\n",
        "        wasted = wasted_bits(out_data)\n",
        "        if wasted > numeric_info(out_dtype).bits/2:\n",
        "            warn(f'st_matmul: Very bad maxval estimate {out_maxval_estimate} vs {out_maxval}, {out_maxval_estimate/out_maxval:.1f}x too large - will lose at least {wasted} bits of precision')\n",
        "\n",
        "        if _maxval(out_data) == 0:\n",
        "            raise ValueError(\"All-data zero - rerun with debug and view st_matmul: WARNING above\")\n",
        "\n",
        "    return ScaledTensorData(out_data, out_scale)\n",
        "\n",
        "\n",
        "def st_relu(a: ScaledTensorData) -> ScaledTensorData:\n",
        "    data = nn.functional.relu(a.data).to(a.data.dtype)\n",
        "    return ScaledTensorData(data, a.scale)\n",
        "\n",
        "# Check operators behave sensibly\n",
        "st1 = ScaledTensorData(tensor(32, dtype=torch.int8), 0.5)\n",
        "st2 = ScaledTensorData(tensor(64, dtype=torch.int8), 0.25)\n",
        "\n",
        "f32 = lambda x: x.to_tensor(torch.float32) if isinstance(x, ScaledTensorData) else x.to(dtype=torch.float32)\n",
        "\n",
        "\n",
        "ic(st1)\n",
        "ic(st2)\n",
        "ic(st_add(st1, st2))\n",
        "ic(f32(st_add(st1, st2)) )\n",
        "ic(f32(st1) + f32(st2))\n",
        "\n",
        "hidden = 32\n",
        "t3 = torch.randn(2, hidden)\n",
        "t4 = torch.randn(hidden, 3)\n",
        "st3 = st_quantise(t3, torch.int8)\n",
        "st4 = st_quantise(t4, torch.int8)\n",
        "ic(st3)\n",
        "ic(st4)\n",
        "ic(st_matmul(st3, st4))\n",
        "print(f'{f32(st_matmul(st3, st4)) = } <-- quantized')\n",
        "print(f'{f32(st3) @ f32(st4) = } <-- intermediate')\n",
        "print(f'{t3 @ t4 = } <-- exact')\n",
        "\n",
        "# st5 = st_quantise(tensor([-2, 3, -0.06, 4]), torch.int8)\n",
        "# ic(st5, f32(st5))\n",
        "# rt5 = st_relu(st5)\n",
        "# ic(rt5, f32(rt5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "### [Aside: Possibly surprising rounding]\n",
        "\n",
        "# print('Starting point: ', tensor([-2, 0.09, 4]))\n",
        "\n",
        "# st5 = st_quantise(tensor([-2, 0.09, 4]), torch.int8)\n",
        "# print(f'{st5=} {f32(st5)=} <-- 0.0900 input rounds to 0.0630')\n",
        "\n",
        "# print('So, put in 0.0630 to begin with')\n",
        "# st5 = st_quantise(tensor([-2, 0.0630, 4]), torch.int8) \n",
        "# print(f'{st5=} {f32(st5)=} <-- great, 0.0630 rounds to 0.0630')\n",
        "\n",
        "# print('But now, put in 0.06')\n",
        "# st5 = st_quantise(tensor([-2, 0.06, 4]), torch.int8) \n",
        "# print(f'{st5=} {f32(st5)=} <-- Eh, 0.06 rounds down to 0.0315?')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sugar hit: override Tensor operations in subclass\n",
        "\n",
        "We define a `ScaledTensor` object that behaves like a torch Tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.], dtype=torch.float16)\n",
            "st.shape=torch.Size([3])\n",
            "st=ScaledTensor(0.023622047156095505 * Tensor(3) Quants{42|42|42|85|85|127})\n",
            "requantise(st)=ScaledTensor(0.023622047156095505 * Tensor(3) Quants{42|42|42|85|85|127})\n",
            "Rounding errors at the high end of the f8 range: tensor([0.992, 2.008, 3.000], dtype=torch.float16)\n",
            "Reshaped: ScaledTensor(0.023622047156095505 * Tensor(3) Quants{42|42|42|85|85|127})\n",
            "\u001b[33mExpect a warning...\u001b[0m\n",
            "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for torch._ops.aten.aten::add.Tensor@(ScaledTensor,int)\n",
            "Addition, but note warning above about \"Upcasting to float32\", so prints as a normal tensor: tensor([2.992, 4.008, 5.000])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ScaledTensor(0.023622047156095505 * Tensor(3) Quants{42|42|42|85|85|127},\n",
              "             requires_grad=True)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# See https://pytorch.org/docs/stable/notes/extending.html#extending-torch-with-a-tensor-like-type\n",
        "# TODO: check\n",
        "#   - _make_wrapper_subclass THPVariable_make_wrapper_subclass https://github.com/pytorch/pytorch/pull/65340\n",
        "#   - _make_subclass\n",
        "#   - See [Note] at https://github.com/albanD/subclass_zoo/blob/276d2f005484d80ebbcd9e274d79685adb6a1da2/negative_tensor.py#L24\n",
        "#     - Doesn't apply in this case as we are composing, not deriving?\n",
        "# Looking at https://github.com/albanD/subclass_zoo\n",
        "#   - trivial_tensor doesn't do autograd\n",
        "#   - inner_autograd_tensor explicitly defers to its `elem`, which is incorrect\n",
        "# In PyTorch core\n",
        "#   - MaskedTensor\n",
        "st = None\n",
        "\n",
        "\n",
        "class ScaledTensor(Tensor):\n",
        "    @staticmethod\n",
        "    def __new__(cls, st: ScaledTensorData, *, requires_grad=None):\n",
        "        return torch.Tensor._make_wrapper_subclass(cls, st.data.shape, dtype=st.scale.dtype)\n",
        "\n",
        "    def __init__(self, st_data: ScaledTensorData):\n",
        "        self.st = st_data\n",
        "\n",
        "    def tolist(self):\n",
        "        return self.st.to_tensor().tolist()\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        # See https://github.com/pytorch/pytorch/issues/73665\n",
        "        with _no_dispatch():\n",
        "            return super().__repr__(tensor_contents=str(self.st._contents_str()))\n",
        "\n",
        "    # @property\n",
        "    # def shape(self) -> torch.Size:\n",
        "    #     return self.st.data.shape\n",
        "\n",
        "    # def size(self, *args, **kwargs) -> torch.Size:\n",
        "    #     return self.st.data.size(*args, **kwargs)\n",
        "\n",
        "    # def sizes(self):\n",
        "    #     return self.st.data.sizes()\n",
        "\n",
        "    # def numel(self) -> int:\n",
        "    #     return self.st.data.numel()\n",
        "\n",
        "    # def detach(self) -> Tensor:\n",
        "    #     return ScaledTensor(ScaledTensorData(self.st.data.detach(), self.st.scale))\n",
        "\n",
        "    @classmethod\n",
        "    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n",
        "        kwargs = kwargs or {}\n",
        "\n",
        "        # Have we registered a handler?\n",
        "        # 1. Turn \"func\" into something we can lookup in the HANDLED_FUNCTIONS dict.\n",
        "        func_to_lookup = func\n",
        "        # if isinstance(func, MethodWrapperType): # TODO: remove this logic - sems not to be needed for dispatch\n",
        "        #     # For methods, e.g. Tensor.T, we want to use that name, so unpack it\n",
        "        #     assert func.__qualname__ == 'getset_descriptor.__get__' # This is the only case tested so far\n",
        "        #     func_to_lookup = func.__self__\n",
        "        # else:\n",
        "        #     # Probably other special cases here\n",
        "        #     pass\n",
        "\n",
        "        if func_to_lookup in cls.DELEGATE_TO_SUPER:\n",
        "            # 2. Delegate to super - just pass the args upwards\n",
        "            if func_to_lookup in cls.HANDLED_FUNCTIONS:  # Keep it unsurprising\n",
        "                warn(f\"{func} in both DELEGATE_TO_SUPER and HANDLED_FUNCTIONS\")\n",
        "\n",
        "            with _no_dispatch():\n",
        "                ret = func(*args, *kwargs)\n",
        "            # sanity check - if it returned a Tensor, maybe not such a good idea to silently delegate upwards\n",
        "            if isinstance(ret, Tensor):\n",
        "                warn(f\"Delegated {func} to super, but it returned a Tensor\")\n",
        "            return ret\n",
        "\n",
        "        # 3. Is this one we handle?\n",
        "        extra_msg = ''\n",
        "        if func_to_lookup in cls.HANDLED_FUNCTIONS:\n",
        "            # Call the handler\n",
        "            handler = cls.HANDLED_FUNCTIONS[func_to_lookup]\n",
        "            ret = handler(func, *args, **kwargs)\n",
        "\n",
        "            # handler may return \"NotImplemented\" to tell us to run the fallback\n",
        "            if ret != NotImplemented:\n",
        "                return ret\n",
        "\n",
        "            extra_msg = f' -- [Handler {function_str(handler)} returned NotImplemented]'\n",
        "            # Otherwise drop through to 4. Fallback\n",
        "\n",
        "        # 4. Fallback: Convert to float32 and call func\n",
        "        func_str = f'{function_str(func_to_lookup)}@({\",\".join(type_str(type(x)) for x in args)})'\n",
        "        warn(f\"ScaledTensor.__torch_dispatch__: Upcasting to float32 for {func_str}\" + extra_msg)\n",
        "\n",
        "        def to_tensor_if_scaled(t: Any) -> Tensor:\n",
        "            if isinstance(t, ScaledTensor):  # TODO: ScaledTensor -> cls\n",
        "                return t.st.to_tensor(torch.float32)\n",
        "            else:\n",
        "                return t\n",
        "\n",
        "        new_args = tree_map(to_tensor_if_scaled, args)\n",
        "        new_kwargs = tree_map(to_tensor_if_scaled, kwargs)\n",
        "\n",
        "        with _no_dispatch():\n",
        "            return func(*new_args, **new_kwargs)\n",
        "\n",
        "    # __torch_function__ = __torch_dispatch__\n",
        "    __torch_function__ = torch._C._disabled_torch_function_impl\n",
        "\n",
        "\n",
        "ScaledTensor.DELEGATE_TO_SUPER = {\n",
        "    torch.ops.aten.is_same_size.default\n",
        "}\n",
        "\n",
        "ScaledTensor.HANDLED_FUNCTIONS = {}\n",
        "\n",
        "\n",
        "def tensor_subclass_override(cls, funcs):\n",
        "    \"\"\"\n",
        "    Decorator to add an implementation of an operation to a Tensor subclass\n",
        "\n",
        "    @tensor_subclass_override(MySubclass, torch.ops.aten.view.default)\n",
        "    def _(func, *args, *kwargs):\n",
        "      print(f'Calling wrapped {func} with {len(args)} args)\n",
        "      with _no_dispatch():\n",
        "        return func(*args, *kwargs)\n",
        "\n",
        "    Calling the implementation \"_\" allows this decorator to overwrite the name with\n",
        "    a more sensible one \"@torch_function_override(MySubclass, torch.ops.aten.view.default)\"\n",
        "    but of course you can just call it \"MySubclass_impl_view\" or \"foo42\" if you prefer.\n",
        "    \"\"\"\n",
        "    funcs = funcs if isinstance(funcs, tuple) else (funcs,)\n",
        "    funcs_str = \",\".join(map(function_str, funcs))\n",
        "\n",
        "    def doit(impl):\n",
        "        # Override impl name if it was just \"_\"\n",
        "        if hasattr(impl, \"__name__\") and impl.__name__ == \"_\":\n",
        "            impl.__name__ = f\"@torch_function_override({cls.__name__}, {funcs_str})\"\n",
        "            if impl.__qualname__ != \"_\":\n",
        "                print(f\"torch_function_override: NOTE: {impl.__qualname__} not overridden\")\n",
        "\n",
        "            if impl.__qualname__ == \"_\":\n",
        "                impl.__qualname__ = f\"torch_function_override({cls.__name__}, {funcs_str})\"\n",
        "\n",
        "        # Record handler in the dictionary for each func\n",
        "        for func in funcs:\n",
        "            cls.HANDLED_FUNCTIONS[func] = impl\n",
        "\n",
        "    return doit\n",
        "\n",
        "\n",
        "@tensor_subclass_override(ScaledTensor, (torch.ops.aten.view.default, torch.ops.aten.permute.default))\n",
        "def passthru_to_data(func, t_self, *args, **kwargs):\n",
        "    new_data = func(t_self.st.data, *args, **kwargs)\n",
        "    return ScaledTensor(ScaledTensorData(new_data, t_self.st.scale))\n",
        "\n",
        "@tensor_subclass_override(ScaledTensor, torch.ops.aten.detach.default)\n",
        "def _(func_, a:Tensor) -> Tensor:\n",
        "    return ScaledTensor(a.st)\n",
        "\n",
        "\n",
        "# Forward the ScaledTensorData ops above on the ScaledTensor type\n",
        "def quantise(x: Tensor, dtype: torch.dtype) -> ScaledTensor:\n",
        "    return ScaledTensor(st_quantise(x, dtype))\n",
        "\n",
        "\n",
        "def requantise(st) -> ScaledTensor:\n",
        "    return ScaledTensor(st_requantise(st.st))\n",
        "\n",
        "\n",
        "# Check basic to/from Subclass\n",
        "f16 = tensor([1, 2, 3], dtype=torch.float16)\n",
        "f8_t = torch.int8\n",
        "print(f16)\n",
        "\n",
        "st = quantise(f16, f8_t)\n",
        "\n",
        "print(f\"{st.shape=}\")\n",
        "assert st.shape == f16.shape\n",
        "s = str(st)\n",
        "print(f\"{st=}\")\n",
        "\n",
        "print(f\"{requantise(st)=}\")\n",
        "print(\"Rounding errors at the high end of the f8 range:\", st.st.to_tensor(f16.dtype))\n",
        "\n",
        "print(\"Reshaped:\", st.T)\n",
        "\n",
        "print(colored('Expect a warning...', 'yellow'))\n",
        "print('Addition, but note warning above about \"Upcasting to float32\", so prints as a normal tensor:', st + 2)\n",
        "\n",
        "st.requires_grad_(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now overrides work, but just punt up to f32 for all ops\n",
        "\n",
        "We will do some adds/multiplies etc, and note that the torch function\n",
        "implementation issues a sequence of \"WARNING: Upcasting to float32\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mExpect four warnings...\u001b[0m\n",
            "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for torch._ops.aten.aten::add.Tensor@(ScaledTensor,int)\n",
            "tensor([2.992, 4.008, 5.000], grad_fn=<AddBackward0>)\n",
            "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for torch._ops.aten.aten::mul.Tensor@(ScaledTensor,int)\n",
            "tensor([1.984, 4.016, 6.000], grad_fn=<MulBackward0>)\n",
            "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for torch._ops.aten.aten::add.Tensor@(ScaledTensor,ScaledTensor)\n",
            "tensor([1.984, 4.016, 6.000], grad_fn=<AddBackward0>)\n",
            "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for torch._ops.aten.aten::mm@(ScaledTensor,ScaledTensor)\n",
            "f32(st3 @ st4)       = tensor([[60000., 60000., 60000., 60000.],\n",
            "        [60000., 60000., 60000., 60000.]]) <-- should be ~21000 quantized, but was done in f32 so exact\n",
            "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for torch._ops.aten.aten::mm@(ScaledTensor,ScaledTensor)\n",
            "f32(st3) @ f32(st4)  = tensor([[60000., 60000., 60000., 60000.],\n",
            "        [60000., 60000., 60000., 60000.]]) <-- 60000 exact\n",
            "Reshaped, but upscaled to f32: ScaledTensor(0.04724409431219101 * Tensor(3x2) Quants{21|21|42|64|106|127})\n"
          ]
        }
      ],
      "source": [
        "print(colored('Expect four warnings...', 'yellow'))\n",
        "\n",
        "print(st + 2)\n",
        "print(2 * st)\n",
        "print(st + st)\n",
        "\n",
        "st3 = quantise(torch.full((2, 3), 100.0), torch.int8)\n",
        "st4 = quantise(torch.full((3, 4), 200.0), torch.int8)\n",
        "\n",
        "print(f'{f32(st3 @ st4)       = } <-- should be ~21000 quantized, but was done in f32 so exact')\n",
        "print(f'{f32(st3) @ f32(st4)  = } <-- 60000 exact')\n",
        "\n",
        "m16 = quantise(tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float16), f8_t)\n",
        "\n",
        "print('Reshaped, but upscaled to f32:', m16.T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autograd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "qx=ScaledTensor(0.008661417290568352 * Tensor(1) Quants{127|127|127|127|127|127},\n",
            "             requires_grad=True)\n",
            "\u001b[33mWARNING\u001b[0m : ScaledTensor.__torch_dispatch__: Upcasting to float32 for torch._ops.aten.aten::add.Tensor@(ScaledTensor,int) -- [Handler __main__.torch_function_override(ScaledTensor, torch._ops.aten.aten::add.Tensor) returned NotImplemented]\n",
            "qy=tensor([4.200], grad_fn=<AddBackward0>)\n",
            "qx.grad=ScaledTensor(1.732283635647036e-05 * Tensor(1) Quants{127|127|127|127|127|127})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class ScaledTensor_add(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, a, b):\n",
        "        assert isinstance(a, ScaledTensor) and isinstance(b, ScaledTensor)\n",
        "        return ScaledTensor(st_add(a.st, b.st))\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dout):\n",
        "        return dout, dout\n",
        "\n",
        "@tensor_subclass_override(ScaledTensor, torch.ops.aten.add.Tensor)\n",
        "def _(func_, a:Tensor, b: Tensor) -> Tensor:\n",
        "    if not (isinstance(a, ScaledTensor) and isinstance(b, ScaledTensor)):\n",
        "        return NotImplemented\n",
        "\n",
        "    return ScaledTensor_add.apply(a, b)\n",
        "\n",
        "x = tensor([1.1, 2.2, 3.3])\n",
        "\n",
        "qx = quantise(x, torch.int8)\n",
        "qx.requires_grad_(True)\n",
        "\n",
        "print(f'{qx=}')\n",
        "\n",
        "\n",
        "qy = qx + qx + 2\n",
        "print(f'{qy=}')\n",
        "\n",
        "dx = x * .001 # no actual need to do 0.001\n",
        "qdx = quantise(dx, torch.int8)\n",
        "qy.backward(qdx)\n",
        "print(f'{qx.grad=}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'torch' has no attribute 'Function'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[57], line 2\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaledTensor_matmul\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFunction\u001b[49m):\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Note that both forward and backward are @staticmethods\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# bias is an optional argument\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, a, b):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, ScaledTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, ScaledTensor)\n",
            "File \u001b[0;32m~/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/__init__.py:1833\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 1833\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'Function'"
          ]
        }
      ],
      "source": [
        "\n",
        "class ScaledTensor_matmul(torch.Function):\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    # bias is an optional argument\n",
        "    def forward(ctx, a, b):\n",
        "        assert isinstance(a, ScaledTensor) and isinstance(b, ScaledTensor)\n",
        "        ctx.save_for_backward(input, a, b)\n",
        "        st_out = st_add(a.st, b.st)\n",
        "        return ScaledTensor(st_out)\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    @staticmethod\n",
        "    def backward(ctx, dout):\n",
        "        a, b = ctx.saved_tensors\n",
        "        da = dout\n",
        "\n",
        "        da = grad_output.mm(weight)\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_weight = grad_output.t().mm(input)\n",
        "        if bias is not None and ctx.needs_input_grad[2]:\n",
        "            grad_bias = grad_output.sum(0)\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias\n",
        "    \n",
        "@tensor_subclass_override(ScaledTensor, Tensor.matmul)\n",
        "def _(a:Tensor, b: Tensor) -> Tensor:\n",
        "    if not (isinstance(a, ScaledTensor) and isinstance(b, ScaledTensor)):\n",
        "        return NotImplemented\n",
        "\n",
        "    return ScaledTensor(st_matmul(a.st, b.st))\n",
        "\n",
        "@tensor_subclass_override(ScaledTensor, Tensor.to)\n",
        "def _(a:Tensor, dtype:torch.dtype) -> Tensor:\n",
        "    assert isinstance(a, ScaledTensor)\n",
        "    return a.st.to_tensor(dtype)\n",
        "\n",
        "@tensor_subclass_override(ScaledTensor, (Tensor.relu, nn.functional.relu))\n",
        "def _(a:Tensor, inplace = False) -> Tensor:\n",
        "    assert isinstance(a, ScaledTensor)\n",
        "    assert not inplace\n",
        "    return ScaledTensor(st_relu(a.st))\n",
        "\n",
        "## Override reshaping operations\n",
        "def reshape_passthru(func, a:Tensor, *args, **kwargs) -> Tensor:\n",
        "    assert isinstance(a, ScaledTensor)\n",
        "    func_to_call = func.__get__ if isinstance(func, GetSetDescriptorType) else func\n",
        "    st_data = func_to_call(a.st.data, *args, **kwargs)\n",
        "    return ScaledTensor(ScaledTensorData(st_data, a.st.scale))\n",
        "\n",
        "for func in (Tensor.T, Tensor.flatten, torch.flatten):\n",
        "    print(function_str(func))\n",
        "    tensor_subclass_override(ScaledTensor, func)(partial(reshape_passthru, func))\n",
        "\n",
        "ic(st3)\n",
        "ic(st3.T)\n",
        "ic(st3.flatten())\n",
        "ic(torch.flatten(st3))\n",
        "\n",
        "ic(st4)\n",
        "ic(st3 @ st4 )\n",
        "print(f'{f32(st3 @ st4)       = } <-- ~21000 quantized')\n",
        "print(f'{f32(st3) @ f32(st4)  = } <-- 60000 exact')\n",
        "\n",
        "print(nn.functional.relu(st))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now, make autograd work..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<ScaledTensor(nan, requires_grad=True) where nan=ScaledTensor(0.025984251871705055 * Tensor(3) Quants{42|46|63|85|105|127})>\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.grad_fn@(ScaledTensor)\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "x = quantise(tensor([1.1, 2.2, 3.3]), torch.int8)\n",
        "\n",
        "x.requires_grad_(True)\n",
        "print(x)\n",
        "\n",
        "y = x + x\n",
        "\n",
        "print(y.grad_fn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# And in a network..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtrOg8FMpDb3",
        "outputId": "cd7bc783-0567-4c37-908d-56ba2ec2e1a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| self.W1: <ScaledTensor(nan) where nan=ScaledTensor(0.03887427598237991 * Tensor(4096x1024) Quants{-127|-42|-17|0|17|122})>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "st_matmul: WARNING: Very bad maxval estimate 22736.822265625 vs 45.10857391357422, 504.0x too large - will lose at least 8 bits of precision\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "All-data zero - rerun with debug and view st_matmul: WARNING above",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m module \u001b[38;5;241m=\u001b[39m FFN(hidden_size, storage_type)\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m quantise(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, hidden_size), storage_type)\n\u001b[0;32m---> 30\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
            "File \u001b[0;32m~/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[9], line 13\u001b[0m, in \u001b[0;36mFFN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Union[Tensor, ScaledTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, ScaledTensor]:\n\u001b[0;32m---> 13\u001b[0m     y \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW0\u001b[49m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, ScaledTensor):\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwasted bits #1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwasted_bits(y)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[6], line 47\u001b[0m, in \u001b[0;36mScaledTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# 2. Is this one we handle?\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func_to_lookup \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mHANDLED_FUNCTIONS:\n\u001b[0;32m---> 47\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHANDLED_FUNCTIONS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc_to_lookup\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m   \u001b[38;5;66;03m# handler may return \"NotImplemented\" to tell us to run the fallback\u001b[39;00m\n\u001b[1;32m     49\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mNotImplemented\u001b[39m:\n",
            "Cell \u001b[0;32mIn[8], line 38\u001b[0m, in \u001b[0;36m_\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(a, ScaledTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, ScaledTensor)):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ScaledTensor(\u001b[43mst_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mst\u001b[49m\u001b[43m)\u001b[49m)\n",
            "Cell \u001b[0;32mIn[4], line 65\u001b[0m, in \u001b[0;36mst_matmul\u001b[0;34m(a, b, debug)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mst_matmul: WARNING: Very bad maxval estimate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_maxval_estimate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_maxval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_maxval_estimate\u001b[38;5;241m/\u001b[39mout_maxval\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx too large - will lose at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwasted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bits of precision\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _maxval(out_data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll-data zero - rerun with debug and view st_matmul: WARNING above\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ScaledTensorData(out_data, out_scale)\n",
            "\u001b[0;31mValueError\u001b[0m: All-data zero - rerun with debug and view st_matmul: WARNING above"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "from icecream import ic\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, hidden_size: int, dtype: torch.dtype):\n",
        "        super().__init__()\n",
        "        q = lambda x: quantise(x, dtype)\n",
        "        self.W0 = q(torch.randn(hidden_size, 4*hidden_size))\n",
        "        self.W1 = q(torch.randn(4*hidden_size, hidden_size))\n",
        "        ic(self.W1)\n",
        "\n",
        "    def forward(self, x: Union[Tensor, ScaledTensor]) -> Union[Tensor, ScaledTensor]:\n",
        "        y = nn.functional.relu(x @ self.W0)\n",
        "\n",
        "        if isinstance(y, ScaledTensor):\n",
        "            print(f\"wasted bits #1: {wasted_bits(y)}\")\n",
        "            y = requantise(y)\n",
        "\n",
        "        y = y @ self.W1\n",
        "\n",
        "        if isinstance(y, ScaledTensor):\n",
        "            print(f\"wasted bits #2: {wasted_bits(y)}\")\n",
        "\n",
        "        return y\n",
        "\n",
        "hidden_size = 1024\n",
        "storage_type = torch.int8\n",
        "module = FFN(hidden_size, storage_type)\n",
        "x = quantise(torch.randn(10, hidden_size), storage_type)\n",
        "result = module(x)\n",
        "print()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAACwCAYAAACviAzDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABH1ElEQVR4nO2de5Ac1XX/T3fPe2dnZh/aXa12VxJCWOJpLCGxxpU4thJMXBgCv8SmSJBtKi4SyQH0qxjLDuQXJ0RUUhWwUzKupAg4FWMcXAYnOIYQgSE4eiPJCCEhofdjd7W7msfOzqu77+8PwtxzTmt6d8VqVqs9n6qt6p7b03379u07d+8553sMpZQCQRAEQRCEOmFOdQUEQRAEQZhZyORDEARBEIS6IpMPQRAEQRDqikw+BEEQBEGoKzL5EARBEAShrsjkQxAEQRCEuiKTD0EQBEEQ6opMPgRBEARBqCsy+RAEQRAEoa7I5EMQBEEQhLpy3iYf69evh3nz5kEkEoHly5fDli1bztelBEEQBEGYRhjnI7fLj370I7jrrrvge9/7Hixfvhwee+wxePbZZ2Hfvn3Q1tbm+13XdeHkyZPQ2NgIhmFMdtUEQRAEQTgPKKUgl8tBZ2cnmOYYaxvqPLBs2TK1atWq6r7jOKqzs1OtW7duzO8eO3ZMAYD8yZ/8yZ/8yZ/8TcO/Y8eOjflbH4BJplwuw/bt22Ht2rXVz0zThBUrVsDGjRs9x5dKJSiVStV99b8LMffffz+Ew+HJrp4gCIIgCOeBUqkEjz76KDQ2No557KRPPgYHB8FxHGhvbyeft7e3w969ez3Hr1u3Dv7iL/7C83k4HJbJhyAIgiBMM8bjMjHl0S5r166FTCZT/Tt27NhUV0kQBEEQhPPIpK98tLa2gmVZ0N/fTz7v7++Hjo4Oz/GywiEIgiAIM4tJX/kIhUKwZMkS2LBhQ/Uz13Vhw4YN0NvbO9mXEwRBEARhmjHpKx8AAGvWrIGVK1fC0qVLYdmyZfDYY49BPp+HL33pSx/63Gv/35+TfdupVLfHDhpGcy3FbFITiOq1QV9IQe2LekrQB4pfnx9s4E2fyrnsNHwfb7NruK466/b756H7Ljqx69CL2GjfYdewbYddU+8/++R3oBZf+cq97Hv0mjiMi7cObi+DhXt5WxJV2HA9pbXhDwx9dwJ9CfcD/uy8x6LnpfjzQce5tAJK0TZw0POy2UXxeRW7hmLHOrhPsDJs8/3JD58AP/5s7f+rblfKRVro2uiktC8ZBq+f3ufPHQy9r8DyrY/yaQPL0t/12LV9nrvp6S743WPPwMXtyu6R9Tt8LK8rfqfLDm0Pv/fSdmiZg461bZuU8Tb44TNPQS3eMpHMAqurabI+C/j9ps/LRPcVNGlZLET3o5b+fQgEQrRCCl+Dtg+vD75vx+H9EI03Bn/QtfskH9cVGUPoeXAz8zb3jut4m52HHMfGcZseGwgEa16zKX8IPiznZfLx+c9/Hk6fPg0PPfQQ9PX1wUc/+lF48cUXPU6ogiAIgiDMPM7L5AMAYPXq1bB69erzdXpBEARBEKYpUx7tIgiCIAjCzOK8rXycL8LMTmZa2i7l53/xPkaN7bGg5w1iIxu75Lgl4bnPBy/2uRVc5DDzNTPxET8Cbv9zkV2Ruxu4zP8B+xE44AOvAKufPdYj+gDuf+HZx5u0Lal9dCJ+HB8C8tzHe5O07mP3X/w9vo/9kChKOWwf2aHZibCp2+MH5NPXvc9g/P/X4DrY5RIpGx4cqG63tjSxL9IKHT92vLpdLNHztKNIu4Z4kpRFYzGybxr4mVBcq3YZbUvWHuxYYph3WCl6XAb3jfBcE9VHcT8BvR/w1IAe6xBnLe4ngH0azj3lRSQwPj8tAOo7pjz+XvrYoEW/FwrQASeofx7AMunPHfYlcdl4zNsS0HdN9hRc4ovF3jXUltYYvxXY/8vbyvg50xKvb1bta7AKkN1QKMiK0fM6D6lOZOVDEARBEIS6IpMPQRAEQRDqyrQzu3BoCNu5m1ImAl3J52tg53hSn2U/Twgd2nf4cpjH7IIq64nJQtfwhOwyswuuDws1w/sui7V1HRqa59q+Rht0PX59vo+2Db7EfY7mNZ8wOc8yLP+qz3X8vkvvy3/p169P4DA9fjXbpm1XQc/AZMvUtAq122Nsxn9spVKubg/0nSRlb7z6SnV73tweUpZIUvPJrl27qtvHjx8nZdctW17dbps9h5RFwhGyj8NpW9tpFu5oMl7dNgN8+ESN54mbZs8WvXs8tBWHvfJm5P8t0j7BQ3Zrm4fBZe8hMhcYPGy61jnBa7bzIxnRoa58id9h4fuFku4T5TIdQ/C4ZXEzLx9/UINVynzswX2fhaN7zOJ6n2drVdh8rbiJSF/DM/LxMG50XhbpS67JQ2QdNsbifuBrvveYlthzR21rWpM/VZCVD0EQBEEQ6opMPgRBEARBqCsy+RAEQRAEoa5Mf58Po3Z4klex/NwcMngIJJHQ9dNQ99NMP8tVyJ6qbUck4XdMOpvbmumxtX0+PLroLncCQeXcXkz0fLlMO6/P+EJfveGhtX1QeJgcltI2TO4P4rkS2vbxKxnL58MnLM3PVwP7+ngk93lNffoEDvHznMcTblxbvptej+17+qi+jtcXYPzOAK7SNuvT/X2k7L29e6vbg6dOkbLuHuoD4pa1lDZUqB383T3vVLf7T52m32N99PRpXb5g4aWkrGfBJdXtxlSClDWldChwQ0MDKQux5Jm4eQIsLNn1+B5pbI99v7ZMOz0pK3OZTDp5h+m7h2W3vbLs4w9lT8V0G/DeYbN3z0Ay6UHuAIF8ZIIGk15nbYf7JX8vKiisW/HwWYP6pCjyO+N3z7QsFA7WOO4sKSMAS/fTY/3CnXnzKKO2X53vWMTrh0N2x85dMmFk5UMQBEEQhLoikw9BEARBEOqKTD4EQRAEQagr097nw8+y7C3DNrZzl7Imtm8f+6zfmbxpz2unzuZlDpbH5vfhp6/O5ZeJU4OPrwhQnwvDYz9GNkaPKjr3D5kcnQ9cBW86biICwsp8PIM8/ih+9tHaOgB++hwcUjSmbDKWUOftg7RW2D17JNRRezk+Pjhj3YeL/I0+jEm4Ui5Wt48cOkjKWlMpvd3SSspy6QzZLyFJ9XKhSMoODh6obidTZ0hZY2Mj2U/EtZbHoXf3k7LjR45Wt9s6aJbucEj7NIRCNH374iuuIPvRBi3pHmPy7tg/hPvZcJ+PckX7ueSyWXqNaFSfh/lqFItUfh73EZOlqS8ifYxcvkDKgsHaPg0cE9XdI5nO9oNoXDUDPKWG/tkKM/0J3g0d9Ilp0rpapq5PoVghZQ73XcMNxMcmtO1xdcKaOkwfhKcgwG53lsPHfOSn5fGzYeO6z5KCn4+QxZ479pfjvnOTgax8CIIgCIJQV2TyIQiCIAhCXZn2ZheSoZOXebIz1t7zvQYPMRzvF32kz71mBB85c092UbQEx6/JzS74ovwaeGmPZdbkIZj4mg4LgyVhn55QWy6NPM7QvDHMLjTcmWeZxMf5myDYRWuXjGlX8Atvw1fgZjKfq/uYYbxhr7WfgUeiG7Wdx5RCJLk9FWD7WPKZXWMCy7S47l1zOknZ4p651e0yMxVsfXMHvSbatvjaM17+5vdcoUvuFXSdzDA10QwPDVe30/2DpKwbyb/n86O0bqx9kk2p6vbJEydIWc9cfc+zu6kUvBGh5hzcnx2bvmtBJP9uMPOEy47FEu88XDUYDKBtarrgpgQ/ihX0/DxmZgqWN3egtrnC8WRT5mN+bfl5LI8fCLPxj6UkcFEduMkch4pz84SLs4G77D7YsTY6Dx9jsZlqrJGIh+FTaof2m2bttrO4pMMkICsfgiAIgiDUFZl8CIIgCIJQV2TyIQiCIAhCXZn+Ph/UwO9/rI98NfEd8ZyHa93WtpmT7/LrEeM/temZnnTqaPvsVQYAAJf5Gii+7yPJTRTTXf8U1y5gnw/ur4JTgPO0zOcWauu6tW2eAFQOmctRk0fgCTudrJAxvzBUXlb7LH7y1P4huj5huD7+Op5y3j7E18hHYh94ODg/dPz/12DbcjhMfRogoMP/KjYNnz10hIblRkKR6rbD5NUTKOy0PU6lz40AHQazo3l0eRp+2BDQ9TPZOxIJ6WuUmM/A5je3k/0l13y0un3mNPUdOZPWfiaZAvUd6TtJJeabm3WY7pyublIWNPV9xZNUCj7UQMN7FQpTLhRYOG1EhyJbAXpfgQm8T3nkV8L92LyjHB6P6DUc5CtRYR0vYNH9EJIaL3NpeAihbR6PzmUJkM8Hl8MH3dcsJm1OVAgUHWNd2rXIe+A4/DzI54PdsxWgJ6rgr7K64vHQ4O+o52dPX/N8TBRk5UMQBEEQhLoikw9BEARBEOrKtDO79J3OsE/0EhRfIuXLUyYyHQTYUite/uZL4RW2hEvSCBo8tKr28himyELx+DVDIb1E53ITCFYbZRlLHZubYZDCH68rWmersFBbT3gv2jet2nNWm4XwjdmWNeDf8zNPTMSUMnlml/EzXvPJWOG8Y4f7vg/PMDsxE834r0/Pw+owAcnThog2l8xqpSqmrqn7c4Uthcdbm8l+3/GT1e0SU+LEI8NocYSUtbbPpudNJnXdmqj6aQeqn8nCV08P62y4uSI1ERUKebI/eEqH1zYhRVUAgF3v7tN1ZeYbVS6T/b7j2kRTyFGF04NH+qvbV1x9DSlLJOh9RdAzwNsAAKGQrp/LFFZLJRqm7MdoBWeG9TeHYjO0yUwy2OzrzVROzxMOYbMdbTsXmUhsFgbLFU696sfo2Io+byDETZX4d4WPv7zuui2dcu1xlLcVrxkNq+ZmF7TN7Sw+7zcf1ycDWfkQBEEQBKGuyORDEARBEIS6MuHJx+uvvw4333wzdHZ2gmEY8Pzzz5NypRQ89NBDMHv2bIhGo7BixQrYv3//2U8mCIIgCMKMY8I+H/l8Hq655hr48pe/DLfddpun/G/+5m/gO9/5Dnz/+9+H+fPnw4MPPgg33ngj7Nmzx2NLPBe27dxN9rHvBpf+rZSpHHOxoG2i8YYoKQsE9Hd5aGJ+lPpnBMM6TI2HpZ0e1GFzjsPtdsiGVmZZFJmvRhD5fAQD9L6I7d2m5+H+IVj+2JNV0dI2P5vZ0yvMlhtEVvNYmLYdDucqMLsqv+Z4bYdj+Xz4hUb7+XVMhc+HH/7+F/774znnmNdkBmMss83D/fizxF/l9mzTK/xfk/TgUHU7wrLBjoxqH6/uefNJ2ao1/5eeB0ufDw2RsoHT2h+j/8xpUpYZpKGu7+x++6x1AwCwUObaeJyGryZRdtzmVBMp60rQ0NbSoPbHODNK3xmV1/4iTp76ivAReyij2ycxPEzKhvsHqttvbXmTnob5x+GxafHixaQs1ZmrbscS9J5PHTsG48VWPj83rMtaNbb5wd60C7QfhqL6mZSzdKwm8gHsCnwfh6BbrLK4r5tMQgFLqJfL1A+oyMZK7IZoqdr+ix7/QJQF+f1K6Pe0UmF34puJmssZYJmG8ftwjZcJTz5uuukmuOmmm85appSCxx57DP7sz/4MbrnlFgAA+Od//mdob2+H559/Hr7whS98uNoKgiAIgjDtmVSfj0OHDkFfXx+sWLGi+lkymYTly5fDxo0bz/qdUqkE2WyW/AmCIAiCcPEyqZOPvr4+AABob28nn7e3t1fLOOvWrYNkMln96+7uPutxgiAIgiBcHEy5zsfatWthzZo11f1sNus7AenrP0n2sf2L26S5PXAkq227NrO3YbgeRqyByjHH4qnq9uEjR2n90CSL+3xgjYuAxVNj07pj/5gk0h0AoD4giqUDt5gtt4Rkk/PMfoxtkGaA2vRGmU5CpajrHgpQ3x0sN6wC/vPZQlGf97K5bTWPm4j/w8Xi8zFW2WTphVCbMX1ehw8frm4PMl+Ij157LT0POSevz/jbedeWrdXtYiZNyi67YmF1m78HRpi+l2ZQ9/2Org5SdgWSoLaZzX7oGE1p/+9Z7eNweJTa6dPoHe47coSUDSJ/pnwT1SD5+FLadrbS40+pRN+1WRHtU2Uyn7KhMl0Ztkv6PCeO0bHRHtXfPeNQ35UU80kpI/0Q/I4CALhoTItHqH9Bbpie1w/T8HpvfIDiKQlc7G9Q+5y8iLnOQTiKfG1MqhFl2+gaXDuDjylYd4T9riSQ/6Bl0N+VwqjuE6UC889jUuxgovbxDKP6+o2NVBfGYppVI0hDyqNXgmHvKPv5JGMl12WZDCZ15aOj4/0Xvr+/n3ze399fLeOEw2FIJBLkTxAEQRCEi5dJnXzMnz8fOjo6YMOGDdXPstksbN68GXp7eyfzUoIgCIIgTFMmbHYZGRmBAwcOVPcPHToEO3fuhObmZujp6YH77rsP/uqv/goWLlxYDbXt7OyEW2+9dVIqPJJlS4toGZSHA3EpYMfWJggehstDVDGjI9REUjhyuLrdP0DD9spIapeHKpJwWpOeMxDkZhhd93KZPqZQUC+7OUxePcAknx0UNMazv5ZRG4SYnG+ILTsW0L1YEXqNYBibYfyX/EPjjNjyZHV0a4fUTSczix9jh8iSvZrH+UnRv1+OQr4dukycQaGbcba8y1sSPyP3Q2QPLmbRcjiTD8+hkNlCLkfKQgbth0OntXnALlPTQUuzNoO0NjEJ9xA1JVw9X5t6YiP0vThwWtfHzdK6tiT0ezC3jV4jGWIZTcO67hY3UTkoVNKm49QoM4kMZbVUfHogTcoCKPw5EKXmI54h2ESm08OHqC6TgcwMxcwZUrZ//7u07i21V64tIotO4WYX3J+4CYR8j7Wdzfs+yuwbitJw50JWt63iYeTc7IIzwCraJ8JBXRYO0Gu889ae6vZ7R6jfY9dc6l7QM1fL/CuW1TsWi6Jteo1MlpuTUP2YOZ/K0Y9l1sUZ4y+AUNtt27bBb/zGb1T3P/DXWLlyJTz11FPwta99DfL5PHzlK1+BdDoNn/jEJ+DFF1+cFI0PQRAEQRCmPxOefHzyk5/0/Q/NMAz41re+Bd/61rc+VMUEQRAEQbg4kdwugiAIgiDUlSkPtZ0ohZEBso/DWbmfAE8LH4tqu1k8Qc1AOWRPxuGpAACKxW9ZyGKZiFFfDcfxmc8h+5sVona7WIyGDTYhG3VLCw3bizdoyWCbhziyS+bzI6iM2jHLI/qeT7H8O5EGarvtmp/SdUehxu8fq30DVInakrmGbwWHI54+DLXgYdKu6xE81lvMJDxefxDOZPmHjFcGfezzeD6pfSzadj1tx76HbvNMhtqLy+h9uqRrASmz2TPAp+Wm9om0ZRaF9MbD9H0aOoVC13/1K1LWMZ/KrRfT+jz79uwkZT1z5lS3Gy6l8uHpkzSkuIL8n4Ix+h7kC3r86R9Mk7JAqw4FPpWmUufNORrain0uKsyPoy2i36cQ79vMNyxj6P0jJ47T+qDxMB6h34tEeXoJ/VPQN8zaA/nDpftO0QpZdLwL+vp86Pp4fD64vx4SVVd+/yNzdxnWER1U9ziTTEgjnx1+DT5W4lBTHnaKUwJEmdS5hc7blmohZbEQ/Q2ykH9KOMbOg8JwMxkabl1mKSuw5AS/DwIbFvzGm/Pg8iErH4IgCIIg1BeZfAiCIAiCUFdk8iEIgiAIQl2Zdj4fsQSznaLU9PlRKh/OZdKDMe0rYbE09Y2mtr81OtTAFW9gegeWPjZf4PHySIaXxY6bSIPDjFCfjzALRW5o0OXRKLX/hZAugWKP0HWp/S+R1HWPx+k1Qqg+W0pc+pe2T1vX3Oq2wzRKSCx5hWof2Ez+3XawzwfUhPt48H3sU8BtledD94M/S85Y2hrngr+8Or0Phe6L18WvbidOUd2cEnp+ZpD2rTLTzcFaDL4yzmNw5NCh6vbiBZeQsgDyaeg7cpCUcS0Pu6z9m/oOUf2JRlPfV7GTyvqXFfXxgpT2W7AqtJ2LR3R79bO+XcpqP4+BCrXLG8w3LITE6eP0VQMY1ArRsRJtcztAx6Lutq7q9tEo9YcbGdL7Taz/Jlk6hXJBj512mtbdRPLhoyx9QkOyEc6FsVIHGAbWBPF5n1mZ69J9B8nPJ6P0PejHejcmlX4PsHECu7Zw/4dKRX+3uYX60tz22Ruq2x0N1HfvcJpqprw3rP2bGhtpux4/pvtWgTn6GUzbyXRr66lQ7Q7atwzF5O/RvqdsEtzjZOVDEARBEIS6IpMPQRAEQRDqyrQzu3TM+QjZDwT1mmWFSTPzhWCcnTbM5I6jKITNYsvUFsvG6KBm42YXB4U9GSZbGkfL1NxcwhWWg3h5k2U/tCs4BIrepcVDxNBX2+MsG2JQm29CLbNJWQbJNgMAjKCMjHEWBqawzDYLd/bEYPrI2GNsFj42lrCd3/54y/haIj6Uh3FzzjW81i8brW8WW88+NkP5n6eMTJU8rLytTZskuLnGI3nvY3apMJOEH6dz2nxyCXsGDsq8XM7SDKoDeRoS6pi6Do5Nr9+H0iAUCnScCDeyUPZZun8f6qeS7gX07oUa6NL4nB4tqb5wDpNwz42S/RQy/SzoaidlJ159o7qdH6ImEMeidR9FZqEUM00mkCT3LIO+++EyHdPySD58NMCeOw5dZ1mzVZDbjGqD3z0eEssxSUZVDnpn+JvgcnOS7j+d7SlSFrJ02/L3yQTazkFkazEt2s7hkG67CIuNDrr6PIEg/a2IRZl5H42ruSztL7atz2uxjOguM5/g3xJviOz4xykSXsy/JmYXQRAEQRCmGzL5EARBEAShrsjkQxAEQRCEujLtfD7mzb2MfaKNUV47MzVM4VDXUIje+uCADqHLpalteU4n9YfA9srcKLUJY5niUIDZ5pC/Q5iH4XJbd0Xb8SyLp5vW5+Gzx9MnadrmQ/verm6XL6Ny1GVkHBzOUNt//2lqTx8e1G3S1d5BylIJHZroOOwZMNtuYJy2Qo9U/jn6fEzEH4T7mVjIvs1DbSfin+EH/t5Y4brEP4QXonDnseqGJfeDzGafSqWq22XuQ8XPi3pfhaUgGMumj9lzREt2Z0aoj8PShfrdm9tOpbsNZuu2gzqUvPsSOk6k01pG/vQpGuPd2k3D94MoDNYsspBZtN/SQNsuAvodipRp2H9H6yyyP+fqq3S9i3QMGcno75qj9H3Cfi0AAPlR/b4nbebLYiE/AZYGvlSgYcqOob/bNp+Od8VG3e4nR+h9BVj4tZ8HCH6FbPZ+85BZE/dnw6c/8xBd9maMFrXvRCxKfWuiyFfDZT5Cszvi7Fgk927TsbIdyepHmU/MGeRrlAnRuhUK9DyljPYJyeX4uKn7tg20zcGYrDB/PqpMvnwARlY+BEEQBEGoKzL5EARBEAShrkw7s4tp0DBPEtrJ4lX5MjGyZEBxlIYybdy0rbp98uQRUnbJAmqucNHiYpqpAeJsrHxJOxLWoW9uhdbNa2ZA4asuXU4tYxVRVXuZDwDg9FF9L+/ueZuUDWVRJl8WajvC2ieCmjZxww2krCWpl2WLZbqU2HeKZsF0fdX3NN724EuA48tcOxGzS4EtRePnFwpRE1o9zC6+obae2MDa5+HmpDPpdHWbtzNuH2524aYnrFbLzS4ToT+tzUA5pBIKADCvTfet1ma6FB6L0iylCy+/prrd2D6HlOG+nztDTYrtnT1kX+W0Mmiw0E/KwhVtno2a1ATR1jqvum0G6TJ+kKmoulmtbpll9TFM/fwUUyW2A1QZOVDUx1qjtP+m0PCTC9DQeSbwDAn0QYq9amn07h0r03GhOEIPpk+Igs0n3kzUdL+MxpFIhKqGkneGmwoseqIyCtW2mMkqHtU/f+UR2nYhg+4nUBbg+d3zSFlTSodcDx6jZu/BnH62jkX7i0GbEoySrl/AoM/ZQesE3qhX9gmSeFCKm4tR3/IMIn5j2OSbYGTlQxAEQRCEuiKTD0EQBEEQ6opMPgRBEARBqCvTzufDZrZlHL7qsmy03L7vIhv11q3bSNm27buq24USNca9vf8wPY/SzVapUHt6Ecn5hsLUTyCCMtc6LPuswcLJaFgasxdju53DfBoqtH1CqA2GR6jdN5fXNshSltkjmaR8Nw43ZlkUM3l9zwP9NLPmnndodlEcvrroEhp+iPFIefuEbk6WvLqfpPsFF2rLLoez2o7Vdukz6ep2Q4KGr2Lj+yjz++EEQ7o/V1jbeWT2fQhEdR2CJvUziTejPhJm2aUjLPQ2rG3v2Tz1uTBD+ruRFO3bgygMFwAgN6J9oZJR+g7ftPgKXZ0YtcvPmq1D0CMmdaowWd1PGLp+gSSVd7/mxs9Ut2OJJCkrObQf7tv0ZnV78M03SVkcxbVXWDhmOUDboITe98N9VGogh3yE4rNom8cnkNUWZ9zm2bf5+5VHIdc8w2wQp8ZgvmAGy07rIl8Fu0LlzZuS+vk1t6ZIWblIfX2iSDYh1UDHP3s0Xd0u5Kj/TgT7fPSnSZkK0rab1aZTh5Qy9L4KRf0bYPLQWjak4RB4z2+g6zM28n38m3SO2cD9kJUPQRAEQRDqikw+BEEQBEGoKzL5EARBEAShrkw7n4+KTe12E9NJ0OWVCvWjOHpc61HkS8weabEU8tiO6NGjQJLGJVofE/lVhBW9D8uk5wkgm6yrqB0cm98sl6W4ZvdcNHDMNymCUIPWSYiwaWiFmRXDSKOk/wz1D8kU9IkLI/S+AlFq10wi+W6A2mnXbSbT7rJU2cQC6fH5wNseSyY7r273k6dOkrLGRm2nnxWi/ike/wxcPd7QPunBsQy54udkdScx+uxMZZRa4MyZM6QsFKY6CblR/fxmdVDJ6XJJP79Dhw6RMpPZ0y9dqCXMbaYJwv1n/HDQvZRZExw5ery6nUpcSsrcML3mGSRL3tFD72vh5fr5NcSoH4dh0vd7zgLtG2WWqJ9UCPsYBLhfhz6PyYTGHZe+YEn0Tr/91m5S1pJsrW73sJQI+SHqU3DgsNbxGXLpmNZiap+csGJ9gI0bh0ta16KvTN+9OQ3aJ6UnSX1Qimz88/tBwdpFeBvA+z4FLN1eo3n6DJIBXQeT+16xdwaPo5kMfS/i0VR1u8LGlyVLP0b2LaXbpJSncvhDp7Wfm2XStgsMI92PgwdIWbjrErLfeu2y6vaISe959JT2gTEN2n/5OIFb0jsU+aSe4MfifT5wnZuLG0FWPgRBEARBqCsTmnysW7cOrrvuOmhsbIS2tja49dZbYd++feSYYrEIq1atgpaWFojH43D77bdDf39/jTMKgiAIgjDTmJDZ5bXXXoNVq1bBddddB7Ztwze+8Q34rd/6LdizZw80/O8S/v333w8/+9nP4Nlnn4VkMgmrV6+G2267DX75y19OSoVtm5sgsFmBhZMxqW8cXXbF4oWkrK1ZLy3u3X+MlJkB2kw4y6xnGR2FAism54vPEnLofYR4+BQJOaRLlEg9F0IspA8suiRXMfA2D8vV13RcupTosGXjbE6Xl1m4ViyCzDds7a4lQJe0I1EkF21TKWKMzbJl8mVZ/8y1ONsqXQbl2TOLyMywd987pKylRS9/J9hys8VCFYsFFJbK1jpDKDSQh69GozrczzRp3bjpooRMGwUmpZ3JpKvbFYe2Fc58CgBkmZbfB84MHWHmGoeFsiu0VM3rys2RfjjoGblMhnxoUKcLyOdpNmUrSuvX1dVV3Y7GaGgrkc5nZpdInIaPNqb0WFDJs/QJUd1eeRYuaqPnHgDarlFmYWxCz7qncy4pC5q6fsNZ+l4qZoIwkex/OUyl2FVKZ+tNBWh75LnVED13g9XdDulnaQbp++yw8djf7IK3aV+3TPrNKHq2aWZGjFT0fQaD9HsmsweELH2eTIaai5sb9XlGcrQs1XgF2Vco83CZjU12I/rt2EdNaOVhbSaby55di0Pf4bClx6KmBtrOw+g2wzE+htF9nCzXzzriKWOpOoBIDfDYfp8Tj5MJTT5efPFFsv/UU09BW1sbbN++HX7t134NMpkMPPHEE/D000/Dpz71KQAAePLJJ2Hx4sWwadMmuP766z98jQVBEARBmNZ8KJ+PTOZ9cZ7m/1012L59O1QqFVixYkX1mEWLFkFPTw9s3LjxrOcolUqQzWbJnyAIgiAIFy/nPPlwXRfuu+8+uOGGG+DKK68EAIC+vj4IhUKQIhENAO3t7dDXd/Yl9nXr1kEymaz+dXd3n2uVBEEQBEGYBpxzqO2qVatg9+7d8MYbb3yoCqxduxbWrFlT3c9ms74TkHKZSWAj+5vhcolnJn2ObObv7dxByiJp7RTbZVJbnF1m4aPILm1aTHYb2WtdRf0mDOQ7YgXHksjV9jdl0MdkIHtxhfmDuCz1Ma6PwdJNm5b+bsig6clTDTREtq1D2zVbGuk9RyzUHhFaVydI7fLER8cnGrPMZPS5ZDjGZO2Dw+v6+miK9hEmMZ9HYaenB+mqWwGlK7/sI8zOa9M+cuDdPbi2rD66H3AH7Wuvvba63dFBfRpCIeqbMDSkZa8LzHekMKLvY7RI/Sb6Bqhc9kc/tkTXzaJ2Xizj3NzSQspKRWrfTyM7OVP1B9sev1EYhzEq7guFbPYj2TS9BjvPSFr7Brz7Ng1rHB7S7/dVV1Ifi+Y5C8h+HIW6Dp2k4dfBBl2foknftRIaJwLMP2ZkhPoU5JFkeWmA9smjJ7WfS1+e+js4LFw0n9HPa24PvY+WJv0OtySoD0GR9Z8eV79DEaaq72R1/zlygv4Tyf1eLp1L61ATFtrK3J0gENR932HH5nK6vZqbqTS9crmfnT5PPk/f2WhYn6ellfp08bDyY8d1yPep40dIWSihw7j/c8N/k7Lukh5/wnnasB39tC0NdN5knPokNse1f0q6QL+XPU0dimLJzuq2w/zPsHscbyuXhYNjeYMgcy1kPzPnxDlNPlavXg0vvPACvP7668TJq6OjA8rlMqTTabL60d/f7xlYPyAcDkM4HD5rmSAIgiAIFx8TMrsopWD16tXw3HPPwSuvvALz51MBnCVLlkAwGIQNGzZUP9u3bx8cPXoUent7J6fGgiAIgiBMaya08rFq1Sp4+umn4ac//Sk0NjZW/TiSySREo1FIJpNw9913w5o1a6C5uRkSiQR89atfhd7e3kmLdMlk6NJ4GC2xR4J0LsXDrkoorHL/nl2krJjWCqdNIaZOGGQhSIYud5i9xEYhYzYLg7VReFnJYCYiVlcbLXkZ7DwWyrAYiNJHGAnRkLpYWIdysiS74Lp6GTLAlj0bWTbNWW16KbohTNsjgNRZuUqnxUK08HImNWZRbIcv29eeJ/Ml/4qt63B6ME3Kjh2jYdQ4BLPE5DVjSAXynXfoMv7be2hInYvC5kZGaJbUADK35XI0dPLIUV2fWW1tpGz+vHlkfwRlIY6x1cK9SCVzcJiamswAX1nUD7shSftLbkQvDdtM5paH2paUfiaJJmqiwUq7Tez14cQC+johtv6Or2ixkPfGEN1/+390OP/hU7QN8PvlDB4mZfO6qCmsOaz7/oH91OySN3RblkL0xvK2NoGM5qgpBTK0PlZWm0+Cabocn0Vh1MPMuBSs0PaJxHU4LX9nTiCV4AoLozyDMhsDAJzIanPXgUFqtpuFxr9wmI1T8fGvWuPxWLEQ3dFRpmKK+lNDhGYPzmb1+6XYOGWysPtSSb8zDstqWynr+7KZ4vXBo1Td90Sf/n04ibYBAOYgFef29jmkLH9an3dn6TgpazxEx6LAK6/qcy4hRTBn9rzq9vG3qRk1n6f3HE2h7ME+9hFeZpm0r0XQczfN8YfOj5cJTT4ef/xxAAD45Cc/ST5/8skn4Ytf/CIAADz66KNgmibcfvvtUCqV4MYbb4Tvfve7k1JZQRAEQRCmPxOafHhzpXiJRCKwfv16WL9+/TlXShAEQRCEixfJ7SIIgiAIQl2ZdlltsTMrAMDll+mQpLndnfRgFgZWcbTNr7ktRcqaWrXt267Q0KUSCxt0Azrczma21FGUEbfkkV/Wcz2D+UIEmQ0/hHw1ghEarhqONqBtag8NB6jEchzFSMWCLATURCGyzFYaYtdsQLLoJk9/iGXt+erYmJmGz84Ikw/fu3cv2cdidDyssziq7cn9AwOkjIvYmaZ+JlzC/fhJbdt9cxf1EUqjsE4A7k9D25n7KmBGkG/AkePUv2DXW2+TfVw/FjUNFRQybPPsuExy/wRqEzNA//8wDN2fLZN+L8CktWd1avt2somGPFr4vGM88rkdqer2vC76DttFfV89C2hW20b2fh3cosPns3lq3581T3/31LvUnt80+B7ZDxR1Gxw5SENk95T1sxxmjzWDwsG5LHqTS30cro7ptuxgcvhuQJ/HYPL3wDLwoiGF+IoAAAzktJ9JqUSfpcX8eeyibsshJlHQhuQEOpubSFnqMhq2TD0nKCYaJwLcl8VgKSSQ/0yU+dylUUhzpUD9ZcINdDwcQWHC8Rjt650dqD+xvrT7nbfIvl3Rz6+pk/p1dF+i26ApniJl+9/Tvljbc9TvJztE/Vyaivp3pyNH86G1ot+5yxb0kLKTffT3qmzoffZ6g4XGu2CQ9okgk7zHfoCHjxwmZW0p2s7ngqx8CIIgCIJQV2TyIQiCIAhCXZHJhyAIgiAIdWXa+XwcPnyY7M/p0NoI+TxLe858EypI5yPVQo+dM1fbmoss5hz7agBQ27fDynIFbW8bKVI7ZgHF4asQ9akIhKkNDft5BMP8WFxGdRp4Ku8G5ByQiND6RILazmlXqM3TsKi2SBDZfQ3F83Gj7zLJXsVl0sfp8zF0hmplbNq8lewPozTbWL4cgLqdVMoslzkDS9W7zPaO080Hmb5wPEnl5wNIattgTg5YwRf7mAAAJBJNqIxpXLC2wjbaSJhJ9yMbuicin+nEKKTzEWT1CaG+HbCYH1KI7re2t1e3uUYJrvnw8cO8RoR4TA9D7e3Up8At675+yWWLSNnoUSpz3RHW99LNnlfA0P3JaaVlHSZ735FPQfvl1M/kV4e1v8zQEPX7GS5pn4vREvXxsJkfxRBqyyTzySk42t8hz98Xg543ibRGGjrbSZlCfkCVEvWNiDA/ipCLJO5d6i+DaxBjPhUNMZqWwc/nA/tVcJ+P1iY6jo2iVBgNLGVDZ5vuI8UCHSfCSdqWBho6HaYtYqC2TSTofaVz1HeuYOvnpSxan6PHjurjhqivTxn/VjRTle8K0GOjbVrbpFik/jsH39uvrx+nz645RZ9BtoTGPJOPRbrvcx+3IB+abf2MiiO0r4P4fAiCIAiCMN2QyYcgCIIgCHVl2pldbrjhBrLf2a4zCvIlbR4SaqGlzkiMLptHYnoZyVV0qT7KQiXjKH7JNOgSmNuslw8dgy7vllFzO5EUKTOYBLaBlmItFuLoomVzl13fcGgbhNFSfiRA78tQeinWcNliPZuWKiTVbJepeQCHZ5ouLeOZhR1s2mDhouTyLFvwp1esoPVBS6bBEDdB6LISW/522H1iU4vD6orNHNEIXYbl57FQqDK/LcOofaOmVVt73OL9GR3r8gzOUEHb7BmwELoSMkUZzNRkkF1aN57pEz+jUoGGpE6E2bP1O7xr1zZSdvlCbfYolGj/HTpDl60bLf3c54ZYv6voTLGB1hQpayvR/nMUjRuX3LCUlDXAzup2OUeXossF3Scsgz67AntPBxz9TGaFWd9C2bAVC0FV3JSMXlSHmTjPoCyuRZaWAuLU7FxES+wh3n9Q/x1hZswm5fMSM/B7yZTpoZJPk31sbWpup+akMBrHjmao1HlnJzWTjY7q9qkUWXoAFJqcqdC+dPzQQbIfiGuTCJY6AAA4tFdntFYs14Nq1L8Hs1poGHlTBx3Xy7Y2IbnMDD+MsnFj9wEAgKYEDb1NRnTjplkW5MHTur14qgebvV8VlEE+nabmLeii4cbngqx8CIIgCIJQV2TyIQiCIAhCXZHJhyAIgiAIdWXa+XzE49TehsOFymVq3+c+HziyyLKonRXLmTsO841g/gcmsokGAjS0y0Shrwaz22GZa5NJV1vsPNjPg4e9KhweytIi2yyy1EEyyjx0E4dcWp5U5uxY5GPg2jwkFNnXWVpvLq2NQw5ZhGzN6wEAxGLUPor9e/xCVKNRlvKb+V/gLqJ8pOAtH98MAADXJ4QY91HergZ6BrzMew3kn+K5Hqoff84s/BlH39k8NBrtK56Omz0TA0dYMx+Y8croA1Dp/ggLkU2g933rth2kbO/GTWT/lh7txxBg6cHNgraTN9jU3yEGtI9U0Dhy4D1q+z8xpH1HRpmPEKBxIhykY0aWuVRlUFoGJ5kiZSUUIlsYpb40RoWOcblcX3XbZL4INurro+weM0wiIIP8CGIRFmKNwmvPjNKQ3c4y6yN0GCMEUdj/ZT2XkLLcwAmyr4L62J4Fs0mZi8aY7s4WUnb11YvJfiqpn3U2QwOBBwd0OoODB6icusnGgpakDu9tQNsAAO5IurpdqdBrNLbp8FozTuvaw1IJuFF9zUaD/nYUTf1TPVIaJGUhoM8dDyObT9Jw9D07dZoIx6GdsoH5QRbRO9PE+uhkICsfgiAIgiDUFZl8CIIgCIJQV6ad2aUxSpdlLaTn6No8rNJl+2ffBgCIoGyEPMMsx0BL3KZBm9DACpHM7GIiOwNf1uP2CQOZMjxJZPFqGQsLtthyKjYzuB4zFDJBmDy2trYZhptEFDoPFz/lJgC6PF977ssVIV22RIjDV/1CWXmZZ59cxMd04nh0Qwm4ufhZ8PImN0f41d1zjXGadsZxoprfc33agNcVH8nNThOpT2FEL+9ec+VHSVkILdWrCr1GjGXSPYHCHA2HjhMxS0tdRjL0PkZj1AR7JqDr/uov3iBlx/N6jCkUmZkXK8lyYVIWapsp6eX5A319pCyMlHUDLPNojKnMRqP6vkyWidpGodGZ4TQpO1mgYZaDyOwSaqBmBROZLkdKVA32yKGjZD911UegFskGXb/u7i5SNhSj7RMMIZMw76MlvZ8MURPalg00VPvMkM5qO3jmNCkr2Tr8eN4CagJZfMXl9JqmrvuJUzT7tI1+Rg2WRraQ1WYzk2XfLrH36dK52mRUKLCwf2TSC5XoPYdZSLxd1mGxAVafeXO1uatUou2a4Kbtgm6vkKLhvZOBrHwIgiAIglBXZPIhCIIgCEJdkcmHIAiCIAh1Zdr5fFg2y0CJsrFy8zm3UZsoGyFTP4ZIg7ajOWUWPstCOa2AthErn/mbwfwoSAZVlhmR+1wQezq3eSLHCq+NnoWLApYPp/4hfr4IHqM1Cm/j7YqrYDO7psucQHAoZwiorZ18zxl/6KafH4Wnrqwtcair4eO3wKX7eWfzCyzF3/XzheD3wY/Fz4uXeernc15yHof7ReFj/f1T8D4P25sIuTQK32R+HQqFLsY7qF2+e+ECsr9zh7b3b87QcSKKbPbJRmq/vvIaep7WRVquuncxy+Q7kK5ub3id+oNUUNvxfh/w+BqhcPAUzegab9S+YjbvZ2zgGkHXzDCJ+/SIbtdilmZJVcx3I2vpsSFu03DR4Zy+l3gDT2Uwfp+ljlmp6nalQJ9BiPnHHTioQ5x373qblu3VGV7TQ2lSls/SNiih6ziK3lcF+THc8n8+R8ouXUh9PkZHhvWOTeuOn6XDpQaQ75rN3qeBwSGy39OjfYjiMdoeZ0a1j06QpTloYOklTg1oSfXh/n5SdvqkLisWaR9NB6ifSXuD7hPB4Lm/37WQlQ9BEARBEOqKTD4EQRAEQagrMvkQBEEQBKGuTDufD55OGNuaAwF6Ox69BRv7h/jY7JldldvTaRp0an/DOhZcrpvIbHO5bmYrVB7BDFJBtOXvq2EgHRSDn9OnrhzsNsDrhv0EHK4PMgGZbQxvcz+fAs+zRNe0bdv3WHxjXE/Fz4+C65dgTRk/vw4u0+53X7wMn9fP/8LPV4SXe3qPT7/jTeenCYKvOaZXAH4G7OiFC7VuhMlSK2SY3wCk2qqblShNw16x9bM8OnCYlqWp5kW7rZ+RG6DaGZWC9hswPNL06H1i/9eZbKjFuh+K6XMUUB/pG6Yp0XNF6rsxUtB+HRWPn5TebrBpfRrDtD4hpJ/U2ED9DZpjut2TjbSsKUL9VfxGkVPHkR/HJioR3j9E7/PAweO67OQpUjaa08eWi9R3JRam/g9B1JYOSxmP5YrSA9RX5PhBql+CZVq4vsso0myxbf5e6vpUmDR+YYRK1Z+ardPUL7jsMlJmBbV/HB9D3Aq9r23/o9MOHHyb+ssMDeq+rlx2HoP6xGSjuj+1NFA/yPaF8KGRlQ9BEARBEOrKhCYfjz/+OFx99dWQSCQgkUhAb28v/PznP6+WF4tFWLVqFbS0tEA8Hofbb78d+pm3rSAIgiAIM5sJmV26urrgkUcegYULF4JSCr7//e/DLbfcAjt27IArrrgC7r//fvjZz34Gzz77LCSTSVi9ejXcdttt8Mtf/nLSKsyXnPCyeoUtP3mCUEkUoc8CIVsnttlypqP08hmXTfaVwMbLyzwLKD8WL41PQJKbXx4vo7vMBKGciZhdcDgZaw9scjC4GcovfJQu2WIKBRYaOIE28CuzArR+xNTCl9F9stryDLTYpMf7C/6uX+ivJzTcz+zDUL6S6bXNMDzNgF+/I3L84B9qi99F/2QFANd//GP6mhXaR5PI1DJSoMvW5TxdKg9Y+koWyyqbaNbniUVo36qwkMPTtl4OHxikUtrHT6er2yXeeKg9bPZ+u8wcWUZZbfefpLLfFUcvfxdZygj+7yJSIYfmMF0aT6LstLNYptpEjJp6ghH93ViUloVQxm2L9Uke9kkNWJSX/vM/q9vZPnpkkScIRk2btGg/TDQiyYRGes/hAG3nSEibK7LUQgMDQ1pe/egemtX234Zoll0D9acyS/1QQfYbx2WhyMjsMpacwY7de6vbC6+9nh6JzEkWa/NR9g/+Kz/6sa5rnrZzY2Oquh0OsefM+k+uqL9bYe/lZDChycfNN99M9h9++GF4/PHHYdOmTdDV1QVPPPEEPP300/CpT30KAACefPJJWLx4MWzatAmuv/76s51SEARBEIQZxjn7fDiOA8888wzk83no7e2F7du3Q6VSgRUrVlSPWbRoEfT09MDGjRtrnqdUKkE2myV/giAIgiBcvEx48vHWW29BPB6HcDgM99xzDzz33HNw+eWXQ19fH4RCIUilUuT49vZ26GNZGzHr1q2DZDJZ/evu7p7wTQiCIAiCMH2YcKjtRz7yEdi5cydkMhn48Y9/DCtXroTXXnvtnCuwdu1aWLNmTXU/m836TkAmEmLoZ/vnvgjEpu9wWXT6XT9JdWyn5zY+8i1v7nlaH1T1ik19WWyntp8APw+Wzy4zn5gJZHMnpzUUS2+P79MTfsjuc5yht2NJjePKB5g/Bn4Gnv7Cnh0udhW1axaLOpTTe322i5871D7Uz/+C+3h4/Ex8Qojxeb3hu1xeHdehdlg5P4/fM+H1oe+Tvz9RZliHXfKwSoO0F2v0ALVRRw1d33fe3UPrE9a2/46uDlKWS1MfELusfT7SGepXMjSi36Gyw+37ettlbW6ydwZc7ctRGqUhl2GUBj0Vo/fY1ED9pFJxbbdvbqA2/ERU33M4XLuu7+/rDzy+Rgq/T/4yBH6MFFFa+IYmegnWR6LIzyMRpPccNHUqjFiUtnPAoO0cjejQ4JOnqJx5rqh9a4bSNPS3L50m+y5Kaa9M5lMFuu8r9tuBw1k9Yexs/73D2r/ozQP0PWho12HkPPXEyJEjZN8e1s4tiSRtOzOm90tsrDYs2kdshfytfBNInBsTnnyEQiG49NJLAQBgyZIlsHXrVvj2t78Nn//856FcLkM6nSarH/39/dDR0VHjbADhcBjC4bFc0gRBEARBuFj40DofrutCqVSCJUuWQDAYhA0bNlTL9u3bB0ePHoXe3t4PexlBEARBEC4SJrTysXbtWrjpppugp6cHcrkcPP300/CLX/wCXnrpJUgmk3D33XfDmjVroLm5GRKJBHz1q1+F3t5eiXQRBEEQBKHKhCYfAwMDcNddd8GpU6cgmUzC1VdfDS+99BL85m/+JgAAPProo2CaJtx+++1QKpXgxhtvhO9+97uTWmGu/4Dx2NM9+9puZXKbp48vgr9XBStD5+E2cmzzUy6T+nVry2wD92VB5+Fy1J5rYh0JbnfGfhxjSGeT83pk2rHGBC3ikuXj9fkolakPQbnM9A7QeXh8eihIbZesRmTPwmt/Hn0MpCFTps/LtOiiYSikTYfctmuXUQp7rv+AGt5iNlfen7FfBfexmIj8PP5u2eOrYaJtlq6Aa62Aj5/ABBgc0A7pUeabEI9qu3OEaXfwZxALafv6QBPtEwPId+Pg4WOkrFikstKOjd9heg0XDZkWdwZD8H4fNGn/bUL6FO0p6v/QEtd+Cqk4lTNPxSNkP4zagBuvTfR8iiZ9ln4J0j1aNOhg/n77pRLgNLVdUt0OsvegUGFpM5AfRZH5YlVQmRtifYKNcXn0/FQrlefvaJil6xNi7WpQ/xkXvdOOQZ8l1mUBpofhFnV9SiXaz7hmSqRR+7LYIfrcjTjyHeHuQ3OpW4PqbNXXZO2Kxx+eboMpw0MFaZQ4PqkUzpUJTT6eeOIJ3/JIJALr16+H9evXf6hKCYIgCIJw8SK5XQRBEARBqCvTLqstXxIkUtFsCZnLXLtoGZKHwlk+cuse6XMkr6t4FlfAJojaWS9dhy7B8asQ803NmoHHjMGXxnH78BAtfGYeQseXU33NSeg+eSir13wzvmVaLjbnCVsmoa1c6rxy1uMAvJmP8RI7cMl7dE2+xM/rY6OMlYZZ+1hP+CoJFfcPLx7vErefhDsAgIPu02Zh3Fg5Ohjy71v4fxeXhUriOgTGyGu7YG6PPiO7xQCSkjZd+s5w02W0tbG63bDkSlJ2ZkQvP/fl6FI0N7vg0HbHps/LLqvaZRW9z02BsQRdRm+bpU0ts+KN9FjcdqxBHJOF3ZdQv/M8H912AYvWJ8AkuvHY4A0Hx9v+EuG+BJDUOMsGHmZZdguOvi+bjyGoO5XYexnkYfdIsyAeom3QiEKa+TgRVrx9dLsbJk/1gELOedZ1lHm5wtID8FQPQSx5H6JGtDJqZ25yBZaFuIjsJ8USvSYZf3gWb4/pHd2Xj1n3XJGVD0EQBEEQ6opMPgRBEARBqCsy+RAEQRAEoa5MO5+PwbRf0ubJYpKahU/tTBRGGIhAPbBqbF/oxDz65Wwf2ytLlZpF3FI5+ZbL96mMfciHZiJq+BjeDcPoTGHLR13Y01g+NfAJOx2LX434fRc/zRAr4/uIWJzt682GNlpEAzDrz+kP8+VpJA590+WdU10F4QJCVj4EQRAEQagrMvkQBEEQBKGuyORDEARBEIS6IpMPQRAEQRDqikw+BEEQBEGoKxdctMsHKoo8CY8gCIIgCBcuH/xue9WQvRhqPEfVkePHj0N3d/dUV0MQBEEQhHPg2LFj0NXV5XvMBTf5cF0XTp48CUop6OnpgWPHjkEikZjqal1wZLNZ6O7ulvapgbSPP9I+/kj7+CPtU5uZ3DZKKcjlctDZ2enJl8O54MwupmlCV1dXNbFYIpGYcQ9wIkj7+CPt44+0jz/SPv5I+9RmprZNMpkc13HicCoIgiAIQl2RyYcgCIIgCHXlgp18hMNh+PM//3MIh6dR8oI6Iu3jj7SPP9I+/kj7+CPtUxtpm/FxwTmcCoIgCIJwcXPBrnwIgiAIgnBxIpMPQRAEQRDqikw+BEEQBEGoKzL5EARBEAShrsjkQxAEQRCEunLBTj7Wr18P8+bNg0gkAsuXL4ctW7ZMdZXqzrp16+C6666DxsZGaGtrg1tvvRX27dtHjikWi7Bq1SpoaWmBeDwOt99+O/T3909RjaeWRx55BAzDgPvuu6/62UxvnxMnTsDv//7vQ0tLC0SjUbjqqqtg27Zt1XKlFDz00EMwe/ZsiEajsGLFCti/f/8U1rh+OI4DDz74IMyfPx+i0SgsWLAA/vIv/5IkxZpJ7fP666/DzTffDJ2dnWAYBjz//POkfDxtMTw8DHfeeSckEglIpVJw9913w8jISB3v4vzh1z6VSgUeeOABuOqqq6ChoQE6OzvhrrvugpMnT5JzXMztM2HUBcgzzzyjQqGQ+qd/+if19ttvqz/8wz9UqVRK9ff3T3XV6sqNN96onnzySbV79261c+dO9du//duqp6dHjYyMVI+55557VHd3t9qwYYPatm2buv7669XHP/7xKaz11LBlyxY1b948dfXVV6t77723+vlMbp/h4WE1d+5c9cUvflFt3rxZHTx4UL300kvqwIED1WMeeeQRlUwm1fPPP6927dqlPve5z6n58+erQqEwhTWvDw8//LBqaWlRL7zwgjp06JB69tlnVTweV9/+9rerx8yk9vmP//gP9c1vflP95Cc/UQCgnnvuOVI+nrb4zGc+o6655hq1adMm9d///d/q0ksvVXfccUed7+T84Nc+6XRarVixQv3oRz9Se/fuVRs3blTLli1TS5YsIee4mNtnolyQk49ly5apVatWVfcdx1GdnZ1q3bp1U1irqWdgYEABgHrttdeUUu93+GAwqJ599tnqMe+8844CALVx48apqmbdyeVyauHCherll19Wv/7rv16dfMz09nnggQfUJz7xiZrlruuqjo4O9bd/+7fVz9LptAqHw+qHP/xhPao4pXz2s59VX/7yl8lnt912m7rzzjuVUjO7ffiP63jaYs+ePQoA1NatW6vH/PznP1eGYagTJ07Ure714GyTM86WLVsUAKgjR44opWZW+4yHC87sUi6XYfv27bBixYrqZ6ZpwooVK2Djxo1TWLOpJ5PJAABAc3MzAABs374dKpUKaatFixZBT0/PjGqrVatWwWc/+1nSDgDSPv/2b/8GS5cuhd/93d+FtrY2uPbaa+Ef//Efq+WHDh2Cvr4+0j7JZBKWL18+I9rn4x//OGzYsAHeffddAADYtWsXvPHGG3DTTTcBgLQPZjxtsXHjRkilUrB06dLqMStWrADTNGHz5s11r/NUk8lkwDAMSKVSACDtw7ngstoODg6C4zjQ3t5OPm9vb4e9e/dOUa2mHtd14b777oMbbrgBrrzySgAA6Ovrg1AoVO3cH9De3g59fX1TUMv688wzz8Cbb74JW7du9ZTN9PY5ePAgPP7447BmzRr4xje+AVu3boU/+ZM/gVAoBCtXrqy2wdnetZnQPl//+tchm83CokWLwLIscBwHHn74YbjzzjsBAGZ8+2DG0xZ9fX3Q1tZGygOBADQ3N8+49ioWi/DAAw/AHXfcUc1sK+1DueAmH8LZWbVqFezevRveeOONqa7KBcOxY8fg3nvvhZdffhkikchUV+eCw3VdWLp0Kfz1X/81AABce+21sHv3bvje974HK1eunOLaTT3/+q//Cj/4wQ/g6aefhiuuuAJ27twJ9913H3R2dkr7COdMpVKB3/u93wOlFDz++ONTXZ0LlgvO7NLa2gqWZXkiEvr7+6Gjo2OKajW1rF69Gl544QV49dVXoaurq/p5R0cHlMtlSKfT5PiZ0lbbt2+HgYEB+NjHPgaBQAACgQC89tpr8J3vfAcCgQC0t7fP6PaZPXs2XH755eSzxYsXw9GjRwEAqm0wU9+1P/3TP4Wvf/3r8IUvfAGuuuoq+IM/+AO4//77Yd26dQAg7YMZT1t0dHTAwMAAKbdtG4aHh2dMe30w8Thy5Ai8/PLL1VUPAGkfzgU3+QiFQrBkyRLYsGFD9TPXdWHDhg3Q29s7hTWrP0opWL16NTz33HPwyiuvwPz580n5kiVLIBgMkrbat28fHD16dEa01ac//Wl46623YOfOndW/pUuXwp133lndnsntc8MNN3hCs999912YO3cuAADMnz8fOjo6SPtks1nYvHnzjGif0dFRME06BFqWBa7rAoC0D2Y8bdHb2wvpdBq2b99ePeaVV14B13Vh+fLlda9zvflg4rF//374r//6L2hpaSHlM719PEy1x+vZeOaZZ1Q4HFZPPfWU2rNnj/rKV76iUqmU6uvrm+qq1ZU/+qM/UslkUv3iF79Qp06dqv6Njo5Wj7nnnntUT0+PeuWVV9S2bdtUb2+v6u3tncJaTy042kWpmd0+W7ZsUYFAQD388MNq//796gc/+IGKxWLqX/7lX6rHPPLIIyqVSqmf/vSn6le/+pW65ZZbLtpQUs7KlSvVnDlzqqG2P/nJT1Rra6v62te+Vj1mJrVPLpdTO3bsUDt27FAAoP7u7/5O7dixoxqtMZ62+MxnPqOuvfZatXnzZvXGG2+ohQsXXjShpH7tUy6X1ec+9znV1dWldu7cScbrUqlUPcfF3D4T5YKcfCil1N///d+rnp4eFQqF1LJly9SmTZumukp1BwDO+vfkk09WjykUCuqP//iPVVNTk4rFYup3fud31KlTp6au0lMMn3zM9Pb593//d3XllVeqcDisFi1apP7hH/6BlLuuqx588EHV3t6uwuGw+vSnP6327ds3RbWtL9lsVt17772qp6dHRSIRdckll6hvfvOb5MdiJrXPq6++etbxZuXKlUqp8bXF0NCQuuOOO1Q8HleJREJ96UtfUrlcbgruZvLxa59Dhw7VHK9fffXV6jku5vaZKIZSSM5PEARBEAThPHPB+XwIgiAIgnBxI5MPQRAEQRDqikw+BEEQBEGoKzL5EARBEAShrsjkQxAEQRCEuiKTD0EQBEEQ6opMPgRBEARBqCsy+RAEQRAEoa7I5EMQBEEQhLoikw9BEARBEOqKTD4EQRAEQagr/x/wYH8+I8GSOAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ship  ship  truck ship \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1.weight Parameter(6x3x5x5) Quants{-0.115|-0.108|-0.058|0.004|0.057|0.115}\n",
            "conv1.bias Parameter(6) Quants{-0.081|-0.064|-0.012|0.013|0.040|0.067}\n",
            "conv2.weight Parameter(16x6x5x5) Quants{-0.082|-0.073|-0.041|0.001|0.038|0.082}\n",
            "conv2.bias Parameter(16) Quants{-0.062|-0.041|-0.008|0.013|0.056|0.078}\n",
            "fc1.weight Parameter(120x400) Quants{-0.050|-0.045|-0.025|0.000|0.024|0.050}\n",
            "fc1.bias Parameter(120) Quants{-0.049|-0.045|-0.029|-0.002|0.026|0.049}\n",
            "fc2.weight Parameter(84x120) Quants{-0.091|-0.082|-0.046|0.000|0.044|0.091}\n",
            "fc2.bias Parameter(84) Quants{-0.091|-0.084|-0.037|-0.001|0.033|0.090}\n",
            "fc3.weight Parameter(10x84) Quants{-0.109|-0.101|-0.056|-0.004|0.053|0.109}\n",
            "fc3.bias Parameter(10) Quants{-0.082|-0.054|-0.006|0.056|0.077|0.098}\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Net:\n\tWhile copying the parameter named \"conv1.weight\", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"conv1.bias\", whose dimensions in the model are torch.Size([6]) and whose dimensions in the checkpoint are torch.Size([6]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"conv2.weight\", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"conv2.bias\", whose dimensions in the model are torch.Size([16]) and whose dimensions in the checkpoint are torch.Size([16]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc1.weight\", whose dimensions in the model are torch.Size([120, 400]) and whose dimensions in the checkpoint are torch.Size([120, 400]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc1.bias\", whose dimensions in the model are torch.Size([120]) and whose dimensions in the checkpoint are torch.Size([120]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc2.weight\", whose dimensions in the model are torch.Size([84, 120]) and whose dimensions in the checkpoint are torch.Size([84, 120]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc2.bias\", whose dimensions in the model are torch.Size([84]) and whose dimensions in the checkpoint are torch.Size([84]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc3.weight\", whose dimensions in the model are torch.Size([10, 84]) and whose dimensions in the checkpoint are torch.Size([10, 84]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc3.bias\", whose dimensions in the model are torch.Size([10]) and whose dimensions in the checkpoint are torch.Size([10]), an exception occurred : (\"name 'typestr' is not defined\",).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m d:\n\u001b[1;32m     32\u001b[0m   d[k] \u001b[38;5;241m=\u001b[39m quantise(d[k], torch\u001b[38;5;241m.\u001b[39mint16)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPost quantisation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Net:\n\tWhile copying the parameter named \"conv1.weight\", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"conv1.bias\", whose dimensions in the model are torch.Size([6]) and whose dimensions in the checkpoint are torch.Size([6]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"conv2.weight\", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"conv2.bias\", whose dimensions in the model are torch.Size([16]) and whose dimensions in the checkpoint are torch.Size([16]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc1.weight\", whose dimensions in the model are torch.Size([120, 400]) and whose dimensions in the checkpoint are torch.Size([120, 400]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc1.bias\", whose dimensions in the model are torch.Size([120]) and whose dimensions in the checkpoint are torch.Size([120]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc2.weight\", whose dimensions in the model are torch.Size([84, 120]) and whose dimensions in the checkpoint are torch.Size([84, 120]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc2.bias\", whose dimensions in the model are torch.Size([84]) and whose dimensions in the checkpoint are torch.Size([84]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc3.weight\", whose dimensions in the model are torch.Size([10, 84]) and whose dimensions in the checkpoint are torch.Size([10, 84]), an exception occurred : (\"name 'typestr' is not defined\",).\n\tWhile copying the parameter named \"fc3.bias\", whose dimensions in the model are torch.Size([10]) and whose dimensions in the checkpoint are torch.Size([10]), an exception occurred : (\"name 'typestr' is not defined\",)."
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "for n,p in net.named_parameters():\n",
        "  print(n,tensor_oneline_str(p))\n",
        "\n",
        "d = net.state_dict()\n",
        "for k in d:\n",
        "  d[k] = quantise(d[k], torch.int16)\n",
        "\n",
        "net.load_state_dict(d, assign=True)\n",
        "\n",
        "print()\n",
        "print(\"Post quantisation\")\n",
        "print(\"=================\")\n",
        "for n,p in net.named_parameters():\n",
        "  print(n,p)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.is_leaf@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.is_leaf@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.is_leaf@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.is_leaf@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.is_leaf@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.is_leaf@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.is_leaf@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.is_leaf@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.is_leaf@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.is_leaf@(<class '__main__.ScaledTensor'>,)\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.grad@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.grad@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.grad@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.grad@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.grad@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.grad@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.grad@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.grad@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.grad@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.grad@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for torch._VariableFunctionsClass.conv2d@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for torch._VariableFunctionsClass.conv2d@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for torch._C._nn.linear@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for torch._C._nn.linear@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for torch._C._nn.linear@(<class '__main__.ScaledTensor'>,)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n",
            "File \u001b[0;32m~/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 0:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
