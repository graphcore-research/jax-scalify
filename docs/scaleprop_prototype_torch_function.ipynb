{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "aAoSgjO1FhVr"
      },
      "outputs": [],
      "source": [
        "from types import MethodDescriptorType\n",
        "from typing import *\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "from torch import Tensor, tensor, nn\n",
        "from icecream import ic\n",
        "\n",
        "def numeric_info(dtype):\n",
        "  return torch.finfo(dtype) if dtype.is_floating_point else torch.iinfo(dtype)\n",
        "\n",
        "def dtype_max(dtype):\n",
        "  return numeric_info(dtype).max\n",
        "\n",
        "def _round(x : Tensor, dtype : torch.dtype) -> Tensor:\n",
        "  if dtype.is_floating_point:\n",
        "    return x.to(dtype)\n",
        "  else:\n",
        "    return torch.round(x).to(dtype)\n",
        "\n",
        "def function_str(func):\n",
        "  # https://stackoverflow.com/questions/251464/how-to-get-a-function-name-as-a-string\n",
        "  # for future expansion e.g. properties\n",
        "  if hasattr(func, '__module__'):\n",
        "    return func.__module__ + '.' + func.__qualname__\n",
        "  else:\n",
        "    return func.__qualname__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core functionality, no syntactic sugar\n",
        "\n",
        "Define a `ScaleTensorData` object, with the minimal information, and with operations\n",
        "defined cleanly, but without PyTorch Tensor integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM8RBHPwIkV2",
        "outputId": "353862f6-c2e3-4346-b891-25de11eb3550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.], dtype=torch.float16)\n",
            "tensor([1., 2., 3.], dtype=torch.float8_e4m3fn)\n",
            "ScaledTensor(0.0066964286379516125 * tensor([144., 288., 448.], dtype=torch.float8_e4m3fn))\n",
            "wasted_bits(st_data.data)=tensor(0.023) <-- of a max of 8 bits, should be wasting nearly zero\n",
            "ScaledTensor(0.0066964286379516125 * tensor([144., 288., 448.], dtype=torch.float8_e4m3fn))\n",
            "tensor([0.964, 1.928, 3.000], dtype=torch.float16) # <- rounding errors at the high end of the f8 range\n"
          ]
        }
      ],
      "source": [
        "# Core\n",
        "GPU = False\n",
        "def _uptype(dtype) -> torch.dtype:\n",
        "    map = {\n",
        "        torch.int8: torch.int16,\n",
        "        torch.int16: torch.int32,\n",
        "        torch.float8_e4m3fn: torch.float16,\n",
        "        torch.float8_e5m2: torch.float16,\n",
        "        torch.float16: torch.float16 if GPU else torch.float32,\n",
        "        torch.float32: torch.float32,\n",
        "    }\n",
        "    return map[dtype]\n",
        "\n",
        "def _to_uptype(t : Tensor) -> Tensor:\n",
        "    return torch.as_tensor(t, dtype=_uptype(t.dtype))\n",
        "\n",
        "\n",
        "def _maxval(t : Tensor):\n",
        "    return _to_uptype(t).abs().max().to(torch.float32)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ScaledTensorData:\n",
        "    data: Tensor\n",
        "    scale: Tensor\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        if not isinstance(self.scale, Tensor):\n",
        "            self.scale = tensor(self.scale)\n",
        "        assert self.scale.dtype == torch.float32\n",
        "        assert self.scale.shape == () \n",
        "        # Possible future expansion to e.g. row-scaled, column-scaled, etc, but\n",
        "        # for now, insist st_scale is a single-element tensor\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"ScaledTensor({self.scale} * {self.data})\"\n",
        "\n",
        "    def to_tensor(self, dtype: torch.dtype) -> Tensor:\n",
        "        return self.data.to(dtype) * self.scale.to(dtype)\n",
        "\n",
        "    @property\n",
        "    def shape(self) -> torch.Size:\n",
        "        return self.data.shape\n",
        "    \n",
        "    @property\n",
        "    def dtype_max(self):\n",
        "        return dtype_max(self.data.dtype)\n",
        "\n",
        "def st_quantise(x: Tensor, dtype: torch.dtype) -> ScaledTensorData:\n",
        "    \"\"\"\n",
        "    Rescale so that max(|data|) == maxFinite(data.dtype)\n",
        "\n",
        "    \"\"\"\n",
        "    maxval = _maxval(x)\n",
        "    if maxval == 0:\n",
        "        # Tensor is all zeros - set scale to 1\n",
        "        scale = Tensor(1.0, dtype=torch.float32)\n",
        "    else:\n",
        "        # Scale so that largest element is the largest finite value of dtype\n",
        "        scale = maxval / dtype_max(dtype)\n",
        "\n",
        "    return ScaledTensorData(_round(x / scale, dtype), scale)\n",
        "\n",
        "\n",
        "def st_requantise(st: ScaledTensorData) -> ScaledTensorData:\n",
        "    \"\"\"\n",
        "    Rescale so that max(|data|) == maxFinite(data.dtype)\n",
        "\n",
        "    Equivalent to quantise(st.to_tensor(torch.float32)) but avoids the conversion\n",
        "\n",
        "    Returned tensor may share its data with input tensor\n",
        "    \"\"\"\n",
        "    maxdataval = _maxval(st.data)\n",
        "    if maxdataval == 0:\n",
        "        # All zero, reset scale to 1\n",
        "        return ScaledTensorData(st.data, st.scale)\n",
        "    else:\n",
        "         rescale = maxdataval / st.dtype_max\n",
        "    return ScaledTensorData((_to_uptype(st.data) * (1 / rescale)).to(st.data.dtype), st.scale * rescale)\n",
        "\n",
        "\n",
        "def wasted_bits(st_data, maxval = None) -> float:\n",
        "    \"\"\"\n",
        "    By how much is tensor `st_data` not using the full dynamic range of its dtype?\n",
        "\n",
        "    E.g.\n",
        "       t = torch.tensor([1,2,-16], dtype=torch.int8)\n",
        "\n",
        "    Is using only 5 (4 + sign) of the available 8 bits.\n",
        "    Therefore \n",
        "       wasted_bits(t) == 3 == 8-3\n",
        "\n",
        "    Optional argument maxval, if the maximum value in the tensor has already \n",
        "    been computed, perhaps with a higher-accuracy method (e.g. pre-rounding)\n",
        "    \"\"\"\n",
        "    if maxval is None:\n",
        "        maxval = _maxval(st_data)\n",
        "\n",
        "    maxval = maxval.to(st_data.dtype)\n",
        "    dtype_bits = numeric_info(st_data.dtype).bits\n",
        "    if maxval == 0:\n",
        "        # All values zero -> all bits are wasted\n",
        "        return dtype_bits\n",
        "    \n",
        "    # Otherwise, how many bits is maxval using.\n",
        "    if st_data.dtype.is_floating_point:\n",
        "      # Convert maxval to integer of the same bitwidth\n",
        "      ints = {\n",
        "          8: torch.int8,\n",
        "          16: torch.int16,\n",
        "          32: torch.int32\n",
        "      }\n",
        "      maxval = maxval.view(ints[dtype_bits])\n",
        "\n",
        "    # Assuming a signed type, max usable bits are dtype_bits-1\n",
        "    return dtype_bits-1 - torch.log2(maxval)\n",
        "\n",
        "def test_wasted_bits():\n",
        "    t = torch.tensor([1,2,-16], dtype=torch.int8)\n",
        "    assert wasted_bits(t) == 3\n",
        "test_wasted_bits()\n",
        "\n",
        "### Quick testing\n",
        "f16 = tensor([1, 2, 3], dtype=torch.float16)\n",
        "f8_t = torch.float8_e4m3fn\n",
        "print(f16)\n",
        "print(f16.to(f8_t))\n",
        "\n",
        "st_data = st_quantise(f16, f8_t)\n",
        "\n",
        "print(st_data)\n",
        "print(f'{wasted_bits(st_data.data)=} <-- of a max of {numeric_info(f8_t).bits} bits, should be wasting nearly zero')\n",
        "print(st_requantise(st_data))\n",
        "print(st_data.to_tensor(f16.dtype), \"# <- rounding errors at the high end of the f8 range\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Operators, without syntactic sugar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| st1: ScaledTensor(0.5 * 32)\n",
            "ic| st2: ScaledTensor(0.25 * 64)\n",
            "ic| st_add(st1, st2): ScaledTensor(0.75 * 42)\n",
            "ic| f32(st_add(st1, st2)): tensor(31.500)\n",
            "ic| f32(st1) + f32(st2): tensor(32.)\n",
            "ic| st3: ScaledTensor(0.019876088947057724 * tensor([[-21,  78,  17,  ...,   8, -53,  41],\n",
            "                 [ 40, -30, -38,  ..., -75,  52,   0]], dtype=torch.int8))\n",
            "ic| st4: ScaledTensor(0.019780419766902924 * tensor([[-28,   3,  -9],\n",
            "                 [-72,   8, -42],\n",
            "                 [ 13, -67,  25],\n",
            "                 ...,\n",
            "                 [ 60, -50, -25],\n",
            "                 [-51,   8, -54],\n",
            "                 [-64, -37,  10]], dtype=torch.int8))\n",
            "ic| st_matmul(st3, st4): ScaledTensor(1.5977916717529297 * tensor([[-4, -6,  0],\n",
            "                                 [ 3,  2, -6]], dtype=torch.int8))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "st_matmul: WARNING: Very bad maxval estimate 202.91954040527344 vs 10.810256004333496, 18.771020889282227x too large - will lose at least 4.415037155151367 bits of precision\n",
            "st_matmul: WARNING: Very bad maxval estimate 202.91954040527344 vs 10.810256004333496, 18.771020889282227x too large - will lose at least 4.415037155151367 bits of precision\n",
            "f32(st_matmul(st3, st4)) = tensor([[-6.391, -9.587,  0.000],\n",
            "        [ 4.793,  3.196, -9.587]]) <-- quantized\n",
            "f32(st3) @ f32(st4) = tensor([[ -6.410, -10.810,  -0.515],\n",
            "        [  6.196,   4.459, -10.164]]) <-- intermediate\n",
            "t3 @ t4 = tensor([[ -6.381, -10.807,  -0.489],\n",
            "        [  6.171,   4.462, -10.129]]) <-- exact\n"
          ]
        }
      ],
      "source": [
        "# Ops\n",
        "# - Use worst-case scaling rules (no overflow!)\n",
        "# - Placeholder impl (fast impl requires custom kernels)\n",
        "# - No autograd support\n",
        "\n",
        "def st_add(a: ScaledTensorData, b: ScaledTensorData) -> ScaledTensorData:\n",
        "    out_dtype = a.data.dtype\n",
        "    scale = a.scale + b.scale\n",
        "    data = (_to_uptype(a.data) * (a.scale / scale) + _to_uptype(b.data) * (b.scale / scale)).to(out_dtype)\n",
        "    return ScaledTensorData(data, scale)\n",
        "\n",
        "\n",
        "def st_matmul(a: ScaledTensorData, b: ScaledTensorData, debug = True) -> ScaledTensorData:\n",
        "    assert a.data.dtype == b.data.dtype\n",
        "    in_dtype = a.data.dtype\n",
        "    out_dtype = a.data.dtype\n",
        "\n",
        "    a_maxval = a.scale * a.dtype_max\n",
        "    b_maxval = b.scale * b.dtype_max\n",
        "\n",
        "    # Predicted maxval for NxK @ KxM\n",
        "    K = a.shape[-1]\n",
        "    out_maxval_estimate = a_maxval * b_maxval * K\n",
        "\n",
        "    out_scale = out_maxval_estimate / dtype_max(out_dtype) \n",
        "\n",
        "    # Derivation of matmul scale factors:\n",
        "    # (ad * as) @ (bd * bs) = (ad @ bd) * (as * bs)\n",
        "    #                       = (ad @ bd) * (as * bs / os * os)\n",
        "    #                       = (ad @ bd * as * bs / os) * os\n",
        "    #                       = (ad @ bd * rat) * os\n",
        "    #                         where rat = as * bs / os\n",
        "    #                       = (ad * sqrt(rat)) @ (bd * sqrt(rat)) * os\n",
        "\n",
        "    rat = a.scale * b.scale / out_scale\n",
        "\n",
        "    if numeric_info(in_dtype).bits < numeric_info(_uptype(in_dtype)).bits:\n",
        "        # Assume low-precision muls will accumulate to uptype, so won't overflow\n",
        "        # to simulate this on cpu, uptype before the matmul;\n",
        "        # on appropriate hardware (e.g. graphcore, h100), call the special matmul\n",
        "        adbd = _to_uptype(a.data) @ _to_uptype(b.data)\n",
        "        if debug:\n",
        "            out_maxval = _maxval(adbd) * (a.scale * b.scale)\n",
        "        out_data = adbd * rat\n",
        "        out_data = out_data.to(out_dtype)\n",
        "    else:\n",
        "        # Inputs are in 16+ bits, and we know the products will certainly \n",
        "        # overflow, as they are scaled to dtype_max, so downscale before multiplying \n",
        "        sqrt_rat = torch.sqrt(rat)\n",
        "        a_down = _to_uptype(a.data) * sqrt_rat\n",
        "        b_down = _to_uptype(b.data) * sqrt_rat\n",
        "        out_data = a_down @ b_down\n",
        "        if debug:\n",
        "            out_maxval = _maxval(out_data) * out_scale\n",
        "        out_data = out_data.to(out_dtype)\n",
        "\n",
        "    # debug check how bad out_maxval_estimate was\n",
        "    if debug:\n",
        "        assert out_maxval_estimate > out_maxval # Should always be an upper bound\n",
        "        wasted = wasted_bits(out_data)\n",
        "        if wasted > numeric_info(out_dtype).bits/2:\n",
        "            print(f'st_matmul: WARNING: Very bad maxval estimate {out_maxval_estimate} vs {out_maxval}, {out_maxval_estimate/out_maxval}x too large - will lose at least {wasted} bits of precision')\n",
        "\n",
        "        if _maxval(out_data) == 0:\n",
        "            raise ValueError(\"All-data zero - rerun with debug and view st_matmul: WARNING above\")\n",
        "\n",
        "    return ScaledTensorData(out_data, out_scale)\n",
        "\n",
        "\n",
        "def st_relu(a: ScaledTensorData) -> ScaledTensorData:\n",
        "    data = nn.functional.relu(a.data).to(a.data.dtype)\n",
        "    return ScaledTensorData(data, a.scale)\n",
        "\n",
        "# Check operators behae sensibly\n",
        "st1 = ScaledTensorData(tensor(32, dtype=torch.int8), 0.5)\n",
        "st2 = ScaledTensorData(tensor(64, dtype=torch.int8), 0.25)\n",
        "\n",
        "f32 = lambda x: x.to_tensor(torch.float32) if isinstance(x, ScaledTensorData) else x.to(dtype=torch.float32)\n",
        "\n",
        "torch.set_printoptions(precision=3, threshold=32)\n",
        "\n",
        "ic(st1)\n",
        "ic(st2)\n",
        "ic(st_add(st1, st2))\n",
        "ic(f32(st_add(st1, st2)) )\n",
        "ic(f32(st1) + f32(st2))\n",
        "\n",
        "hidden = 32\n",
        "t3 = torch.randn(2, hidden)\n",
        "t4 = torch.randn(hidden, 3)\n",
        "st3 = st_quantise(t3, torch.int8)\n",
        "st4 = st_quantise(t4, torch.int8)\n",
        "ic(st3)\n",
        "ic(st4)\n",
        "ic(st_matmul(st3, st4))\n",
        "print(f'{f32(st_matmul(st3, st4)) = } <-- quantized')\n",
        "print(f'{f32(st3) @ f32(st4) = } <-- intermediate')\n",
        "print(f'{t3 @ t4 = } <-- exact')\n",
        "\n",
        "# st5 = st_quantise(tensor([-2, 3, -0.06, 4]), torch.int8)\n",
        "# ic(st5, f32(st5))\n",
        "# rt5 = st_relu(st5)\n",
        "# ic(rt5, f32(rt5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {},
      "outputs": [],
      "source": [
        "### [Aside: Possibly surprising rounding]\n",
        "\n",
        "# print('Starting point: ', tensor([-2, 0.09, 4]))\n",
        "\n",
        "# st5 = st_quantise(tensor([-2, 0.09, 4]), torch.int8)\n",
        "# print(f'{st5=} {f32(st5)=} <-- 0.0900 input rounds to 0.0630')\n",
        "\n",
        "# print('So, put in 0.0630 to begin with')\n",
        "# st5 = st_quantise(tensor([-2, 0.0630, 4]), torch.int8) \n",
        "# print(f'{st5=} {f32(st5)=} <-- great, 0.0630 rounds to 0.0630')\n",
        "\n",
        "# print('But now, put in 0.06')\n",
        "# st5 = st_quantise(tensor([-2, 0.06, 4]), torch.int8) \n",
        "# print(f'{st5=} {f32(st5)=} <-- Eh, 0.06 rounds down to 0.0315?')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Override Tensor operations in subclass\n",
        "\n",
        "We define a `ScaledTensor` object that behaves like a torch Tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.], dtype=torch.float16)\n",
            "tensor([1, 2, 3], dtype=torch.int8)\n",
            "ScaledTensor(0.023622047156095505 * tensor([ 42,  85, 127], dtype=torch.int8))\n",
            "ScaledTensor(0.023622047156095505 * tensor([ 42,  85, 127], dtype=torch.int8))\n",
            "Rounding errors at the high end of the f8 range: tensor([0.992, 2.008, 3.000], dtype=torch.float16)\n",
            "Viewing as a tensor should show NaN: tensor(nan)\n"
          ]
        }
      ],
      "source": [
        "# See https://pytorch.org/docs/stable/notes/extending.html#extending-torch-with-a-tensor-like-type\n",
        "class ScaledTensor(Tensor):\n",
        "    def __init__(self, st_data : ScaledTensorData):\n",
        "        super().__init__()\n",
        "        self.st = st_data\n",
        "        \n",
        "    def __new__(cls, st_data, *args, **kwargs):\n",
        "        # Ensure that if the tensor is ever viewed as the base class, it is NaN\n",
        "        return super().__new__(cls, tensor(torch.nan), *args, **kwargs)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"{self.st}\"\n",
        "\n",
        "    @property\n",
        "    def shape(self) -> torch.Size:\n",
        "        return self.st.shape\n",
        "\n",
        "    @classmethod\n",
        "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
        "        kwargs = kwargs or {}\n",
        "        # Have we registered a handler?\n",
        "        if func in cls.HANDLED_FUNCTIONS:\n",
        "          ret = cls.HANDLED_FUNCTIONS[func](*args, **kwargs)\n",
        "          if ret != NotImplemented:\n",
        "              return ret\n",
        "          # Otherwise drop through to the fallback\n",
        "\n",
        "        # Fallback: Convert to float32 and call func\n",
        "        print(f\"ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for {function_str(func)}@{types}\")\n",
        "\n",
        "        def to_tensor_if_scaled(t) -> Tensor:\n",
        "            if isinstance(t, ScaledTensor):\n",
        "                return t.st.to_tensor(torch.float32)\n",
        "            else:\n",
        "                return t\n",
        "\n",
        "        new_args = tuple(to_tensor_if_scaled(a) for a in args)\n",
        "        new_kwargs = { k:to_tensor_if_scaled(v) for (k,v) in kwargs.items() }\n",
        "\n",
        "        # We don't want the auto-downcast of \n",
        "        #    ret = super().__torch_function__(func, types, new_args, new_kwargs)\n",
        "        # because the super()'s handler will construct other types of tensor which\n",
        "        # don't simply reinterpret to a ScaledTensor.\n",
        "\n",
        "        with torch._C.DisableTorchFunctionSubclass():\n",
        "            return func(*new_args, **new_kwargs)\n",
        "\n",
        "ScaledTensor.HANDLED_FUNCTIONS = {}\n",
        "\n",
        "# Forward the ScaledTensorData ops above on the ScaledTensor type\n",
        "def quantise(x: Tensor, dtype: torch.dtype) -> ScaledTensor:\n",
        "    return ScaledTensor(st_quantise(x, dtype))\n",
        "\n",
        "def requantise(st) -> ScaledTensor:\n",
        "    return ScaledTensor(st_requantise(st.st))\n",
        "\n",
        "# Check basic to/from Subclass\n",
        "f16 = tensor([1, 2, 3], dtype=torch.float16)\n",
        "f8_t = torch.int8\n",
        "print(f16)\n",
        "print(f16.to(f8_t))\n",
        "\n",
        "st = quantise(f16, f8_t)\n",
        "\n",
        "print(st)\n",
        "print(requantise(st))\n",
        "print('Rounding errors at the high end of the f8 range:', st.st.to_tensor(f16.dtype))\n",
        "print('Viewing as a tensor should show NaN:', Tensor(st))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now overrides work, but just punt up to f32 for all ops\n",
        "\n",
        "We will do some adds/multiplies etc, and note that the torch function\n",
        "implementation issues a sequence of \"WARNING: Upcasting to float32\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.add@(<class '__main__.ScaledTensor'>,)\n",
            "tensor([2.992, 4.008, 5.000])\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.mul@(<class '__main__.ScaledTensor'>,)\n",
            "tensor([1.984, 4.016, 6.000])\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.add@(<class '__main__.ScaledTensor'>,)\n",
            "tensor([1.984, 4.016, 6.000])\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.matmul@(<class '__main__.ScaledTensor'>,)\n",
            "f32(st3 @ st4)       = tensor([[60000., 60000., 60000., 60000.],\n",
            "        [60000., 60000., 60000., 60000.]]) <-- should be ~48200 quantized, but was done in f32 so exact\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.to@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for _TensorBase.to@(<class '__main__.ScaledTensor'>,)\n",
            "f32(st3) @ f32(st4)  = tensor([[60000., 60000., 60000., 60000.],\n",
            "        [60000., 60000., 60000., 60000.]]) <-- 60000 exact\n"
          ]
        }
      ],
      "source": [
        "print(st + 2)\n",
        "print(2 * st)\n",
        "print(st + st)\n",
        "\n",
        "st3 = quantise(torch.full((2, 3), 100.0), torch.int8)\n",
        "st4 = quantise(torch.full((3, 4), 200.0), torch.int8)\n",
        "\n",
        "print(f'{f32(st3 @ st4)       = } <-- should be ~48200 quantized, but was done in f32 so exact')\n",
        "print(f'{f32(st3) @ f32(st4)  = } <-- 60000 exact')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| st3: ScaledTensor(0.787401556968689 * tensor([[127, 127, 127],\n",
            "                 [127, 127, 127]], dtype=torch.int8))\n",
            "ic| st4: ScaledTensor(1.574803113937378 * tensor([[127, 127, 127, 127],\n",
            "                 [127, 127, 127, 127],\n",
            "                 [127, 127, 127, 127]], dtype=torch.int8))\n",
            "ic| st3 @ st4: ScaledTensor(472.4409484863281 * tensor([[-45, -45, -45, -45],\n",
            "                       [-45, -45, -45, -45]], dtype=torch.int8))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f32(st3 @ st4)       = tensor([[-21259.842, -21259.842, -21259.842, -21259.842],\n",
            "        [-21259.842, -21259.842, -21259.842, -21259.842]]) <-- ~48200 quantized\n",
            "f32(st3) @ f32(st4)  = tensor([[60000., 60000., 60000., 60000.],\n",
            "        [60000., 60000., 60000., 60000.]]) <-- 60000 exact\n",
            "ScaledTensor(0.023622047156095505 * tensor([ 42,  85, 127], dtype=torch.int8))\n"
          ]
        }
      ],
      "source": [
        "def torch_function_override(cls, funcs):\n",
        "    funcs = funcs if isinstance(funcs, tuple) else (funcs,)\n",
        "\n",
        "    def doit(impl):\n",
        "        # This work is also done by functools.wraps, but it doesn't make sensible names,\n",
        "        # and doesn't fix qualname\n",
        "        if impl.__name__ == '_':\n",
        "            impl.__name__ = f'@torch_function_override({cls.__name__}, {funcs})'\n",
        "            if impl.__qualname__ != '_':\n",
        "                print(f'torch_function_override: NOTE: {impl.__qualname__} not overridden')\n",
        "        if impl.__qualname__ == '_':\n",
        "            impl.__qualname__ = f'torch_function_override({cls.__name__}, {funcs})'\n",
        "\n",
        "        # Record it in the dictionary\n",
        "        for func in funcs:\n",
        "          cls.HANDLED_FUNCTIONS[func] = impl\n",
        "\n",
        "    return doit\n",
        "\n",
        "@torch_function_override(ScaledTensor, Tensor.add)\n",
        "def _(a:Tensor, b: Tensor) -> Tensor:\n",
        "    if not (isinstance(a, ScaledTensor) and isinstance(b, ScaledTensor)):\n",
        "        print(f'ScaledTensor: Punting on {type(a), type(b)=}')\n",
        "        return NotImplemented\n",
        "\n",
        "    return ScaledTensor(st_add(a.st, b.st))\n",
        "\n",
        "@torch_function_override(ScaledTensor, Tensor.matmul)\n",
        "def _(a:Tensor, b: Tensor) -> Tensor:\n",
        "    if not (isinstance(a, ScaledTensor) and isinstance(b, ScaledTensor)):\n",
        "        return NotImplemented\n",
        "\n",
        "    return ScaledTensor(st_matmul(a.st, b.st))\n",
        "\n",
        "@torch_function_override(ScaledTensor, Tensor.to)\n",
        "def _(a:Tensor, dtype:torch.dtype) -> Tensor:\n",
        "    assert isinstance(a, ScaledTensor)\n",
        "    return a.st.to_tensor(dtype)\n",
        "\n",
        "@torch_function_override(ScaledTensor, (Tensor.relu, nn.functional.relu))\n",
        "def _(a:Tensor, inplace = False) -> Tensor:\n",
        "    assert isinstance(a, ScaledTensor)\n",
        "    assert not inplace\n",
        "    return ScaledTensor(st_relu(a.st))\n",
        "\n",
        "ic(st3)\n",
        "ic(st4)\n",
        "ic(st3 @ st4 )\n",
        "print(f'{f32(st3 @ st4)       = } <-- ~48200 quantized')\n",
        "print(f'{f32(st3) @ f32(st4)  = } <-- 60000 exact')\n",
        "\n",
        "print(nn.functional.relu(st))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# And in a network..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtrOg8FMpDb3",
        "outputId": "cd7bc783-0567-4c37-908d-56ba2ec2e1a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| self.W1: ScaledTensor(0.042678870260715485 * tensor([[-19, -18,  -2,  ...,  20,  12,  18],\n",
            "                     [  7, -54, -29,  ..., -25, -14,  51],\n",
            "                     [  2,   6,  -8,  ...,   7, -13,  -5],\n",
            "                     ...,\n",
            "                     [-11,  46,  -6,  ..., -51,  22,  10],\n",
            "                     [ -9,   6, -29,  ..., -24,  49,   5],\n",
            "                     [ -7, -36, -25,  ...,   3,  29, -18]], dtype=torch.int8))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "st_matmul: WARNING: Very bad maxval estimate 20579.41015625 vs 40.82963180541992, 504.03125x too large - will lose at least 8 bits of precision\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "All-data zero - rerun with debug and view st_matmul: WARNING above",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[251], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m module \u001b[38;5;241m=\u001b[39m FFN(hidden_size, storage_type)\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m quantise(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, hidden_size), storage_type)\n\u001b[0;32m---> 30\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
            "File \u001b[0;32m~/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/micromamba/envs/scaledarith/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[251], line 13\u001b[0m, in \u001b[0;36mFFN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Union[Tensor, ScaledTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, ScaledTensor]:\n\u001b[0;32m---> 13\u001b[0m     y \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW0\u001b[49m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, ScaledTensor):\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwasted bits #1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwasted_bits(y)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[246], line 23\u001b[0m, in \u001b[0;36mScaledTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Have we registered a handler?\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mHANDLED_FUNCTIONS:\n\u001b[0;32m---> 23\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHANDLED_FUNCTIONS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m     25\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
            "Cell \u001b[0;32mIn[248], line 33\u001b[0m, in \u001b[0;36m_\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(a, ScaledTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, ScaledTensor)):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ScaledTensor(\u001b[43mst_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mst\u001b[49m\u001b[43m)\u001b[49m)\n",
            "Cell \u001b[0;32mIn[244], line 65\u001b[0m, in \u001b[0;36mst_matmul\u001b[0;34m(a, b, debug)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mst_matmul: WARNING: Very bad maxval estimate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_maxval_estimate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_maxval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_maxval_estimate\u001b[38;5;241m/\u001b[39mout_maxval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx too large - will lose at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwasted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bits of precision\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _maxval(out_data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll-data zero - rerun with debug and view st_matmul: WARNING above\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ScaledTensorData(out_data, out_scale)\n",
            "\u001b[0;31mValueError\u001b[0m: All-data zero - rerun with debug and view st_matmul: WARNING above"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "from icecream import ic\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, hidden_size: int, dtype: torch.dtype):\n",
        "        super().__init__()\n",
        "        q = lambda x: quantise(x, dtype)\n",
        "        self.W0 = q(torch.randn(hidden_size, 4*hidden_size))\n",
        "        self.W1 = q(torch.randn(4*hidden_size, hidden_size))\n",
        "        ic(self.W1)\n",
        "\n",
        "    def forward(self, x: Union[Tensor, ScaledTensor]) -> Union[Tensor, ScaledTensor]:\n",
        "        y = nn.functional.relu(x @ self.W0)\n",
        "\n",
        "        if isinstance(y, ScaledTensor):\n",
        "            print(f\"wasted bits #1: {wasted_bits(y)}\")\n",
        "            y = requantise(y)\n",
        "\n",
        "        y = y @ self.W1\n",
        "\n",
        "        if isinstance(y, ScaledTensor):\n",
        "            print(f\"wasted bits #2: {wasted_bits(y)}\")\n",
        "\n",
        "        return y\n",
        "\n",
        "hidden_size = 1024\n",
        "storage_type = torch.int8\n",
        "module = FFN(hidden_size, storage_type)\n",
        "x = quantise(torch.randn(10, hidden_size), storage_type)\n",
        "result = module(x)\n",
        "print()\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
