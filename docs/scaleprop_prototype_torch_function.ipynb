{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "aAoSgjO1FhVr"
      },
      "outputs": [],
      "source": [
        "from types import MethodDescriptorType\n",
        "from typing import *\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "from torch import Tensor, tensor, nn\n",
        "\n",
        "def numeric_info(dtype):\n",
        "  return torch.finfo(dtype) if dtype.is_floating_point else torch.iinfo(dtype)\n",
        "\n",
        "def _maxval(dtype):\n",
        "  return numeric_info(dtype).max"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First, core functionality\n",
        "\n",
        "Define a `ScaleTensorData` object, with the minimal information, and with operations\n",
        "defined cleanly, but without PyTorch Tensor integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM8RBHPwIkV2",
        "outputId": "353862f6-c2e3-4346-b891-25de11eb3550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.], dtype=torch.float16)\n",
            "tensor([1., 2., 3.], dtype=torch.float8_e4m3fn)\n",
            "ScaledTensor(0.0066964286379516125 * tensor([144., 288., 448.], dtype=torch.float8_e4m3fn))\n",
            "ScaledTensor(0.0066964286379516125 * tensor([144., 288., 448.], dtype=torch.float8_e4m3fn))\n",
            "tensor([0.9639, 1.9277, 3.0000], dtype=torch.float16) # <- rounding errors at the high end of the f8 range\n"
          ]
        }
      ],
      "source": [
        "# Core\n",
        "@dataclass\n",
        "class ScaledTensorData:\n",
        "    data: Tensor\n",
        "    scale: Tensor\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        if not isinstance(self.scale, Tensor):\n",
        "            self.scale = tensor(self.scale)\n",
        "        assert self.scale.dtype == torch.float32\n",
        "        assert self.scale.shape == () \n",
        "        # Possible future expansion to e.g. row-scaled, column-scaled, etc, but\n",
        "        # for now, insist st_scale is a single-element tensor\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"ScaledTensor({self.scale} * {self.data})\"\n",
        "\n",
        "    def to_tensor(self, dtype: torch.dtype) -> Tensor:\n",
        "        return self.data.to(dtype) * self.scale.to(dtype)\n",
        "\n",
        "    @property\n",
        "    def shape(self) -> torch.Size:\n",
        "        return self.data.shape\n",
        "    \n",
        "    @property\n",
        "    def dtype_max(self):\n",
        "        return numeric_info(self.data.dtype).max\n",
        "\n",
        "GPU = False\n",
        "def _uptype(dtype) -> torch.dtype:\n",
        "    if numeric_info(dtype).bits < 16:\n",
        "        return torch.float16 if GPU else torch.float32\n",
        "    else:\n",
        "        return dtype\n",
        "    \n",
        "def _to_uptype(t : Tensor) -> Tensor:\n",
        "    return torch.as_tensor(t, dtype=_uptype(t.dtype))\n",
        "\n",
        "def _maxabs(t : Tensor):\n",
        "    return _to_uptype(t).abs().max().to(torch.float32)\n",
        "\n",
        "def st_quantise(x: Tensor, dtype: torch.dtype) -> ScaledTensorData:\n",
        "    \"\"\"\n",
        "    Rescale so that max(|data|) == maxFinite(data.dtype)\n",
        "\n",
        "    \"\"\"\n",
        "    maxval = _maxabs(x)\n",
        "    if maxval == 0:\n",
        "        # Tensor is all zeros - set scale to 1\n",
        "        scale = Tensor(1.0, dtype=torch.float32)\n",
        "    else:\n",
        "        # Scale so that largest element is the largest finite value of dtype\n",
        "        scale = maxval / _maxval(dtype)\n",
        "\n",
        "    return ScaledTensorData((x / scale).to(dtype), scale)\n",
        "\n",
        "def st_requantise(st: ScaledTensorData) -> ScaledTensorData:\n",
        "    \"\"\"\n",
        "    Rescale so that max(|data|) == maxFinite(data.dtype)\n",
        "\n",
        "    Equivalent to quantise(st.to_tensor(torch.float32)) but avoids the conversion\n",
        "\n",
        "    Returned tensor may share its data with input tensor\n",
        "    \"\"\"\n",
        "    maxdataval = _maxabs(st.data)\n",
        "    if maxdataval == 0:\n",
        "        # All zero, reset scale to 1\n",
        "        return ScaledTensorData(st.data, st.scale)\n",
        "    else:\n",
        "         rescale = maxdataval / st.dtype_max\n",
        "    return ScaledTensorData((_to_uptype(st.data) * (1 / rescale)).to(st.data.dtype), st.scale * rescale)\n",
        "\n",
        "### Quick testing\n",
        "f16 = tensor([1, 2, 3], dtype=torch.float16)\n",
        "f8_t = torch.float8_e4m3fn\n",
        "print(f16)\n",
        "print(f16.to(f8_t))\n",
        "\n",
        "st_data = st_quantise(f16, f8_t)\n",
        "\n",
        "print(st_data)\n",
        "print(st_requantise(st_data))\n",
        "print(st_data.to_tensor(f16.dtype), \"# <- rounding errors at the high end of the f8 range\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "st1=ScaledTensor(0.5 * 32)\n",
            "st2=ScaledTensor(0.25 * 64)\n",
            "st_add(st1, st2)=ScaledTensor(0.75 * 42)\n",
            "f32(st_add(st1, st2)) = tensor(31.5000)\n",
            "f32(st1) + f32(st2)   = tensor(32.)\n",
            "st3=ScaledTensor(0.787401556968689 * tensor([[127, 127, 127],\n",
            "        [127, 127, 127]], dtype=torch.int8))\n",
            "st4=ScaledTensor(1.574803113937378 * tensor([[127, 127, 127, 127],\n",
            "        [127, 127, 127, 127],\n",
            "        [127, 127, 127, 127]], dtype=torch.int8))\n",
            "st_matmul(st3, st4) = ScaledTensor(472.4409484863281 * tensor([[102, 102, 102, 102],\n",
            "        [102, 102, 102, 102]], dtype=torch.int8))\n",
            "f32(st_matmul(st3, st4)) = tensor([[48188.9766, 48188.9766, 48188.9766, 48188.9766],\n",
            "        [48188.9766, 48188.9766, 48188.9766, 48188.9766]]) <-- ~48200 quantized\n",
            "f32(st3) @ f32(st4)      = tensor([[60000., 60000., 60000., 60000.],\n",
            "        [60000., 60000., 60000., 60000.]]) <-- 60000 exact\n"
          ]
        }
      ],
      "source": [
        "# Ops\n",
        "# - Use worst-case scaling rules (no overflow!)\n",
        "# - Placeholder impl (fast impl requires custom kernels)\n",
        "# - No autograd support\n",
        "\n",
        "def st_add(a: ScaledTensorData, b: ScaledTensorData) -> ScaledTensorData:\n",
        "    out_dtype = a.data.dtype\n",
        "    scale = a.scale + b.scale\n",
        "    data = (a.data * (a.scale / scale) + b.data * (b.scale / scale)).to(out_dtype)\n",
        "    return ScaledTensorData(data, scale)\n",
        "\n",
        "\n",
        "def st_matmul(a: ScaledTensorData, b: ScaledTensorData) -> ScaledTensorData:\n",
        "    # simplified version: assume low-precision a.data and b.data are multiplied to \n",
        "    # higher precision (e.g. f16), then downscaled.\n",
        "    assert a.data.dtype == b.data.dtype\n",
        "    out_dtype = a.data.dtype\n",
        "\n",
        "    a_maxval = a.scale * a.dtype_max\n",
        "    b_maxval = b.scale * b.dtype_max\n",
        "\n",
        "    # Predicted maxval for NxK @ KxM\n",
        "    K = a.shape[-1]\n",
        "    out_maxval = a_maxval * b_maxval * K\n",
        "\n",
        "    out_scale = out_maxval / _maxval(out_dtype) \n",
        "\n",
        "    # Assume low-precision muls will accumulate to uptype\n",
        "    # to simulate this on cpu, uptype before the matmul;\n",
        "    # on appropriate hardware (e.g. graphcore, h100), call the special matmul\n",
        "    out_data = (_to_uptype(a.data) @ _to_uptype(b.data)) * (1 / out_scale)\n",
        "    out_data = out_data.to(out_dtype)\n",
        "\n",
        "    return ScaledTensorData(out_data, out_scale)\n",
        "\n",
        "\n",
        "def st_relu(a: ScaledTensorData) -> ScaledTensorData:\n",
        "    return ScaledTensorData(nn.functional.relu(a.data), a.scale, a.dtype)\n",
        "\n",
        "st1 = ScaledTensorData(tensor(32, dtype=torch.int8), 0.5)\n",
        "st2 = ScaledTensorData(tensor(64, dtype=torch.int8), 0.25)\n",
        "\n",
        "f32 = lambda x: x.to_tensor(torch.float32) if isinstance(x, ScaledTensorData) else x.to(dtype=torch.float32)\n",
        "\n",
        "print(f'{st1=}')\n",
        "print(f'{st2=}')\n",
        "print(f'{st_add(st1, st2)=}')\n",
        "print(f'{f32(st_add(st1, st2)) = }')\n",
        "print(f'{f32(st1) + f32(st2)   = }')\n",
        "\n",
        "st3 = st_quantise(torch.full((2, 3), 100.0), torch.int8)\n",
        "st4 = st_quantise(torch.full((3, 4), 200.0), torch.int8)\n",
        "print(f'{st3=}')\n",
        "print(f'{st4=}')\n",
        "print(f'{st_matmul(st3, st4) = }')\n",
        "print(f'{f32(st_matmul(st3, st4)) = } <-- ~48200 quantized')\n",
        "print(f'{f32(st3) @ f32(st4)      = } <-- 60000 exact')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now plug into Tensor subclass for convenience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.], dtype=torch.float16)\n",
            "tensor([1, 2, 3], dtype=torch.int8)\n",
            "ScaledTensor(0.023622047156095505 * tensor([ 42,  84, 127], dtype=torch.int8))\n",
            "ScaledTensor(0.023622047156095505 * tensor([ 42,  84, 127], dtype=torch.int8))\n",
            "Rounding errors at the high end of the f8 range: tensor([0.9922, 1.9844, 3.0000], dtype=torch.float16)\n",
            "Viewing as a tensor should show NaN: tensor(nan)\n"
          ]
        }
      ],
      "source": [
        "# https://pytorch.org/docs/stable/notes/extending.html#extending-torch-with-a-tensor-like-type\n",
        "class ScaledTensor(Tensor):\n",
        "    def __init__(self, st_data : ScaledTensorData):\n",
        "        super().__init__()\n",
        "        self.st = st_data\n",
        "        \n",
        "    def __new__(cls, st_data, *args, **kwargs):\n",
        "        # Ensure that if the tensor is ever viewed as the base class, it is NaN\n",
        "        return super().__new__(cls, tensor(torch.nan), *args, **kwargs)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"{self.st}\"\n",
        "\n",
        "    def to_tensor(self, dtype: torch.dtype) -> Tensor:\n",
        "        return self.st.to_tensor(dtype)\n",
        "\n",
        "    @property\n",
        "    def shape(self) -> torch.Size:\n",
        "        return self.st.shape\n",
        "\n",
        "    @classmethod\n",
        "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
        "        kwargs = kwargs or {}\n",
        "        if func in cls.HANDLED_FUNCTIONS:\n",
        "          ret = cls.HANDLED_FUNCTIONS[func](*args, **kwargs)\n",
        "          if ret != NotImplemented:\n",
        "              return ret\n",
        "          # Otherwise drop through to the fallback\n",
        "\n",
        "        # Convert to float32 and call func\n",
        "        print(f\"ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for {func}@{types}\")\n",
        "\n",
        "        def to_tensor(t) -> Tensor:\n",
        "            if isinstance(t, ScaledTensor):\n",
        "                return t.to_tensor(torch.float32)\n",
        "            else:\n",
        "                return t\n",
        "\n",
        "        new_args = tuple(to_tensor(a) for a in args)\n",
        "        new_kwargs = { k:to_tensor(v) for (k,v) in kwargs.items() }\n",
        "\n",
        "        # We don't want the auto-downcast of \n",
        "        #    ret = super().__torch_function__(func, types, new_args, new_kwargs)\n",
        "        # because the super()'s handler will construct other types of tensor which\n",
        "        # don't simply reinterpret to a ScaledTensor.  If the handler's *do* construct\n",
        "        # ScaledTensors, of course, they should pass through, so it would also be \n",
        "        # incorrect to simply upcast the result.\n",
        "\n",
        "        with torch._C.DisableTorchFunctionSubclass():\n",
        "            return func(*new_args, **new_kwargs)\n",
        "\n",
        "ScaledTensor.HANDLED_FUNCTIONS = {}\n",
        "\n",
        "def quantise(x: Tensor, dtype: torch.dtype) -> ScaledTensor:\n",
        "    return ScaledTensor(st_quantise(x, dtype))\n",
        "\n",
        "def requantise(st) -> ScaledTensor:\n",
        "    return ScaledTensor(st_requantise(st.st))\n",
        "\n",
        "f16 = tensor([1, 2, 3], dtype=torch.float16)\n",
        "f8_t = torch.int8\n",
        "print(f16)\n",
        "print(f16.to(f8_t))\n",
        "\n",
        "st = quantise(f16, f8_t)\n",
        "\n",
        "print(st)\n",
        "print(requantise(st))\n",
        "print('Rounding errors at the high end of the f8 range:', st.to_tensor(f16.dtype))\n",
        "print('Viewing as a tensor should show NaN:', Tensor(st))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now overrides work, but just punt up to f32 for all ops: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for <method 'add' of 'torch._C._TensorBase' objects>@(<class '__main__.ScaledTensor'>,)\n",
            "tensor([2.9921, 3.9843, 5.0000])\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for <method 'mul' of 'torch._C._TensorBase' objects>@(<class '__main__.ScaledTensor'>,)\n",
            "tensor([1.9843, 3.9685, 6.0000])\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for <method 'add' of 'torch._C._TensorBase' objects>@(<class '__main__.ScaledTensor'>,)\n",
            "tensor([1.9843, 3.9685, 6.0000])\n",
            "st3=ScaledTensor(0.787401556968689 * tensor([[127, 127, 127],\n",
            "        [127, 127, 127]], dtype=torch.int8))\n",
            "st4=ScaledTensor(1.574803113937378 * tensor([[127, 127, 127, 127],\n",
            "        [127, 127, 127, 127],\n",
            "        [127, 127, 127, 127]], dtype=torch.int8))\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for <method 'matmul' of 'torch._C._TensorBase' objects>@(<class '__main__.ScaledTensor'>,)\n",
            "st3 @ st4 = tensor([[60000., 60000., 60000., 60000.],\n",
            "        [60000., 60000., 60000., 60000.]])\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for <method 'matmul' of 'torch._C._TensorBase' objects>@(<class '__main__.ScaledTensor'>,)\n",
            "f32(st3 @ st4)       = tensor([[60000., 60000., 60000., 60000.],\n",
            "        [60000., 60000., 60000., 60000.]]) <-- ~48200 quantized\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for <method 'to' of 'torch._C._TensorBase' objects>@(<class '__main__.ScaledTensor'>,)\n",
            "ScaledTensor.__torch_function__: WARNING: Upcasting to float32 for <method 'to' of 'torch._C._TensorBase' objects>@(<class '__main__.ScaledTensor'>,)\n",
            "f32(st3) @ f32(st4)  = tensor([[60000., 60000., 60000., 60000.],\n",
            "        [60000., 60000., 60000., 60000.]]) <-- 60000 exact\n"
          ]
        }
      ],
      "source": [
        "print(st + 2)\n",
        "print(2 * st)\n",
        "print(st + st)\n",
        "\n",
        "st3 = quantise(torch.full((2, 3), 100.0), torch.int8)\n",
        "st4 = quantise(torch.full((3, 4), 200.0), torch.int8)\n",
        "print(f'{st3=}')\n",
        "print(f'{st4=}')\n",
        "print(f'{st3 @ st4 = }')\n",
        "print(f'{f32(st3 @ st4)       = } <-- ~48200 quantized')\n",
        "print(f'{f32(st3) @ f32(st4)  = } <-- 60000 exact')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "st + st=ScaledTensor(0.04724409431219101 * tensor([ 42,  84, 127], dtype=torch.int8))\n",
            "f32(st + st)=tensor([1.9843, 3.9685, 6.0000])\n",
            "f32(st) + f32(st)=tensor([1.9843, 3.9685, 6.0000])\n",
            "st3=ScaledTensor(0.787401556968689 * tensor([[127, 127, 127],\n",
            "        [127, 127, 127]], dtype=torch.int8))\n",
            "st4=ScaledTensor(1.574803113937378 * tensor([[127, 127, 127, 127],\n",
            "        [127, 127, 127, 127],\n",
            "        [127, 127, 127, 127]], dtype=torch.int8))\n",
            "st3 @ st4 = ScaledTensor(472.4409484863281 * tensor([[102, 102, 102, 102],\n",
            "        [102, 102, 102, 102]], dtype=torch.int8))\n",
            "f32(st3 @ st4)       = tensor([[48188.9766, 48188.9766, 48188.9766, 48188.9766],\n",
            "        [48188.9766, 48188.9766, 48188.9766, 48188.9766]]) <-- ~48200 quantized\n",
            "f32(st3) @ f32(st4)  = tensor([[60000., 60000., 60000., 60000.],\n",
            "        [60000., 60000., 60000., 60000.]]) <-- 60000 exact\n"
          ]
        }
      ],
      "source": [
        "def torch_function_override(cls, func):\n",
        "    def doit(impl):\n",
        "        cls.HANDLED_FUNCTIONS[func] = impl\n",
        "    return doit\n",
        "\n",
        "@torch_function_override(ScaledTensor, Tensor.add)\n",
        "def _(a:Tensor, b: Tensor) -> Tensor:\n",
        "    if not (isinstance(a, ScaledTensor) and isinstance(b, ScaledTensor)):\n",
        "        return NotImplemented\n",
        "\n",
        "    return ScaledTensor(st_add(a.st, b.st))\n",
        "\n",
        "@torch_function_override(ScaledTensor, Tensor.matmul)\n",
        "def _(a:Tensor, b: Tensor) -> Tensor:\n",
        "    if not (isinstance(a, ScaledTensor) and isinstance(b, ScaledTensor)):\n",
        "        return NotImplemented\n",
        "\n",
        "    return ScaledTensor(st_matmul(a.st, b.st))\n",
        "\n",
        "@torch_function_override(ScaledTensor, Tensor.to)\n",
        "def _(a:Tensor, dtype:torch.dtype) -> Tensor:\n",
        "    assert isinstance(a, ScaledTensor)\n",
        "    return a.st.to_tensor(dtype)\n",
        "\n",
        "print(f'{st + st=}')\n",
        "print(f'{f32(st + st)=}')\n",
        "print(f'{f32(st) + f32(st)=}')\n",
        "\n",
        "print(f'{st3=}')\n",
        "print(f'{st4=}')\n",
        "print(f'{st3 @ st4 = }')\n",
        "print(f'{f32(st3 @ st4)       = } <-- ~48200 quantized')\n",
        "print(f'{f32(st3) @ f32(st4)  = } <-- 60000 exact')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# And in a network..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtrOg8FMpDb3",
        "outputId": "cd7bc783-0567-4c37-908d-56ba2ec2e1a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wasted bits #1: 8.258488655090332\n",
            "wasted bits #2: 10.414993286132812\n",
            "\n",
            "stensor([[   0.        -41.124104  164.49641  ...  287.8687     41.124104\n",
            "   205.62051 ]\n",
            " [ -82.24821  -123.37231   -82.24821  ... -164.49641  -164.49641\n",
            "  -164.49641 ]\n",
            " [ 123.37231  -205.62051   411.24103  ...  -82.24821     0.\n",
            "     0.      ]\n",
            " ...\n",
            " [ -41.124104  123.37231   205.62051  ... -287.8687   -287.8687\n",
            "  -411.24103 ]\n",
            " [ -82.24821  -164.49641    82.24821  ...   82.24821    41.124104\n",
            "   -41.124104]\n",
            " [  41.124104  205.62051    82.24821  ...   82.24821  -164.49641\n",
            "  -123.37231 ]], dtype=sint16)\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, hidden_size: int, dtype: Union[torch.dtype, ScaledDType]):\n",
        "        super().__init__()\n",
        "        self.W0 = torch.randn(hidden_size, 4*hidden_size).to(dtype)\n",
        "        self.W1 = torch.randn(4*hidden_size, hidden_size).to(dtype)\n",
        "        self.relu = relu if isinstance(dtype, ScaledDType) else nn.functional.relu\n",
        "\n",
        "    def forward(self, x: Union[Tensor, ScaledTensor]) -> Union[Tensor, ScaledTensor]:\n",
        "        y = self.relu(x @ self.W0)\n",
        "\n",
        "        if isinstance(y, ScaledTensor):\n",
        "            print(f\"wasted bits #1: {wasted_bits(y)}\")\n",
        "            y = requantise(y)\n",
        "\n",
        "        y = y @ self.W1\n",
        "\n",
        "        if isinstance(y, ScaledTensor):\n",
        "            print(f\"wasted bits #2: {wasted_bits(y)}\")\n",
        "\n",
        "        return y\n",
        "\n",
        "hidden_size = 1024\n",
        "dtype = sint16\n",
        "module = FFN(hidden_size, dtype)\n",
        "result = module(torch.randn(10, hidden_size).to(dtype))\n",
        "print()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def st_matmul(a: ScaledTensorData, b: ScaledTensorData) -> ScaledTensorData:\n",
        "    # NxK @ KxM\n",
        "    K = a.shape[-1]\n",
        "    \n",
        "    assert a.data.dtype == b.data.dtype\n",
        "    out_dtype = a.data.dtype\n",
        "\n",
        "    a_maxval = a.scale * a.dtype_max\n",
        "    b_maxval = b.scale * b.dtype_max\n",
        "\n",
        "    out_maxval = a_maxval * b_maxval * K\n",
        "\n",
        "    # Scale each multiplicand to sqrt(dtype_max * K)\n",
        "    a_downscale = 1 / torch.sqrt(a.dtype_max * K)\n",
        "    b_downscale = 1 / torch.sqrt(b.dtype_max * K)\n",
        "\n",
        "    print(f'{a_downscale=} {b_downscale=}')\n",
        "\n",
        "    a_scaled = a.data * a_downscale\n",
        "    b_scaled = b.data * b_downscale\n",
        "\n",
        "    # Assume low-precision muls will accumulate to uptype\n",
        "    # to simulate this on cpu, uptype before the matmul;\n",
        "    # on appropriate hardware (e.g. graphcore, h100), call the special matmul\n",
        "    out_data = _uptype(a_scaled) @ _uptype(b_scaled)\n",
        "    # out_data = (ad @ bd) * (a_downscale * b_downscale)\n",
        "\n",
        "    # A = ad * as\n",
        "    # B = bd * bs\n",
        "    # O = A @ B\n",
        "    #   = (ad @ bd) * (as * bs)\n",
        "    #   = (ad @ bd) * (as * bs)\n",
        "\n",
        "    # O = A @ B = ScaledTensor(A @ B * (out_dtype_max / out_maxval), 1 / (dtype_max / out_maxval))\n",
        "    #  OD_final = A @ B * (out_dtype_max / out_maxval)\n",
        "    #  OD_final = ad @ bd * (as * bs * out_dtype_max / out_maxval)\n",
        "    #  OD_final = ad @ bd * (a_downscale * b_downscale) * (as * bs * out_dtype_max / out_maxval / (a_downscale * b_downscale))\n",
        "    #  OD_final = out_data * (as * bs * out_dtype_max / out_maxval / (a_downscale * b_downscale))\n",
        "\n",
        "\n",
        "\n",
        "    scale = a.scale * b.scale * a.shape[-1]\n",
        "    downscale = (a.shape[-1] / _maxval(out_dtype)) ** -.5\n",
        "    out_data = (a.data * downscale).to(out_dtype) @ (b.data * downscale).to(out_dtype)\n",
        "    return ScaledTensorData(out_data, scale)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
