{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aAoSgjO1FhVr"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from typing import *\n",
        "_torch_tensor_to = torch.Tensor.to\n",
        "_torch_tensor = torch.tensor\n",
        "\n",
        "def numeric_info(dtype):\n",
        "  return torch.finfo(dtype) if dtype.is_floating_point else torch.iinfo(dtype)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM8RBHPwIkV2",
        "outputId": "353862f6-c2e3-4346-b891-25de11eb3550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.], dtype=torch.float16)\n",
            "tensor([1., 2., 3.], dtype=torch.float8_e4m3fn)\n",
            "ScaledTensor(0.0066964286379516125 * tensor([144., 288., 448.], dtype=torch.float8_e4m3fn))\n",
            "ScaledTensor(0.0066964286379516125 * tensor([144., 288., 448.], dtype=torch.float8_e4m3fn))\n",
            "tensor([0.9639, 1.9277, 3.0000], dtype=torch.float16) # <- rounding errors at the high end of the f8 range\n"
          ]
        }
      ],
      "source": [
        "# Core\n",
        "\n",
        "# https://pytorch.org/docs/stable/notes/extending.html#extending-torch-with-a-tensor-like-type\n",
        "@dataclass\n",
        "class ScaledTensor:\n",
        "    data: Tensor\n",
        "    scale: Tensor\n",
        "\n",
        "    def __init__(self, data, scale = torch.tensor(1.0, dtype=torch.float32)):\n",
        "        self.data = data\n",
        "        self.scale = scale\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        assert self.scale.dtype == torch.float32\n",
        "        assert self.scale.shape == (1,) \n",
        "        # Possible future expansion to e.g. row-scaled, column-scaled, etc, but\n",
        "        # for now, insist scale is a single-element tensor\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"ScaledTensor({self.scale} * {self.data})\"\n",
        "\n",
        "    def to_tensor(self, dtype: torch.dtype) -> Tensor:\n",
        "        return self.data.to(dtype) * self.scale.to(dtype)\n",
        "\n",
        "    @property\n",
        "    def shape(self) -> torch.Size:\n",
        "        return self.data.shape\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
        "        kwargs = kwargs or {}\n",
        "        if func in cls.HANDLED_FUNCTIONS:\n",
        "          return cls.HANDLED_FUNCTIONS[func](*args, **kwargs)\n",
        "        \n",
        "        print(f\"ScaledTensor: Not handling {func}({types})\")\n",
        "        # Convert to float32 and call func\n",
        "        def to_tensor(t) -> Tensor:\n",
        "            if isinstance(t, ScaledTensor):\n",
        "                return t.to_tensor(torch.float32)\n",
        "            else:\n",
        "                return t\n",
        "        new_args = ( to_tensor(a) for a in args)\n",
        "        new_kwargs = { k:to_tensor(v) for (k,v) in kwargs.items()}\n",
        "        return func(*new_args, **new_kwargs)\n",
        "\n",
        "ScaledTensor.HANDLED_FUNCTIONS = {}\n",
        "\n",
        "def _uptype(dtype) -> torch.dtype:\n",
        "    if torch.finfo(dtype).bits < 16:\n",
        "        return torch.float16\n",
        "    else:\n",
        "        return dtype\n",
        "    \n",
        "def _to_uptype(t : Tensor) -> Tensor:\n",
        "    return t.to(_uptype(t.dtype))\n",
        "\n",
        "def _maxabs(t : Tensor):\n",
        "    return _to_uptype(t).abs().max().to(torch.float32)\n",
        "\n",
        "def quantise(x: Tensor, dtype: torch.dtype) -> ScaledTensor:\n",
        "    maxval = _maxabs(x)\n",
        "    if maxval == 0:\n",
        "        # Tensor is all zeros - set scale to 1\n",
        "        scale = torch.tensor(1.0, dtype=torch.float32)\n",
        "    else:\n",
        "        # Scale so that largest element is the largest finite value of dtype\n",
        "        scale = maxval / torch.finfo(dtype).max\n",
        "\n",
        "    return ScaledTensor(data=(x / scale).to(dtype), scale=scale)\n",
        "\n",
        "def requantise(st: ScaledTensor) -> ScaledTensor:\n",
        "    \"\"\"\n",
        "    Rescale so that max(|data|) == maxFinite(data.dtype)\n",
        "\n",
        "    Equivalent to quantise(st.to_tensor(torch.float32)) but avoids the conversion\n",
        "\n",
        "    Returned tensor may share its data with input tensor\n",
        "    \"\"\"\n",
        "    maxdataval = _maxabs(st.data)\n",
        "    if maxdataval == 0:\n",
        "        # All zero, reset scale to 1\n",
        "        return ScaledTensor(st.data, st.scale)\n",
        "    else:\n",
        "         rescale = maxdataval / torch.finfo(st.data.dtype).max\n",
        "    return ScaledTensor((_to_uptype(st.data) * (1 / rescale)).to(st.data.dtype), st.scale * rescale)\n",
        "\n",
        "f16 = torch.tensor([1, 2, 3], dtype=torch.float16)\n",
        "f8_t=torch.float8_e4m3fn\n",
        "st = quantise(f16, f8_t)\n",
        "\n",
        "print(f16)\n",
        "print(f16.to(f8_t))\n",
        "print(st)\n",
        "print(requantise(st))\n",
        "print(st.to_tensor(f16.dtype), \"# <- rounding errors at the high end of the f8 range\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3., 4., 5.], dtype=torch.float16)\n"
          ]
        }
      ],
      "source": [
        "print(f16 + 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ScaledTensor: Not handling <method 'add' of 'torch._C._TensorBase' objects>((<class '__main__.ScaledTensor'>,))\n",
            "tensor([2.9643, 3.9286, 5.0000])\n",
            "ScaledTensor: Not handling <method 'mul' of 'torch._C._TensorBase' objects>((<class '__main__.ScaledTensor'>,))\n",
            "tensor([1.9286, 3.8571, 6.0000])\n"
          ]
        }
      ],
      "source": [
        "# Downside 1: can't do st + 3\n",
        "print(st + torch.tensor([2]))\n",
        "print(torch.tensor([2]) * st)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylM9CdOGKpn2",
        "outputId": "9d4fa786-a6b2-4d04-aa23-3200a789588e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stensor(300.0, dtype=sint8)\n",
            "stensor([[390636.9 390636.9 390636.9]\n",
            " [390636.9 390636.9 390636.9]], dtype=sint16)\n"
          ]
        }
      ],
      "source": [
        "# Ops\n",
        "# - Use worst-case scaling rules (no overflow!)\n",
        "# - Placeholder impl (fast impl requires custom kernls)\n",
        "# - No autograd support\n",
        "\n",
        "\n",
        "def add(a: ScaledTensor, b: ScaledTensor) -> ScaledTensor:\n",
        "    assert a.dtype == b.dtype\n",
        "    scale = a.scale + b.scale\n",
        "    data = (a.data * (a.scale / scale) + b.data * (b.scale / scale)).to(a.data.dtype)\n",
        "    return ScaledTensor(data, scale, a.dtype)\n",
        "\n",
        "\n",
        "def matmul(a: ScaledTensor, b: ScaledTensor) -> ScaledTensor:\n",
        "    assert a.dtype == b.dtype\n",
        "    scale = a.scale * b.scale * a.shape[-1]\n",
        "    downscale = (a.shape[-1] / a.dtype.base_scale) ** -.5\n",
        "    return ScaledTensor(\n",
        "        (a.data * downscale).to(a.dtype.dtype) @ (b.data * downscale).to(b.dtype.dtype),\n",
        "        scale, a.dtype)\n",
        "\n",
        "\n",
        "def relu(a: ScaledTensor) -> ScaledTensor:\n",
        "    return ScaledTensor(nn.functional.relu(a.data), a.scale, a.dtype)\n",
        "\n",
        "\n",
        "ScaledTensor.__add__ = add\n",
        "ScaledTensor.__matmul__ = matmul\n",
        "\n",
        "\n",
        "print(torch.tensor(100, dtype=sint8) + torch.tensor(200, dtype=sint8))\n",
        "print(torch.full((2, 20), 100).to(sint16) @ torch.full((20, 3), 200).to(sint16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtrOg8FMpDb3",
        "outputId": "cd7bc783-0567-4c37-908d-56ba2ec2e1a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wasted bits #1: 8.258488655090332\n",
            "wasted bits #2: 10.414993286132812\n",
            "\n",
            "stensor([[   0.        -41.124104  164.49641  ...  287.8687     41.124104\n",
            "   205.62051 ]\n",
            " [ -82.24821  -123.37231   -82.24821  ... -164.49641  -164.49641\n",
            "  -164.49641 ]\n",
            " [ 123.37231  -205.62051   411.24103  ...  -82.24821     0.\n",
            "     0.      ]\n",
            " ...\n",
            " [ -41.124104  123.37231   205.62051  ... -287.8687   -287.8687\n",
            "  -411.24103 ]\n",
            " [ -82.24821  -164.49641    82.24821  ...   82.24821    41.124104\n",
            "   -41.124104]\n",
            " [  41.124104  205.62051    82.24821  ...   82.24821  -164.49641\n",
            "  -123.37231 ]], dtype=sint16)\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, hidden_size: int, dtype: Union[torch.dtype, ScaledDType]):\n",
        "        super().__init__()\n",
        "        self.W0 = torch.randn(hidden_size, 4*hidden_size).to(dtype)\n",
        "        self.W1 = torch.randn(4*hidden_size, hidden_size).to(dtype)\n",
        "        self.relu = relu if isinstance(dtype, ScaledDType) else nn.functional.relu\n",
        "\n",
        "    def forward(self, x: Union[Tensor, ScaledTensor]) -> Union[Tensor, ScaledTensor]:\n",
        "        y = self.relu(x @ self.W0)\n",
        "\n",
        "        if isinstance(y, ScaledTensor):\n",
        "            print(f\"wasted bits #1: {wasted_bits(y)}\")\n",
        "            y = requantise(y)\n",
        "\n",
        "        y = y @ self.W1\n",
        "\n",
        "        if isinstance(y, ScaledTensor):\n",
        "            print(f\"wasted bits #2: {wasted_bits(y)}\")\n",
        "\n",
        "        return y\n",
        "\n",
        "hidden_size = 1024\n",
        "dtype = sint16\n",
        "module = FFN(hidden_size, dtype)\n",
        "result = module(torch.randn(10, hidden_size).to(dtype))\n",
        "print()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
